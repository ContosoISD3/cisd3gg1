{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "syn-oea-cisd3gg1"
		},
		"EdFi_Ods_Production_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'EdFi_Ods_Production'"
		},
		"LS_Azure_SQL_DB_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LS_Azure_SQL_DB'"
		},
		"LS_SQL_Serverless_OEA_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LS_SQL_Serverless_OEA'"
		},
		"syn-oea-cisd3gg1-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'syn-oea-cisd3gg1-WorkspaceDefaultSqlServer'"
		},
		"LS_ADLS_OEA_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeacisd3gg1.dfs.core.windows.net"
		},
		"LS_HTTP_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{linkedService().baseURL}"
		},
		"LS_KeyVault_OEA_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://yourkeyvault.vault.azure.net/"
		},
		"bing-covid-19-data_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'bing-covid-19-data'"
		},
		"syn-oea-cisd3gg1-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeacisd3gg1.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Clever_land_in_s1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Lands data from Clever into the inbound folder in stage1, under a folder with the execution datetime.",
				"activities": [
					{
						"name": "copy Clever data",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "set exec_date_time",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "DS_HTTP_binary",
								"type": "DatasetReference",
								"parameters": {
									"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Clever/test_data/2020-10-27-resource-usage-students.csv"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ADLS_binary_file",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"filename": "clever/part1.csv"
								}
							}
						]
					},
					{
						"name": "set exec_date_time",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "exec_date_time",
							"value": {
								"value": "@{formatDateTime(convertTimeZone(utcnow(), 'UTC', 'Eastern Standard Time'), 'yyyy-MM-ddTHHmm_ss')}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"exec_date_time": {
						"type": "String"
					}
				},
				"folder": {
					"name": "1) Land in s1"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-15T13:43:17Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_HTTP_binary')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_file')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Clever_s2_to_s3')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "placholder",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Clever_land_in_s1",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "3) Process from s2 into s3"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-15T15:27:48Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Clever_land_in_s1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Contoso_ISD_package')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Copies in the Contoso test data and lands it in stage1np.\nUse this pipeline as part of the Contoso example.",
				"activities": [
					{
						"name": "Copy_from_each_URL - SIS test data",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Copy_from_each_URL",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"endpoints": {
									"value": "@json('[{\"URL\": \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentattendance.csv\",\"sinkDirectory\": \"contoso_sis\",\"sinkFilename\": \"studentattendance/studentattendance.csv\"}, {\"URL\": \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentdemographics.csv\",\"sinkDirectory\": \"contoso_sis\",\"sinkFilename\": \"studentdemographics/studentdemographics.csv\"}, {\"URL\": \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentsectionmark.csv\",\"sinkDirectory\": \"contoso_sis\",\"sinkFilename\": \"studentsectionmark/studentsectionmark.csv\"}]')",
									"type": "Expression"
								},
								"sinkFilesystem": "stage1np",
								"timezone": "Eastern Standard Time"
							}
						}
					},
					{
						"name": "Copy_from_each_URL - M365 test data",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Copy_from_each_URL",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"endpoints": {
									"value": "@json('[{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Activity/ApplicationUsage/Activity.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"Activity/Activity.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/Calendar.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"Calendar/Calendar.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/Course.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"Course/Course.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/Org.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"Org/Org.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/Person.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"Person/Person.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/PersonIdentifier.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"PersonIdentifier/PersonIdentifier.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/RefDefinition.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"RefDefinition/RefDefinition.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/Section.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"Section/Section.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/Session.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"Session/Session.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/StaffOrgAffiliation.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"StaffOrgAffiliation/StaffOrgAffiliation.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/StaffSectionMembership.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"StaffSectionMembership/StaffSectionMembership.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/StudentOrgAffiliation.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"StudentOrgAffiliation/StudentOrgAffiliation.csv\"},\n{\"URL\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/M365/test_data/DIPData/Roster/StudentSectionMembership.csv\", \"sinkDirectory\":\"m365\", \"sinkFilename\":\"StudentSectionMembership/StudentSectionMembership.csv\"}]')",
									"type": "Expression"
								},
								"sinkFilesystem": "stage1np",
								"timezone": "Eastern Standard Time"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-10-28T13:57:38Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Copy_from_each_URL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Contoso_SIS_main')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy_from_each_URL - SIS data",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Copy_from_each_URL",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"endpoints": {
									"value": "@json('[{\"URL\": \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/batch1/studentattendance.csv\",\"sinkDirectory\": \"contoso_sis/studentattendance\",\"sinkFilename\": \"part1.csv\"}, {\"URL\": \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/batch1/studentdemographics.csv\",\"sinkDirectory\": \"contoso_sis/studentdemographics\",\"sinkFilename\": \"part1.csv\"}, {\"URL\": \"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/batch1/studentsectionmark.csv\",\"sinkDirectory\": \"contoso_sis/studentsectionmark\",\"sinkFilename\": \"part1.csv\"}]')",
									"type": "Expression"
								},
								"sinkFilesystem": "stage1np",
								"timezone": "Eastern Standard Time"
							}
						}
					},
					{
						"name": "Contoso_SIS_ingest",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Copy_from_each_URL - SIS data",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "OEA_connector",
								"type": "NotebookReference"
							},
							"parameters": {
								"object_name": {
									"value": "contoso_sis",
									"type": "string"
								},
								"method_name": {
									"value": "ingest",
									"type": "string"
								}
							},
							"snapshot": true
						}
					},
					{
						"name": "If create_sql_db",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Contoso_SIS_ingest",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@pipeline().parameters.create_sql_db",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "create_sql_db",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "create_sql_db",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"storageAccount": {
												"value": "@pipeline().parameters.storageAccount",
												"type": "Expression"
											},
											"sourceDirectory": {
												"value": "@pipeline().parameters.sourceDirectory",
												"type": "Expression"
											},
											"stage": "2"
										}
									}
								}
							]
						}
					},
					{
						"name": "If create_lake_db",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Contoso_SIS_ingest",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@pipeline().parameters.create_lake_db",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "create_lake_db",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "tmpcreate_lake_db",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"stageNum": "2",
											"sourceDirectory": {
												"value": "@pipeline().parameters.sourceDirectory",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"storageAccount": {
						"type": "string",
						"defaultValue": "stoeacisd3gg1"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"create_sql_db": {
						"type": "bool",
						"defaultValue": false
					},
					"create_lake_db": {
						"type": "bool",
						"defaultValue": false
					}
				},
				"folder": {
					"name": "OEA_Modules"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Copy_from_each_URL')]",
				"[concat(variables('workspaceId'), '/notebooks/OEA_connector')]",
				"[concat(variables('workspaceId'), '/pipelines/create_sql_db')]",
				"[concat(variables('workspaceId'), '/pipelines/tmpcreate_lake_db')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_all_from_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get list of tables",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": "select schema_name(t.schema_id) as schema_name, t.name as table_name\nfrom sys.tables t",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Azure_SQL_DB",
								"type": "DatasetReference",
								"parameters": {
									"dbServer": {
										"value": "@pipeline().parameters.dbServer",
										"type": "Expression"
									},
									"dbName": {
										"value": "@pipeline().parameters.dbName",
										"type": "Expression"
									},
									"userName": {
										"value": "@pipeline().parameters.userName",
										"type": "Expression"
									},
									"keyVaultSecretName": {
										"value": "@pipeline().parameters.keyVaultSecretName",
										"type": "Expression"
									}
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Set currentDateTime",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "currentDateTime",
							"value": {
								"value": "@{formatDateTime(convertTimeZone(utcnow(), 'UTC', pipeline().parameters.timezone), 'yyyy-MM-ddTHHmm_ss')}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get list of tables",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set currentDateTime",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get list of tables').output.value",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Copy_from_Azure_SQL_DB",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "Copy_from_Azure_SQL_DB",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"dbServer": {
												"value": "@pipeline().parameters.dbServer",
												"type": "Expression"
											},
											"dbName": {
												"value": "@pipeline().parameters.dbName",
												"type": "Expression"
											},
											"userName": {
												"value": "@pipeline().parameters.userName",
												"type": "Expression"
											},
											"keyVaultSecretName": {
												"value": "@pipeline().parameters.keyVaultSecretName",
												"type": "Expression"
											},
											"query": {
												"value": "select * from @{item().schema_name}.@{item().table_name}",
												"type": "Expression"
											},
											"sinkFilesystem": {
												"value": "@pipeline().parameters.sinkFilesystem",
												"type": "Expression"
											},
											"sinkDirectory": {
												"value": "@{pipeline().parameters.sinkDirectory}/@{variables('currentDateTime')}/@{item().table_name}",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "contosoisd3a.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqladmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqladmin-password"
					},
					"sinkFilesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"sinkDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis_db"
					},
					"timezone": {
						"type": "string",
						"defaultValue": "Eastern Standard Time"
					}
				},
				"variables": {
					"currentDateTime": {
						"type": "String"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2021-10-29T22:15:07Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_Azure_SQL_DB')]",
				"[concat(variables('workspaceId'), '/pipelines/Copy_from_Azure_SQL_DB')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_from_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy from Azure SQL DB",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": {
									"value": "@pipeline().parameters.query",
									"type": "Expression"
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "DS_Azure_SQL_DB",
								"type": "DatasetReference",
								"parameters": {
									"dbServer": {
										"value": "@pipeline().parameters.dbServer",
										"type": "Expression"
									},
									"dbName": {
										"value": "@pipeline().parameters.dbName",
										"type": "Expression"
									},
									"userName": {
										"value": "@pipeline().parameters.userName",
										"type": "Expression"
									},
									"keyVaultSecretName": {
										"value": "@pipeline().parameters.keyVaultSecretName",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ADLS_parquet",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.sinkFilesystem",
										"type": "Expression"
									},
									"directory": {
										"value": "@pipeline().parameters.sinkDirectory",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "contosoisd3a.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqladmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqladmin-password"
					},
					"query": {
						"type": "string",
						"defaultValue": "select * from person"
					},
					"sinkFilesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"sinkDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis_db/person"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2021-10-29T22:15:03Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_Azure_SQL_DB')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_parquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_from_URL')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Copies data from the specified URL and lands it in the specified location in the data lake.",
				"activities": [
					{
						"name": "copy from URL",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "DS_HTTP_binary",
								"type": "DatasetReference",
								"parameters": {
									"URL": {
										"value": "@pipeline().parameters.URL",
										"type": "Expression"
									}
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ADLS_binary_file",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.sinkFilesystem",
										"type": "Expression"
									},
									"filename": {
										"value": "@pipeline().parameters.sinkFilename",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"URL": {
						"type": "string",
						"defaultValue": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentattendance.csv"
					},
					"sinkFilesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"sinkFilename": {
						"type": "string",
						"defaultValue": "contoso_sis/example1/studentattendance.csv"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2021-10-28T13:57:30Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_HTTP_binary')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_file')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Copy_from_each_URL')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Retrieves data from multiple HTTP endpoints as specified in the 'endpoints' parameter.\nThe data is landed in the data lake within a folder named with the current datetime (in the timezone specified).\n\nFor a list of timezones, see: https://docs.microsoft.com/en-us/azure/data-factory/control-flow-expression-language-functions#convertfromutc",
				"activities": [
					{
						"name": "get data for each endpoint",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Set currentDateTime",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.endpoints",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Copy_from_URL",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "Copy_from_URL",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"URL": {
												"value": "@item().URL",
												"type": "Expression"
											},
											"sinkFilesystem": {
												"value": "@pipeline().parameters.sinkFilesystem",
												"type": "Expression"
											},
											"sinkFilename": {
												"value": "@{item().sinkDirectory}/@{variables('currentDateTime')}/@{item().sinkFilename}",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "Set currentDateTime",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "currentDateTime",
							"value": {
								"value": "@{formatDateTime(convertTimeZone(utcnow(), 'UTC', pipeline().parameters.timezone), 'yyyy-MM-ddTHHmm_ss')}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"endpoints": {
						"type": "array",
						"defaultValue": [
							{
								"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentattendance.csv",
								"sinkDirectory": "contoso_sis",
								"sinkFilename": "studentattendance/studentattendance.csv"
							},
							{
								"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentdemographics.csv",
								"sinkDirectory": "contoso_sis",
								"sinkFilename": "studentdemographics/studentdemographics.csv"
							},
							{
								"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentsectionmark.csv",
								"sinkDirectory": "contoso_sis",
								"sinkFilename": "studentsectionmark/studentsectionmark.csv"
							}
						]
					},
					"sinkFilesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"timezone": {
						"type": "string",
						"defaultValue": "Eastern Standard Time"
					}
				},
				"variables": {
					"currentDateTime": {
						"type": "String"
					}
				},
				"folder": {
					"name": "OEA_Framework/Extracts"
				},
				"annotations": [],
				"lastPublishTime": "2021-10-28T13:57:35Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Copy_from_URL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_SQLS2_DB_For_Source')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get folders in stage2p",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "create sql db unless it already exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2p",
									"directory": {
										"value": "@pipeline().parameters.sourceDirectory",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "for each folder",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get folders in stage2p",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get folders in stage2p').output.childItems",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "create view",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [
										{
											"name": "sql",
											"value": "CREATE OR ALTER VIEW @{item()} AS SELECT * FROM OPENROWSET(     BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage2p/@{pipeline().parameters.sourceDirectory}/@{item()}',     FORMAT='DELTA' ) AS [r]"
										}
									],
									"typeProperties": {
										"storedProcedureName": "[dbo].[sp_executesql]",
										"storedProcedureParameters": {
											"command": {
												"value": {
													"value": "CREATE OR ALTER VIEW @{item().name} AS\nSELECT * FROM OPENROWSET(\n    BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage2p/@{pipeline().parameters.sourceDirectory}/@{item().name}',\n    FORMAT='DELTA'\n) AS [r]",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_SQL_Serverless_OEA",
										"type": "LinkedServiceReference",
										"parameters": {
											"databaseName": {
												"value": "sqls2_@{pipeline().parameters.sourceDirectory}",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "create sql db unless it already exists",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "sql",
								"value": "\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '@{pipeline().parameters.sourceDirectory}') BEGIN CREATE DATABASE @{pipeline().parameters.sourceDirectory}; END;\""
							}
						],
						"typeProperties": {
							"storedProcedureName": "[dbo].[sp_executesql]",
							"storedProcedureParameters": {
								"command": {
									"value": {
										"value": "IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = 'sqls2_@{pipeline().parameters.sourceDirectory}') \nBEGIN\n  CREATE DATABASE sqls2_@{pipeline().parameters.sourceDirectory}; \nEND;",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "LS_SQL_Serverless_OEA",
							"type": "LinkedServiceReference",
							"parameters": {
								"databaseName": "master"
							}
						}
					},
					{
						"name": "Get folders in stage2np",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "create sql db unless it already exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2np",
									"directory": {
										"value": "@pipeline().parameters.sourceDirectory",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "for each folder2",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get folders in stage2np",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get folders in stage2np').output.childItems",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "create view_copy1",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [
										{
											"name": "sql",
											"value": "CREATE OR ALTER VIEW @{item()} AS SELECT * FROM OPENROWSET(     BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage2np/@{pipeline().parameters.sourceDirectory}/@{item()}',     FORMAT='DELTA' ) AS [r]"
										}
									],
									"typeProperties": {
										"storedProcedureName": "[dbo].[sp_executesql]",
										"storedProcedureParameters": {
											"command": {
												"value": {
													"value": "CREATE OR ALTER VIEW @{item().name} AS\nSELECT * FROM OPENROWSET(\n    BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage2np/@{pipeline().parameters.sourceDirectory}/@{item().name}',\n    FORMAT='DELTA'\n) AS [r]",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_SQL_Serverless_OEA",
										"type": "LinkedServiceReference",
										"parameters": {
											"databaseName": {
												"value": "sqls2_@{pipeline().parameters.sourceDirectory}",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"storageAccount": {
						"type": "string",
						"defaultValue": "stoeacisd3gg1"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis"
					}
				},
				"variables": {
					"filesystem_p": {
						"type": "String",
						"defaultValue": "stage2p"
					},
					"filesystem_np": {
						"type": "String",
						"defaultValue": "stage2np"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_folder')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_SQL_Serverless_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_roster_s1_into_s2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "folder_metadata",
						"description": "Gets the list of files in the inbound folder in stage1.",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_file",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"filename": "clever"
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "for each file in inbound folder",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "folders",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('folders').output.Value",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "M365_s1_to_s2",
									"type": "SynapseNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebook": {
											"referenceName": "M365_roster_v0p3_s1_to_s2",
											"type": "NotebookReference"
										},
										"parameters": {
											"storage_account": {
												"value": "stoeacisd3gg1",
												"type": "string"
											},
											"file_path": {
												"value": {
													"value": "/M365/inbound/roster/@{item().name}",
													"type": "Expression"
												},
												"type": "string"
											}
										},
										"snapshot": true
									}
								}
							]
						}
					},
					{
						"name": "folders",
						"type": "Filter",
						"dependsOn": [
							{
								"activity": "folder_metadata",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('folder_metadata').output.childitems",
								"type": "Expression"
							},
							"condition": {
								"value": "@equals(item().type, 'Folder')",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "2) Process from s1 into s2"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-16T20:23:32Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_file')]",
				"[concat(variables('workspaceId'), '/notebooks/M365_roster_v0p3_s1_to_s2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Push_to_SDS')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "placholder",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Clever_land_in_s1",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "4) Outbound"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-15T15:28:15Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Clever_land_in_s1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/call_oea_framework')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "OEA_connector",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "OEA_connector",
								"type": "NotebookReference"
							},
							"parameters": {
								"object_name": {
									"value": {
										"value": "@pipeline().parameters.object_name",
										"type": "Expression"
									},
									"type": "string"
								},
								"method_name": {
									"value": {
										"value": "@pipeline().parameters.method_name",
										"type": "Expression"
									},
									"type": "string"
								},
								"kwargs": {
									"value": {
										"value": "@pipeline().parameters.kwargs",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"object_name": {
						"type": "string",
						"defaultValue": "oea"
					},
					"method_name": {
						"type": "string"
					},
					"kwargs": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/OEA_connector')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tmpcreate_lake_db')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "create_lake_db",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "OEA_connector",
								"type": "NotebookReference"
							},
							"parameters": {
								"object_name": {
									"value": "oea",
									"type": "string"
								},
								"method_name": {
									"value": "create_lake_db",
									"type": "string"
								},
								"kwargs": {
									"value": {
										"value": "{'stage_num':@{pipeline().parameters.stageNum},'source_dir':@{pipeline().parameters.sourceDirectory}}",
										"type": "Expression"
									},
									"type": "string"
								}
							},
							"snapshot": true
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"stageNum": {
						"type": "string",
						"defaultValue": "2"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis"
					}
				},
				"folder": {
					"name": "OEA_Framework/Ingest"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/OEA_connector')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create_sql_db')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "set sqlDBName",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "sqlDBName",
							"value": {
								"value": "sqls@{pipeline().parameters.stage}_@{pipeline().parameters.sourceDirectory}",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Stored procedure1",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "set sqlDBName",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[dbo].[sp_executesql]",
							"storedProcedureParameters": {
								"command": {
									"value": {
										"value": "IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '@{variables('sqlDBName')}') \nBEGIN\n  CREATE DATABASE @{variables('sqlDBName')}; \nEND;",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "LS_SQL_Serverless_OEA",
							"type": "LinkedServiceReference",
							"parameters": {
								"databaseName": "master"
							}
						}
					},
					{
						"name": "get folders in stageXp",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "Stored procedure1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "stage@{pipeline().parameters.stage}p",
										"type": "Expression"
									},
									"directory": {
										"value": "@pipeline().parameters.sourceDirectory",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "get folders in stageXp",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('get folders in stageXp').output.childItems",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "create or alter view for pseduonymized tables",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"storedProcedureName": "[dbo].[sp_executesql]",
										"storedProcedureParameters": {
											"command": {
												"value": {
													"value": "CREATE OR ALTER VIEW @{item().name} AS\nSELECT * FROM OPENROWSET(\nBULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage@{pipeline().parameters.stage}p/@{pipeline().parameters.sourceDirectory}/@{item().name}',\nFORMAT='DELTA'\n) AS [r]",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_SQL_Serverless_OEA",
										"type": "LinkedServiceReference",
										"parameters": {
											"databaseName": {
												"value": "@variables('sqlDBName')",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "get folders in stageXnp",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "Stored procedure1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "stage@{pipeline().parameters.stage}np",
										"type": "Expression"
									},
									"directory": {
										"value": "@pipeline().parameters.sourceDirectory",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "ForEach2",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "get folders in stageXnp",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('get folders in stageXnp').output.childItems",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "create or alter view for lookup tables",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"storedProcedureName": "[dbo].[sp_executesql]",
										"storedProcedureParameters": {
											"command": {
												"value": {
													"value": "CREATE OR ALTER VIEW @{item().name} AS\nSELECT * FROM OPENROWSET(\nBULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/stage@{pipeline().parameters.stage}np/@{pipeline().parameters.sourceDirectory}/@{item().name}',\nFORMAT='DELTA'\n) AS [r]",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_SQL_Serverless_OEA",
										"type": "LinkedServiceReference",
										"parameters": {
											"databaseName": {
												"value": "@variables('sqlDBName')",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"storageAccount": {
						"type": "string",
						"defaultValue": "stoeacisd3gg1"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"stage": {
						"type": "string",
						"defaultValue": "2"
					}
				},
				"variables": {
					"sqlDBName": {
						"type": "String",
						"defaultValue": "sqls2_mydb"
					}
				},
				"folder": {
					"name": "OEA_Framework/Ingest"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_SQL_Serverless_OEA')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_folder')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create_sql_views_for_source')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get folder list 1",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "create sql db unless it already exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.filesystem1",
										"type": "Expression"
									},
									"directory": {
										"value": "@pipeline().parameters.sourceDirectory",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get folder list 1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get folder list 1').output.childItems",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "create_view",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "create_view",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"storageAccount": {
												"value": "@pipeline().parameters.storageAccount",
												"type": "Expression"
											},
											"filesystem": {
												"value": "@pipeline().parameters.filesystem1",
												"type": "Expression"
											},
											"sourceDirectory": {
												"value": "@pipeline().parameters.sourceDirectory",
												"type": "Expression"
											},
											"table": {
												"value": "@item().name",
												"type": "Expression"
											},
											"databaseName": {
												"value": "@pipeline().parameters.databaseName",
												"type": "Expression"
											}
										}
									}
								},
								{
									"name": "create_view_copy3",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "create_view",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"storageAccount": {
												"value": "@pipeline().parameters.storageAccount",
												"type": "Expression"
											},
											"filesystem": {
												"value": "@pipeline().parameters.filesystem1",
												"type": "Expression"
											},
											"sourceDirectory": {
												"value": "@pipeline().parameters.sourceDirectory",
												"type": "Expression"
											},
											"table": {
												"value": "@item().name",
												"type": "Expression"
											},
											"databaseName": {
												"value": "@pipeline().parameters.databaseName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "Get folder list 2",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "create sql db unless it already exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.filesystem2",
										"type": "Expression"
									},
									"directory": {
										"value": "@pipeline().parameters.sourceDirectory",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "ForEach1_copy1",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Get folder list 2",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Get folder list 2').output.childItems",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "create_view_copy1",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "create_view",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"storageAccount": {
												"value": "@pipeline().parameters.storageAccount",
												"type": "Expression"
											},
											"filesystem": {
												"value": "@pipeline().parameters.filesystem2",
												"type": "Expression"
											},
											"sourceDirectory": {
												"value": "@pipeline().parameters.sourceDirectory",
												"type": "Expression"
											},
											"table": {
												"value": "@{item().name}_lookup",
												"type": "Expression"
											},
											"databaseName": {
												"value": "@pipeline().parameters.databaseName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "create sql db unless it already exists",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "sql",
								"value": "\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '@{pipeline().parameters.databaseName}') BEGIN CREATE DATABASE @{pipeline().parameters.databaseName}; END;\""
							}
						],
						"typeProperties": {
							"storedProcedureName": "[dbo].[sp_executesql]",
							"storedProcedureParameters": {
								"command": {
									"value": {
										"value": "IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '@{pipeline().parameters.databaseName}') \nBEGIN\n  CREATE DATABASE @{pipeline().parameters.databaseName}; \nEND;",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "LS_SQL_Serverless_OEA",
							"type": "LinkedServiceReference",
							"parameters": {
								"databaseName": "master"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"storageAccount": {
						"type": "string",
						"defaultValue": "stoeacisd3gg1"
					},
					"filesystem1": {
						"type": "string",
						"defaultValue": "stage2p"
					},
					"filesystem2": {
						"type": "string",
						"defaultValue": "stage2np"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"databaseName": {
						"type": "string",
						"defaultValue": "sqls2e_contoso_sis"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_folder')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_SQL_Serverless_OEA')]",
				"[concat(variables('workspaceId'), '/pipelines/create_view')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create_view')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "create view",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "sql",
								"value": "CREATE OR ALTER VIEW @{item()} AS SELECT * FROM OPENROWSET(     BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/@{pipeline().parameters.filesystem}/@{pipeline().parameters.sourceDirectory}/@{item()}',     FORMAT='DELTA' ) AS [r]"
							}
						],
						"typeProperties": {
							"storedProcedureName": "[dbo].[sp_executesql]",
							"storedProcedureParameters": {
								"command": {
									"value": {
										"value": "CREATE OR ALTER VIEW @{pipeline().parameters.table} AS\nSELECT * FROM OPENROWSET(\n    BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/@{pipeline().parameters.filesystem}/@{pipeline().parameters.sourceDirectory}/@{pipeline().parameters.table}',\n    FORMAT='DELTA'\n) AS [r]",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "LS_SQL_Serverless_OEA",
							"type": "LinkedServiceReference",
							"parameters": {
								"databaseName": {
									"value": "@pipeline().parameters.databaseName",
									"type": "Expression"
								}
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"storageAccount": {
						"type": "string",
						"defaultValue": "stoeacisd3gg1"
					},
					"filesystem": {
						"type": "string",
						"defaultValue": "stage2p"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"table": {
						"type": "string",
						"defaultValue": "studentattendance"
					},
					"databaseName": {
						"type": "string",
						"defaultValue": "sqls2_contoso_sis"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_SQL_Serverless_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create_views')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "create sql db unless it already exists",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "sql",
								"value": "\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '@{pipeline().parameters.databaseName}') BEGIN CREATE DATABASE @{pipeline().parameters.databaseName}; END;\""
							}
						],
						"typeProperties": {
							"storedProcedureName": "[dbo].[sp_executesql]",
							"storedProcedureParameters": {
								"command": {
									"value": {
										"value": "IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '@{pipeline().parameters.databaseName}') \nBEGIN\n  CREATE DATABASE @{pipeline().parameters.databaseName}; \nEND;",
										"type": "Expression"
									},
									"type": "String"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "LS_SQL_Serverless_OEA",
							"type": "LinkedServiceReference",
							"parameters": {
								"databaseName": "master"
							}
						}
					},
					{
						"name": "for each table",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "create sql db unless it already exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.tables",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "create view",
									"type": "SqlServerStoredProcedure",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [
										{
											"name": "sql",
											"value": "CREATE OR ALTER VIEW @{item()} AS SELECT * FROM OPENROWSET(     BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/@{pipeline().parameters.filesystem}/@{pipeline().parameters.sourceDirectory}/@{item()}',     FORMAT='DELTA' ) AS [r]"
										}
									],
									"typeProperties": {
										"storedProcedureName": "[dbo].[sp_executesql]",
										"storedProcedureParameters": {
											"command": {
												"value": {
													"value": "CREATE OR ALTER VIEW @{item()} AS\nSELECT * FROM OPENROWSET(\n    BULK 'https://@{pipeline().parameters.storageAccount}.dfs.core.windows.net/@{pipeline().parameters.filesystem}/@{pipeline().parameters.sourceDirectory}/@{item()}',\n    FORMAT='DELTA'\n) AS [r]",
													"type": "Expression"
												},
												"type": "String"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_SQL_Serverless_OEA",
										"type": "LinkedServiceReference",
										"parameters": {
											"databaseName": {
												"value": "@pipeline().parameters.databaseName",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"storageAccount": {
						"type": "string",
						"defaultValue": "stoeacisd3gg1"
					},
					"filesystem": {
						"type": "string",
						"defaultValue": "stage2p"
					},
					"sourceDirectory": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"tables": {
						"type": "array",
						"defaultValue": [
							"studentattendance",
							"studentsectionmark"
						]
					},
					"databaseName": {
						"type": "string",
						"defaultValue": "sqls2_contoso_sis"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_SQL_Serverless_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reset_all_for_source')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Deletes the ingestion for the specified system and table - in order to start over when testing during implementation.",
				"activities": [
					{
						"name": "delete source system dir from stage1np",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"directory": {
										"value": "@pipeline().parameters.sourceSystem",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete from stage2np",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2np",
									"directory": {
										"value": "@pipeline().parameters.sourceSystem",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete from stage2p",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2p",
									"directory": {
										"value": "@pipeline().parameters.sourceSystem",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"sourceSystem": {
						"type": "string",
						"defaultValue": "contoso_sis"
					}
				},
				"folder": {
					"name": "OEA_Framework/Reset"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_folder')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/reset_ingestion_of_table')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Deletes the ingestion for the specified system and table - in order to start over when testing during implementation.",
				"activities": [
					{
						"name": "delete _checkpoints_p",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"directory": {
										"value": "@{pipeline().parameters.sourceSystem}/@{pipeline().parameters.tablename}/_checkpoints_p",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete _checkpoints_np",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"directory": {
										"value": "@{pipeline().parameters.sourceSystem}/@{pipeline().parameters.tablename}/_checkpoints_np",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete from stage2np",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2np",
									"directory": {
										"value": "@{pipeline().parameters.sourceSystem}/@{pipeline().parameters.tablename}",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					},
					{
						"name": "delete from stage2p",
						"type": "Delete",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DS_ADLS_binary_folder",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage2p",
									"directory": {
										"value": "@{pipeline().parameters.sourceSystem}/@{pipeline().parameters.tablename}",
										"type": "Expression"
									}
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"sourceSystem": {
						"type": "string",
						"defaultValue": "contoso_sis"
					},
					"tablename": {
						"type": "string",
						"defaultValue": "studentattendance"
					}
				},
				"folder": {
					"name": "OEA_Framework/Reset"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary_folder')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_file')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used for landing data in the data lake.\nDefaults to landing data in stage1np.\nNote that you can specify a full path in the filename param (eg, to land a file in a specific folder filename param can be 'contoso_sis/students/students.csv').\n",
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"filename": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary_folder')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_parquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used for landing data in the data lake as in parquet format.\nDefaults to landing data in stage1np.\nNote that you cannot specify a filename because with parquet the filename should be auto-generated.\n",
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"directory": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_Azure_SQL_DB",
					"type": "LinkedServiceReference",
					"parameters": {
						"dbServer": {
							"value": "@dataset().dbServer",
							"type": "Expression"
						},
						"dbName": {
							"value": "@dataset().dbName",
							"type": "Expression"
						},
						"userName": {
							"value": "@dataset().userName",
							"type": "Expression"
						},
						"keyVaultSecretName": {
							"value": "@dataset().keyVaultSecretName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "myserver.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "testdb"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqlAdmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqlAdminPwd"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_Azure_SQL_DB')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_HTTP_binary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Retrieves data from an http endpoint.\nThe data can be in any format - the binary dataset allows us to pull any payload without affecting it.",
				"linkedServiceName": {
					"referenceName": "LS_HTTP",
					"type": "LinkedServiceReference",
					"parameters": {
						"baseURL": {
							"value": "@dataset().URL",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"URL": {
						"type": "string"
					}
				},
				"folder": {
					"name": "OEA_Framework"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_HTTP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EdFi_Ods_Production')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('EdFi_Ods_Production_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_ADLS_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Connection to the OEA data lake",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('LS_ADLS_OEA_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_Azure_SQL_DB')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Allows for connecting to an Azure SQL database using SQL authentication and retrieving the user password from the key vault.",
				"parameters": {
					"dbServer": {
						"type": "string",
						"defaultValue": "myserver.database.windows.net"
					},
					"dbName": {
						"type": "string",
						"defaultValue": "testdb"
					},
					"userName": {
						"type": "string",
						"defaultValue": "sqlAdmin"
					},
					"keyVaultSecretName": {
						"type": "string",
						"defaultValue": "sqlAdminPwd"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('LS_Azure_SQL_DB_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "LS_KeyVault_OEA",
							"type": "LinkedServiceReference"
						},
						"secretName": {
							"value": "@linkedService().keyVaultSecretName",
							"type": "Expression"
						}
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_KeyVault_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_HTTP')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Connection to an HTTP endpoint.\nThe baseURL parameter must be passed in from the dataset that utilizes this linked service.",
				"parameters": {
					"baseURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('LS_HTTP_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_KeyVault_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('LS_KeyVault_OEA_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQL_Serverless_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"databaseName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('LS_SQL_Serverless_OEA_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bing-covid-19-data')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('bing-covid-19-data_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-cisd3gg1-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('syn-oea-cisd3gg1-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-cisd3gg1-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('syn-oea-cisd3gg1-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [id]\n,[student_id_pseudonym]\n,[school_year]\n,[school_id]\n,[attendance_date]\n,[all_day]\n,[attendance_sequence]\n,[Period]\n,[section_id]\n,[AttendanceCode]\n,[PresenceFlag]\n,[attendance_status]\n,[attendance_type]\n FROM [dbo].[studentattendance]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sql_s2_contoso_sis",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [student_id]\n,[school_year]\n,[student_id_pseudonym]\n FROM [dbo].[studentattendance]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqls2_contoso_sis",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage2p/contoso_sis/studentsectionmark_pseudo/school_year=2021/part-00000-cbfae834-e953-4401-8441-6d70f0501009.c000.snappy.parquet',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create_sql_db')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- 1) Create a db via SQL serverless pool\nCREATE DATABASE s2sql_contoso_sis;\nGO\nUSE s2sql_contoso_sis;\nGO\n\n-- Always use UTF-8 collations.\n-- https://techcommunity.microsoft.com/t5/azure-synapse-analytics/always-use-utf-8-collations-to-read-utf-8-text-in-serverless-sql/ba-p/1883633\nALTER DATABASE s2sql_contoso_sis COLLATE Latin1_General_100_BIN2_UTF8;\n\nDROP VIEW IF EXISTS studentattendance;\nGO\nCREATE VIEW studentattendance AS\nSELECT * FROM OPENROWSET(\n    BULK 'contoso_sis/studentattendance',\n    DATA_SOURCE='stage2p',\n    FORMAT='DELTA'\n) AS [r]\nGO\n\nDROP VIEW IF EXISTS studentattendance;\nGO\nCREATE VIEW studentattendance AS\nSELECT * FROM OPENROWSET(\n    BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage2p/contoso_sis/studentattendance',\n    FORMAT='DELTA'\n) AS [r]\nGO\n\n\n-- 2) set location info\nCREATE EXTERNAL DATA SOURCE stage2p WITH ( location = 'https://stoeacisd3gg1.dfs.core.windows.net/stage2p' );\nCREATE EXTERNAL DATA SOURCE stage2np WITH ( location = 'https://stoeacisd3gg1.dfs.core.windows.net/stage2np' );\nGO\nCREATE EXTERNAL FILE FORMAT DeltaLakeFormat WITH ( FORMAT_TYPE = DELTA );\nGO\n\n-- 3) Create an external table\n-- for info on data types: https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver15\n-- Note: this does not work for the columns used in partitioning the data because you always get null values (in this example you always get null for year and month). \n-- This is a known limitation: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#partitioning-column-returns-null-values\nCREATE EXTERNAL TABLE studentattendance () WITH (\n    LOCATION = '/contoso_sis/studentattendance',\n    DATA_SOURCE = stage2p,\n    FILE_FORMAT = DeltaLakeFormat\n);\n\n\nselect * from TechActivity where year is not null",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sql_s2_contoso_sis",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ms_TechActivity')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Limitations of SQL Serverless with Delta Lake: \n-- https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n\n\n-- 0) Get info on inferred data types\n-- Notes: When querying the delta lake via T-SQL, the data types will be inferred based on what is specified in parquet.\n--        Parquet does not give you a way to specify column size, so strings are inferred as varchar(8000) which is not optimal.\nEXEC sp_describe_first_result_set N'\n\tSELECT * FROM \n    OPENROWSET(\n        BULK ''https://stoeacisd3ggimpl3.dfs.core.windows.net/stage2/ms_insights/TechActivity'',\n        FORMAT=''DELTA''\n    ) AS result';\n\n\n-- 1) Create a db via SQL serverless pool\nCREATE DATABASE s2_ms_insights;\nGO\nUSE s2_ms_insights;\nGO\n\n-- Always use UTF-8 collations.\n-- https://techcommunity.microsoft.com/t5/azure-synapse-analytics/always-use-utf-8-collations-to-read-utf-8-text-in-serverless-sql/ba-p/1883633\nALTER DATABASE s2_ms_insights \n    COLLATE Latin1_General_100_BIN2_UTF8;\n\n-- 2) set location info\nCREATE EXTERNAL DATA SOURCE DeltaLakeStorage\nWITH ( location = 'https://stoeacisd3ggimpl3.dfs.core.windows.net/stage2' );\nGO\nCREATE EXTERNAL FILE FORMAT DeltaLakeFormat WITH ( FORMAT_TYPE = DELTA );\nGO\n\n-- 3) Create an external table\n-- for info on data types: https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver15\n-- Note: this does not work for the columns used in partitioning the data because you always get null values (in this example you always get null for year and month). \n-- This is a known limitation: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#partitioning-column-returns-null-values\nCREATE EXTERNAL TABLE TechActivity (\n     SignalType varchar(200),\n     StartTime datetime2,\n     UserAgent varchar(200),\n     SignalId varchar(200),\n     SisClassId varchar(200),\n     ClassId varchar(200),\n     ChannelId varchar(200),\n     AppName varchar(200),\n     ActorId varchar(200),\n     ActorRole varchar(200),\n     SchemaVersion varchar(50),\n     AssignmentId varchar(200),\n     SubmissionId varchar(200),\n     [Action] varchar(200),\n     DueDate datetime2,\n     ClassCreationDate datetime2,\n     Grade varchar(50),\n     SourceFileExtension varchar(50),\n     MeetingDuration int,\n     [year] smallint,\n     [month] tinyint\n) WITH (\n    LOCATION = '/ms_insights/TechActivity',\n    DATA_SOURCE = DeltaLakeStorage,\n    FILE_FORMAT = DeltaLakeFormat\n);\n\n\nselect * from TechActivity where year is not null\n\n--EXEC sp_describe_first_result_set N'\n\t--SELECT * FROM TechActivity AS result';\n\ndrop external table TechActivity2\n\nCREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET );\nGO\n\n-- This will perform the select and copy the data to the specified LOCATION\nCREATE EXTERNAL TABLE TechActivity2\n  WITH (\n    LOCATION = '/ms_insights/TechActivity2',\n    DATA_SOURCE = DeltaLakeStorage,\n    FILE_FORMAT = ParquetFormat\n)\n  AS select \n     [SignalType] VARCHAR (200),\n     StartTime\n     from \n        OPENROWSET(BULK 'https://stoeacisd3ggimpl3.dfs.core.windows.net/stage2/ms_insights/TechActivity/year=2021/**',\n        FORMAT='PARQUET') AS [r]\n\n\n\n-- Create a partitioned view\n-- https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-views#delta-lake-partitioned-views\nCREATE OR ALTER VIEW TechActivityView\nAS SELECT \n    CAST(SignalType as varchar(200)) as SignalType,\n    CAST(StartTime as datetime2) as StartTime,\n    CAST(UserAgent as varchar(200)) as UserAgent,\n    CAST(SignalId as varchar(200)) as SignalId,\n    CAST(SisClassId as varchar(200)) as SisClassId,\n    CAST(ClassId as varchar(200)) as ClassId,\n    CAST(ChannelId as varchar(200)) as ChannelId,\n    CAST(AppName as varchar(200)) as AppName,\n    CAST(ActorId as varchar(200)) as ActorId,\n    CAST(ActorRole as varchar(200)) as ActorRole,\n    CAST(SchemaVersion as varchar(50)) as SchemaVersion,\n    CAST(AssignmentId as varchar(200)) as AssignmentId,\n    CAST(SubmissionId as varchar(200)) as SubmissionId,\n    CAST([Action] as varchar(200)) as [Action],\n    CAST(DueDate as datetime2) as DueDate,\n    CAST(ClassCreationDate as datetime2) as ClassCreationDate,\n    CAST(Grade as varchar(50)) as Grade,\n    CAST(SourceFileExtension as varchar(50)) as SourceFileExtension,\n    CAST(MeetingDuration as int) as MeetingDuration,\n    CAST([year] as smallint) as [year],\n    CAST([month] as tinyint) as [month]\nFROM  \n    OPENROWSET(\n        BULK 'ms_insights/TechActivity',\n        DATA_SOURCE = 'DeltaLakeStorage',\n        FORMAT='DELTA'\n    ) AS [r]\n\nSELECT count(*) FROM TechActivityView\n\ndrop view TechActivityView2\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/query_delta')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage2/tutorial_01/Person',\n        FORMAT='DELTA'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tutorial_01')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Tutorial on use of SQL serverless pool.\n-- For more info: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview\n\n-- 1) Create a db via SQL serverless pool\nCREATE DATABASE tutorial_sql_01;\nGO\nUSE tutorial_sql_01;\nGO\n\nCREATE EXTERNAL DATA SOURCE DeltaLakeStorage\nWITH ( location = 'https://stoeacisd3gg1.dfs.core.windows.net/stage2' );\nGO\nCREATE EXTERNAL FILE FORMAT DeltaLakeFormat WITH ( FORMAT_TYPE = DELTA );\nGO\n\n-- 2) Create an external table\n-- for info on data types: https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver15\nCREATE EXTERNAL TABLE Person (\n     Id varchar(50),\n     FirstSeenDateTime datetime,\n     LastSeenDateTime datetime,\n     GivenName varchar(70),\n     MiddleName varchar(70),\n     PreferredGivenName varchar(70),\n     PreferredMiddleName varchar(70),\n     PreferredSurname varchar(70),\n     Surname varchar(70)\n) WITH (\n    LOCATION = '/tutorial_01/Person',\n    DATA_SOURCE = DeltaLakeStorage,\n    FILE_FORMAT = DeltaLakeFormat\n);\n\n-- 3) Create a view\ncreate or alter view PersonView\nas\nselect *\nfrom openrowset(\n           bulk '/tutorial_01/Person',\n           data_source = 'DeltaLakeStorage',\n           format = 'delta'\n    ) with (\n        Id varchar(50),\n        FirstSeenDateTime datetime,\n        LastSeenDateTime datetime,\n        GivenName varchar(70),\n        MiddleName varchar(70),\n        PreferredGivenName varchar(70),\n        PreferredMiddleName varchar(70),\n        PreferredSurname varchar(70),\n        Surname varchar(70)\n        ) as rows\n\n-- 4) use CETAS to export select statement with OPENROWSET result to  storage\n-- more info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas\n-- Note: CETAS does not support writing out as Delta Lake format\nCREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET );\n\nCREATE EXTERNAL TABLE PersonAthruE\nWITH (\n    LOCATION = '/tutorial_01/PersonAthruE',\n    DATA_SOURCE = DeltaLakeStorage,\n    FILE_FORMAT = ParquetFormat\n)  \nAS\nSELECT *\nFROM\n    OPENROWSET(\n           bulk '/tutorial_01/Person',\n           data_source = 'DeltaLakeStorage',\n           format = 'delta'\n    ) AS [r]\nWHERE Surname LIKE '[A-E]%'\nGO\n\n\n-- 5) Create stored procs (note that you won't see the stored proc listed in Synapse studio, but you will see it in SSMS)\n-- more info: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-stored-procedures\nCREATE PROCEDURE drop_external_table_if_exists @name SYSNAME\nAS BEGIN\n    IF (0 <> (SELECT COUNT(*) FROM sys.external_tables WHERE name = @name))\n    BEGIN\n        DECLARE @drop_stmt NVARCHAR(200) = N'DROP EXTERNAL TABLE ' + @name; \n        EXEC sp_executesql @tsql = @drop_stmt;\n    END\nEND\n\n-- execute the stored proc\nEXEC drop_external_table_if_exists PersonAthruE\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "tutorial_sql_02",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Clever_s1_to_s2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 0,
				"nbformat_minor": 0,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Setup Clever data\n",
							"\n",
							"\n",
							"\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\n",
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\n",
							"stage_anon = 'abfss://stage-anon@' + storage_account + '.dfs.core.windows.net'\n",
							"\n",
							"folder_to_process = 'inbound'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/Clever/' + folder_to_process, header='true')\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/Clever')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Contoso ISD')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Packages"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ContosoISD_example')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark4v3p1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 4,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "4",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "16a64b45-d245-499f-a13a-e0515eab592b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark4v3p1",
						"name": "spark4v3p1",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark4v3p1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ContosoISD Example\n",
							"This example demonstrates how to use the OEA framework and modules to process incoming data, perform data prep, and view the data in an example Power BI dashboard.\n",
							"\n",
							"# Running the example\n",
							"1) Run the Copy_Contoso_test_data pipeline in order to land test data in your data lake. Do this by clicking on \"Integrate\" in the left navigation, then click on Copy_Contoso_test_data, then click on \"Add Trigger\" and then \"Trigger Now\".\n",
							"\n",
							"2) Click on \"Run all\" at the top of this tab (and wait for the processing to complete - which can take around 5 to 10 minutes).\n",
							"\n",
							"3) Open the dashboard in Power BI desktop and point it to your newly setup data lake (you can download the pbix from here: [techInequityDashboardContoso v2.pbix](https://github.com/microsoft/OpenEduAnalytics/blob/main/packages/ContosoISD/power_bi/techInequityDashboardContoso%20v2.pbix) )\n",
							"\n",
							"# More info\n",
							"See [OEA Solution Guide](https://github.com/microsoft/OpenEduAnalytics/blob/main/docs/OpenEduAnalyticsSolutionGuide.pdf) for more details on this example."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /example_modules_py"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 1) Initialize the OEA framework and modules needed.\n",
							"oea = OEA()\n",
							"m365 = M365(oea)\n",
							"contoso_sis = ContosoSIS(oea)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df = spark.read.load(f'{oea.stage2p}/contoso_sis/studentattendance', format='delta')\r\n",
							"#display(df)\r\n",
							"df.schema\r\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 2) Process the raw data (csv format) from stage1 into stage2 (adds schema details and writes out in parquet format).\n",
							"#    [Note: we're not performing pseudonymization in this example, so everything is written to container stage2np.]\n",
							"m365.process_latest_roster_from_stage1()\n",
							"contoso_sis.process_latest_from_stage1()\n",
							"m365.process_activity_data_from_stage1()"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 3) Run additional prep on the data to create a unified dataset that can be used in a Power BI report\n",
							"\n",
							"# Process sectionmark data. Convert id values to use the Person.Id and Section.Id values set in the m365 data.\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2p + '/contoso_sis/studentsectionmark'), 'SectionMark')\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/m365/Person'), 'Person')\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/m365/Section'), 'Section')\n",
							"df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\n",
							"sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\n",
							"sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\n",
							"from SectionMark sm, Person p, Section s \\\n",
							"where sm.student_id = p.ExternalId \\\n",
							"and sm.section_id = s.ExternalId\")\n",
							"df.write.format('delta').mode('overwrite').save(oea.stage2np + '/ContosoISD/SectionMark')\n",
							"\n",
							"# Repeat the above process, this time for student attendance\n",
							"# Convert id values to use the Person.Id, Org.Id and Section.Id values\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/contoso_sis/studentattendance'), 'Attendance')\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/m365/Org'), 'Org')\n",
							"df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\n",
							"att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\n",
							"att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\n",
							"from Attendance att, Org o, Person p, Section s \\\n",
							"where att.student_id = p.ExternalId \\\n",
							"and att.school_id = o.ExternalId \\\n",
							"and att.section_id = s.ExternalId\")\n",
							"df.write.format('delta').mode('overwrite').save(oea.stage2np +'/ContosoISD/Attendance')\n",
							"\n",
							"# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/m365/Course'), 'Course')\n",
							"df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\n",
							"df.write.format('delta').mode('overwrite').save(oea.stage2np + '/ContosoISD/Course')"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 4) Create spark db's that point to the data in the data lake to allow for connecting via Power BI through use of the Serverless SQL endpoint.\n",
							"contoso_sis.create_stage2_db('PARQUET')\n",
							"m365.create_stage2_db('PARQUET')\n",
							"\n",
							"spark.sql('CREATE DATABASE IF NOT EXISTS s2_ContosoISD')\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Activity using PARQUET location '\" + oea.stage2np + \"/m365/TechActivity'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Calendar using PARQUET location '\" + oea.stage2np + \"/m365/Calendar'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Org using PARQUET location '\" + oea.stage2np + \"/m365/Org'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Person using PARQUET location '\" + oea.stage2np + \"/m365/Person'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.PersonIdentifier using PARQUET location '\" + oea.stage2np + \"/m365/PersonIdentifier'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.RefDefinition using PARQUET location '\" + oea.stage2np + \"/m365/RefDefinition'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Section using PARQUET location '\" + oea.stage2np + \"/m365/Section'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Session using PARQUET location '\" + oea.stage2np + \"/m365/Session'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StaffOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StaffOrgAffiliation'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StaffSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StaffSectionMembership'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StudentOrgAffiliation'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StudentSectionMembership'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Course using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Course'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Attendance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Attendance'\")\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.SectionMark using PARQUET location '\" + oea.stage2np + \"/ContosoISD/SectionMark'\")\n",
							"\n",
							"print(f\"Created spark db's.\\nYou can now open the 'techInequityDashboardContoso v2.pbix' dashboard and change the datasource to point to: {oea.serverless_sql_endpoint}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything\n",
							"You can uncomment line 11 in the cell below and run the cell to reset everything and walk through the process again from the top (starting from running the pipeline to bring test data into stage1np).\n",
							"\n",
							"Note: remember to comment out line 11 again to prevent accidental resetting of the example"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def reset_all_processing():\n",
							"    contoso_sis.delete_all_stages()\n",
							"    m365.delete_all_stages()\n",
							"    oea.rm_if_exists(oea.stage2np + '/ContosoISD')\n",
							"\n",
							"    oea.drop_db('s2_contoso_sis')\n",
							"    oea.drop_db('s2_contosoisd')\n",
							"    oea.drop_db('s2_m365')\n",
							"\n",
							"# Uncomment the following line and run this cell to reset everything if you want to walk through the process again.\n",
							"#reset_all_processing()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ContosoSIS_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "72fa1011-c5ad-4f1e-983a-7d1df8a3bbd1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class ContosoSIS(BaseOEAModule):\r\n",
							"    def __init__(self, source_folder='contoso_sis', pseudonymize = True):\r\n",
							"        BaseOEAModule.__init__(self, source_folder, pseudonymize)\r\n",
							"        self.schemas['studentattendance'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash'],\r\n",
							"                                            ['school_year', 'integer', 'partition-by'],\r\n",
							"                                            ['school_id', 'string', 'no-op'],\r\n",
							"                                            ['attendance_date', 'timestamp', 'no-op'],\r\n",
							"                                            ['all_day', 'string', 'no-op'],\r\n",
							"                                            ['Period', 'short', 'no-op'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['AttendanceCode', 'string', 'no-op'],\r\n",
							"                                            ['PresenceFlag', 'boolean', 'no-op'],\r\n",
							"                                            ['attendance_status', 'string', 'no-op'],\r\n",
							"                                            ['attendance_type', 'string', 'no-op'],\r\n",
							"                                            ['attendance_sequence', 'short', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['studentsectionmark'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['school_year', 'string', 'partition-by'],\r\n",
							"                                            ['term_id', 'string', 'no-op'],\r\n",
							"                                            ['numeric_grade_earned', 'short', 'no-op'],\r\n",
							"                                            ['alpha_grade_earned', 'string', 'no-op'],\r\n",
							"                                            ['is_final_grade', 'string', 'no-op'],\r\n",
							"                                            ['credits_attempted', 'short', 'no-op'],\r\n",
							"                                            ['credits_earned', 'short', 'no-op'],\r\n",
							"                                            ['grad_credit_type', 'string', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['studentdemographics'] = [['SIS ID', 'string', 'hash'],\r\n",
							"                                            ['FederalRaceCategory', 'string', 'no-op'],\r\n",
							"                                            ['PrimaryLanguage', 'string', 'no-op'],\r\n",
							"                                            ['ELLStatus', 'string', 'no-op'],\r\n",
							"                                            ['SpecialEducation', 'string', 'no-op'],\r\n",
							"                                            ['LowIncome', 'boolean', 'no-op']]                                            \r\n",
							"\r\n",
							"    def ingest(self):\r\n",
							"        oea.ingest_incremental_csv_data(self.source_folder, 'studentattendance', self.schemas['studentattendance'], 'school_year', 'id')\r\n",
							"        oea.ingest_incremental_csv_data(self.source_folder, 'studentsectionmark', self.schemas['studentsectionmark'], 'school_year', 'id')\r\n",
							"        #oea.ingest_snapshot_csv_data(self.source_folder, 'studentsectionmark', self.schemas['studentsectionmark'], 'school_year', 'id')\r\n",
							"        #oea.ingest_delta_csv_data(self.source_folder, 'studentdemographics', self.schemas['studentdemographics'])\r\n",
							"\r\n",
							"contoso_sis = ContosoSIS()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Lake Map')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1092c832-611f-4cff-98bc-8932583cd799"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark1",
						"name": "spark1",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"![OEA](https://openeducationanalytics.org/assets/imgs/img_oea_logo.png)\r\n",
							"![image-alt-text](https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/docs/pics/diagrams/OEA_ref_arch_v0.5.png)\r\n",
							"\r\n",
							"&nbsp\r\n",
							"&nbsp\r\n",
							"\r\n",
							"---\r\n",
							"\r\n",
							"# Data Lake Map\r\n",
							"This notebook provides documentation on the structure of the data lake and details on what data is brought in and how it's processed.\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# M365\r\n",
							"\r\n",
							"M365 data is ingested through the Insights Premium product, and setup via the SDS admin pages.\r\n",
							"\r\n",
							"## Roster data\r\n",
							"This is info about how roster data is received and processed.\r\n",
							"\r\n",
							"## Activity data\r\n",
							"This is info about how roster data is received and processed.\r\n",
							"\r\n",
							"# Test2"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Test\r\n",
							"this is a test"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataGen_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5c59ce2c-5327-4f9f-81b9-dd4a86b21a02"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import random\r\n",
							"import json\r\n",
							"from faker import Faker\r\n",
							"\r\n",
							"\"\"\" From DataGenUtil.py \"\"\"\r\n",
							"def list_of_dict_to_csv(list_of_dict, includeHeaders = True):\r\n",
							"    csv_str = ''\r\n",
							"    if includeHeaders == True:\r\n",
							"        header = []\r\n",
							"        for column_name in list_of_dict[0].keys(): \r\n",
							"            if not column_name.startswith('_'): header.append(column_name)\r\n",
							"        csv_str += \",\".join(header) + \"\\n\"\r\n",
							"\r\n",
							"    for row in list_of_dict:\r\n",
							"        csv_str += obj_to_csv(row) + \"\\n\"\r\n",
							"\r\n",
							"    return csv_str\r\n",
							"\r\n",
							"def obj_to_csv(obj):\r\n",
							"    csv = ''\r\n",
							"    for key in obj:\r\n",
							"        if not (key.startswith('_')): csv += str(obj[key]) + ','\r\n",
							"    return csv[:-1]\r\n",
							"\r\n",
							"def list_of_dict_to_json(list_of_dict):\r\n",
							"    json_str = '['\r\n",
							"    for row in list_of_dict:\r\n",
							"        json_str += obj_to_json(row) + \",\\n\"\r\n",
							"    return json_str[:-2] + ']'\r\n",
							"\r\n",
							"def obj_to_json(obj):\r\n",
							"    json_dict = {}\r\n",
							"    for key in obj:\r\n",
							"        if not (key.startswith('_')): json_dict[key] = obj[key]\r\n",
							"    return json.dumps(json_dict)\r\n",
							"\r\n",
							"\"\"\" From EdFiDataGenerator.py \"\"\"\r\n",
							"GENDER = ['Male','Female']\r\n",
							"BOOLEAN = [True, False]\r\n",
							"OPERATIONAL_STATUS = ['Active','Inactive']\r\n",
							"CHARTER_STATUS = ['School Charter', 'Open Enrollment Charter', 'Not a Charter School']\r\n",
							"GRADE_LEVEL = ['First Grade','Second Grade','Third Grade','Fourth Grade','Fifth Grade','Sixth Grade','Seventh Grade','Eighth Grade','Ninth Grade','Tenth Grade','Eleventh Grade','Twelfth Grade']\r\n",
							"SCHOOL_TYPES = ['High School', 'Middle School', 'Elementary School']\r\n",
							"SUBJECT_NAMES = [('Math','Algebra'), ('Math','Geometry'), ('Language','English'), ('History','World History'),('Science','Biology'), ('Science','Health'), ('Technology',' Programming'), ('Physical Education','Sports'), ('Arts','Music')]\r\n",
							"LEVELS_OF_EDUCATION = ['Some College No Degree', 'Doctorate', 'Bachelor\\'s','Master\\'s']\r\n",
							"PERSONAL_INFORMATION_VERIFICATION_DESCRIPTIONS = ['Entry in family Bible', 'Other official document', 'State-issued ID', 'Hospital certificate', 'Passport', 'Parents affidavit', 'Immigration document/visa', 'Drivers license']\r\n",
							"RACES = ['Asian' , 'Native Hawaiian - Pacific Islander', 'American Indian - Alaska Native', 'White']\r\n",
							"\r\n",
							"class EdFiDataGenerator:\r\n",
							"    def __init__(self,number_students_per_school=100, include_optional_fields=True, school_year='2021', credit_conversion_factor = 2.0, number_of_grades_per_school = 5, is_current_school_year = True, graduation_plans_per_school = 10, unique_id_length = 5, number_staffs_per_school = 50, number_sections_per_school = 10):\r\n",
							"        # Set a seed value in Faker so it generates same values every run.\r\n",
							"        self.faker = Faker('en_US')\r\n",
							"        Faker.seed(1)\r\n",
							"\r\n",
							"        self.include_optional_fields = include_optional_fields\r\n",
							"        self.graduation_plans_per_school = graduation_plans_per_school\r\n",
							"        self.school_year = school_year\r\n",
							"        self.country = 'United States of America'\r\n",
							"        self.number_students_per_school = number_students_per_school\r\n",
							"        self.credit_conversion_factor = credit_conversion_factor\r\n",
							"        self.number_of_grades_per_school = number_of_grades_per_school\r\n",
							"        self.is_current_school_year = is_current_school_year\r\n",
							"        self.unique_id_length = unique_id_length\r\n",
							"        self.number_staffs_per_school = number_staffs_per_school\r\n",
							"        self.number_sections_per_school = number_sections_per_school\r\n",
							"\r\n",
							"    def get_descriptor_string(self, key, value):\r\n",
							"        return \"uri://ed-fi.org/{}#{}\".format(key,value)\r\n",
							"\r\n",
							"    def generate_data(self, num_of_schools, writer):\r\n",
							"        edfi_data = [self.create_school() for _ in range(num_of_schools)]\r\n",
							"        edfi_data_formatted = self.format_edfi_data(edfi_data)\r\n",
							"\r\n",
							"\r\n",
							"        writer.write(f'EdFi/School.json',list_of_dict_to_json(edfi_data_formatted['Schools']))\r\n",
							"        writer.write(f'EdFi/Student.json',list_of_dict_to_json(edfi_data_formatted['Students']))\r\n",
							"        writer.write(f'EdFi/StudentSchoolAssociation.json',list_of_dict_to_json(edfi_data_formatted['StudentSchoolAssociations']))\r\n",
							"        writer.write(f'EdFi/Course.json',list_of_dict_to_json(edfi_data_formatted['Courses']))\r\n",
							"        writer.write(f'EdFi/Calendar.json',list_of_dict_to_json(edfi_data_formatted['Calendars']))\r\n",
							"        writer.write(f'EdFi/Sessions.json',list_of_dict_to_json(edfi_data_formatted['Sessions']))\r\n",
							"        writer.write(f'EdFi/StaffSchoolAssociations.json',list_of_dict_to_json(edfi_data_formatted['StaffSchoolAssociations']))\r\n",
							"        writer.write(f'EdFi/Sections.json',list_of_dict_to_json(edfi_data_formatted['Sections']))\r\n",
							"        writer.write(f'EdFi/Staffs.json',list_of_dict_to_json(edfi_data_formatted['Staffs']))\r\n",
							"        writer.write(f'EdFi/StudentSectionAssociations.json',list_of_dict_to_json(edfi_data_formatted['StudentSectionAssociations']))\r\n",
							"        writer.write(f'EdFi/StaffSectionAssociations.json',list_of_dict_to_json(edfi_data_formatted['StaffSectionAssociations']))\r\n",
							"\r\n",
							"\r\n",
							"    def create_school(self):\r\n",
							"        school_type = random.choice(SCHOOL_TYPES)\r\n",
							"        school_name = self.faker.city() + ' ' + school_type\r\n",
							"        school = {\r\n",
							"            'Id': self.faker.uuid4().replace('-',''),\r\n",
							"            'SchoolId': self.faker.random_number(digits = self.unique_id_length),\r\n",
							"            'NameOfInstitution': school_name,\r\n",
							"            'OperationalStatusDescriptor': self.get_descriptor_string('OperationalStatusDescriptor',random.choice(OPERATIONAL_STATUS)),\r\n",
							"            'ShortNameOfInstitution': ''.join([word[0] for word in school_name.split()]),\r\n",
							"            'Website':''.join(['www.',school_name.lower().replace(' ',''),'.com']),\r\n",
							"            'AdministrativeFundingControlDescriptor': self.get_descriptor_string('AdministrativeFundingControlDescriptor',random.choice(['public', 'private']) + ' School'),\r\n",
							"            'CharterStatusDescriptor': self.get_descriptor_string('CharterStatusDescriptor',random.choice(CHARTER_STATUS)),\r\n",
							"            'SchoolTypeDescriptor': self.get_descriptor_string('SchoolTypeDescriptor','Regular'),\r\n",
							"            'TitleIPartASchoolDesignationDescriptor': self.get_descriptor_string('TitleIPartASchoolDesignationDescriptor','Not A Title I School'),\r\n",
							"            'Addresses': self.create_address() if self.include_optional_fields else '',\r\n",
							"            'EducationOrganizationCategories':[{'EducationOrganizationCategoryDescriptor': self.get_descriptor_string('educationOrganizationCategoryDescriptor','School')}],\r\n",
							"            'IdentificationCodes': [\r\n",
							"                {\r\n",
							"                    'educationOrganizationIdentificationSystemDescriptor': self.get_descriptor_string('educationOrganizationIdentificationSystemDescriptor','SEA'),\r\n",
							"                    'identificationCode': self.faker.random_number(digits=10)\r\n",
							"                }\r\n",
							"            ],\r\n",
							"            'InstitutionTelephones': self.create_telephones(),\r\n",
							"            'InternationalAddresses': [],\r\n",
							"            'SchoolCategories': [\r\n",
							"                {\r\n",
							"                    'SchoolCategoryDescriptor': self.get_descriptor_string('SchoolCategoryDescriptor',school_type)\r\n",
							"                }\r\n",
							"            ],\r\n",
							"            'gradeLevels': [\r\n",
							"                {'gradeLevelDescriptor': self.get_descriptor_string('GradeLevelDescriptor',random.choice(GRADE_LEVEL))} for _ in range(4)\r\n",
							"            ]\r\n",
							"        }\r\n",
							"\r\n",
							"        school['_SchoolYears'] = self.create_school_years()\r\n",
							"        school['_Calendars'] = self.create_calendars(school)\r\n",
							"        school['_Students'] = self.create_students()\r\n",
							"        school['_Courses'] = self.create_courses(school['SchoolId'],school['Id'],school_name)\r\n",
							"        school['_GraduationPlans'] = self.create_graduation_plans(school)\r\n",
							"        school['_StudentAssociations'] = self.create_student_school_associations(school)\r\n",
							"        school['_Staffs'] = self.create_staffs()\r\n",
							"        school['_StaffSchoolAssociations'] = self.create_staff_school_associations(school)\r\n",
							"        school['_Sessions'] = self.create_sessions(school)\r\n",
							"        school['_Sections'] = self.create_sections(school)\r\n",
							"        school['_StaffSectionAssociations'] = self.create_staff_section_associations(school)\r\n",
							"        school['_StudentSectionAssociations'] = self.create_student_section_associations(school)\r\n",
							"        return school\r\n",
							"\r\n",
							"    def create_students(self):\r\n",
							"        students = []\r\n",
							"        for _ in range(self.number_students_per_school):\r\n",
							"            gender = random.choice(GENDER)\r\n",
							"            fname = self.faker.first_name_male() if gender == 'Male' else self.faker.first_name_female()\r\n",
							"            students.append({\r\n",
							"                'Id': self.faker.uuid4().replace('-',''),\r\n",
							"                'StudentUniqueId': self.faker.random_number(digits = self.unique_id_length),\r\n",
							"                \"BirthCity\": self.faker.city(),\r\n",
							"                \"BirthDate\": str(self.faker.date_between(start_date='-18y',end_date='-5y')),\r\n",
							"                \"BirthSexDescriptor\": self.get_descriptor_string('birthStateAbbreviationDescriptor', gender),\r\n",
							"                \"FirstName\": fname,\r\n",
							"                \"IdentificationDocuments\": [],\r\n",
							"                \"LastSurname\": self.faker.last_name(),\r\n",
							"                \"OtherNames\": [\r\n",
							"                    {\r\n",
							"                        \"OtherNameTypeDescriptor\": self.get_descriptor_string('otherNameTypeDescriptor','Nickname'),\r\n",
							"                        \"FirstName\": self.faker.first_name_male() if gender == 'Male' else self.faker.first_name_female(),\r\n",
							"                        \"PersonalTitlePrefix\": 'Mr' if gender == 'Male' else 'Ms'\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"PersonalIdentificationDocuments\": [],\r\n",
							"                \"PersonalTitlePrefix\": 'Mr' if gender == 'Male' else 'Ms',\r\n",
							"                \"Visas\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"        })\r\n",
							"        return students\r\n",
							"\r\n",
							"\r\n",
							"    def create_student_school_associations(self,school):\r\n",
							"        result = []\r\n",
							"        graduation_plan_ids = [gp['Id'] for gp in school['_GraduationPlans']]\r\n",
							"        for student in school['_Students']:\r\n",
							"            result.append({\r\n",
							"                'Id': self.faker.uuid4().replace('-',''),\r\n",
							"                \"GraduationPlanReference\": {\r\n",
							"                    \"EducationOrganizationId\": school['SchoolId'],\r\n",
							"                    \"GraduationPlanTypeDescriptor\": \"uri://ed-fi.org/GraduationPlanTypeDescriptor#Minimum\",\r\n",
							"                    \"GraduationSchoolYear\": self.school_year,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GraduationPlan\",\r\n",
							"                        \"href\": '/ed-fi/graduationPlans/{}'.format(random.choice(graduation_plan_ids))\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"SchoolReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": '/ed-fi/schools/{}'.format(school['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"StudentReference\": {\r\n",
							"                    \"StudentUniqueId\": student['StudentUniqueId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Student\",\r\n",
							"                        \"href\": \"/ed-fi/students/{}\".format(student['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"EntryDate\": str(self.faker.date_between(start_date='-5y',end_date='today')),\r\n",
							"                \"EntryGradeLevelDescriptor\": \"uri://ed-fi.org/GradeLevelDescriptor#{}\".format(random.choice(GRADE_LEVEL)),\r\n",
							"                \"AlternativeGraduationPlans\": [],\r\n",
							"                \"EducationPlans\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return result\r\n",
							"\r\n",
							"    def create_calendars(self,school):\r\n",
							"        return {\r\n",
							"            'Id': self.faker.uuid4().replace('-',''),\r\n",
							"            'CalendarCode':self.faker.random_number(digits = self.unique_id_length),\r\n",
							"            \"SchoolReference\": {\r\n",
							"                \"SchoolId\": school['SchoolId'],\r\n",
							"                \"Link\": {\r\n",
							"                    \"rel\": \"School\",\r\n",
							"                    \"href\": \"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SchoolYearTypeReference\": {\r\n",
							"                \"SchoolYear\": self.school_year,\r\n",
							"                \"Link\": {\r\n",
							"                    \"rel\": \"SchoolYearType\",\r\n",
							"                    \"href\": \"/ed-fi/schoolYearTypes/{}\".format(school['_SchoolYears']['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            'CalendarTypeDescriptor': self.get_descriptor_string('calendarTypeDescriptor','Student Specific'),\r\n",
							"            'GradeLevel': []\r\n",
							"        }\r\n",
							"\r\n",
							"    def create_address(self):\r\n",
							"        address = []\r\n",
							"        state = self.faker.state_abbr()\r\n",
							"        for n in ['Physical', 'Mailing']:\r\n",
							"            address.append({\r\n",
							"                'AddressType':n,\r\n",
							"                'City':self.faker.city(),\r\n",
							"                'PostalCode':self.faker.postcode(),\r\n",
							"                'StateAbbreviation':state,\r\n",
							"                'StreetNumberName':self.faker.street_name()\r\n",
							"            })\r\n",
							"        return address\r\n",
							"\r\n",
							"    def create_courses(self,school_id,id,school_name):\r\n",
							"        courses = []\r\n",
							"        for subject,course_name in SUBJECT_NAMES:\r\n",
							"            courseCode = '{}-{}'.format(course_name[0:3].upper(),random.choice(range(1,5)))\r\n",
							"            courses.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"EducationOrganizationReference\": {\r\n",
							"                    \"EducationOrganizationId\": school_id,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": \"/ed-fi/schools/{}\".format(id)\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"CourseCode\": courseCode,\r\n",
							"                \"AcademicSubjectDescriptor\": self.get_descriptor_string('academicSubjectDescriptor', subject),\r\n",
							"                \"CourseDefinedByDescriptor\": self.get_descriptor_string('CourseDefinedByDescriptor','SEA'),\r\n",
							"                \"CourseDescription\": 'Description about {}'.format(course_name),\r\n",
							"                \"CourseGPAApplicabilityDescriptor\": self.get_descriptor_string('CourseGPAApplicabilityDescriptor',random.choice(['Applicable','Not Applicable'])),\r\n",
							"                \"CourseTitle\": course_name,\r\n",
							"                \"HighSchoolCourseRequirement\": random.choice(BOOLEAN),\r\n",
							"                \"NumberOfParts\": 1,\r\n",
							"                \"CompetencyLevels\": [],\r\n",
							"                \"IdentificationCodes\": [\r\n",
							"                    {\r\n",
							"                        \"CourseIdentificationSystemDescriptor\": self.get_descriptor_string('CourseIdentificationSystemDescriptor','LEA course code'),\r\n",
							"                        \"CourseCatalogURL\": \"http://www.{}.edu/coursecatalog\".format(school_name.lower().replace(' ','')),\r\n",
							"                        \"IdentificationCode\": courseCode\r\n",
							"                    },\r\n",
							"                    {\r\n",
							"                        \"CourseIdentificationSystemDescriptor\": self.get_descriptor_string('CourseIdentificationSystemDescriptor','State course code'),\r\n",
							"                        \"IdentificationCode\": self.faker.random_number(digits = self.unique_id_length)\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"LearningObjectives\": [],\r\n",
							"                \"LearningStandards\": [\r\n",
							"                    {\r\n",
							"                        \"LearningStandardReference\": {\r\n",
							"                            \"LearningStandardId\": self.faker.random_number(digits = self.unique_id_length),\r\n",
							"                            \"Link\": {\r\n",
							"                                \"rel\": \"LearningStandard\",\r\n",
							"                                \"href\": \"/ed-fi/learningStandards/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                            }\r\n",
							"                        }\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"LevelCharacteristics\": [\r\n",
							"                    {\r\n",
							"                        \"CourseLevelCharacteristicDescriptor\": self.get_descriptor_string('CourseLevelCharacteristicDescriptor','Core Subject')\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"OfferedGradeLevels\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return courses\r\n",
							"\r\n",
							"\r\n",
							"    def create_graduation_plans(self, school):\r\n",
							"        graduation_plans = []\r\n",
							"        for _ in range(self.graduation_plans_per_school):\r\n",
							"            graduation_plans.append({\r\n",
							"                'Id': self.faker.uuid4().replace('-',''),\r\n",
							"                \"EducationOrganizationReference\": {\r\n",
							"                    \"EducationOrganizationId\": school['SchoolId'],\r\n",
							"                    \"link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": \"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"GraduationSchoolYearTypeReference\": {\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"SchoolYearType\",\r\n",
							"                        \"href\": \"/ed-fi/schoolYearTypes/{}\".format(school['_SchoolYears']['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"GraduationPlanTypeDescriptor\": self.get_descriptor_string('GraduationPlanTypeDescriptor', random.choice(['Minimum','Recommended'])),\r\n",
							"                \"TotalRequiredCredits\": random.choice(range(20,30)),\r\n",
							"                \"CreditsByCourses\": [],\r\n",
							"                \"CreditsByCreditCategories\": [\r\n",
							"                    {\r\n",
							"                        \"CreditCategoryDescriptor\": self.get_descriptor_string('CreditCategoryDescriptor','Honors'),\r\n",
							"                        \"Credits\": random.choice(range(5,15))\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"CreditsBySubjects\": [],\r\n",
							"                \"RequiredAssessments\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return graduation_plans\r\n",
							"\r\n",
							"    def create_school_years(self):\r\n",
							"        return {\r\n",
							"            'Id': self.faker.uuid4().replace('-',''),\r\n",
							"            'SchoolYear': self.school_year,\r\n",
							"            'CurrentSchoolYear': self.is_current_school_year,\r\n",
							"            'schoolYearDescription': 'Description about school year',\r\n",
							"            '_etag': self.faker.random_number(digits=10)\r\n",
							"        }\r\n",
							"\r\n",
							"    def create_telephones(self):\r\n",
							"        return [\r\n",
							"            {\r\n",
							"                'InstitutionTelephoneNumberTypeDescriptor': self.get_descriptor_string('InstitutionTelephoneNumberTypeDescriptor', _),\r\n",
							"                \"TelephoneNumber\": self.faker.phone_number()\r\n",
							"            }\r\n",
							"            for _ in ['Fax','Main']\r\n",
							"        ]\r\n",
							"\r\n",
							"    def create_staffs(self):\r\n",
							"        staffs = []\r\n",
							"        for _ in range(self.number_staffs_per_school):\r\n",
							"            gender = random.choice(GENDER)\r\n",
							"            fname = self.faker.first_name_male() if gender == 'Male' else self.faker.first_name_female()\r\n",
							"            lname = self.faker.last_name()\r\n",
							"            staffs.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"StaffUniqueId\": self.faker.random_number(digits = self.unique_id_length),\r\n",
							"                \"BirthDate\": str(self.faker.date_between(start_date='-60y',end_date='-30y')),\r\n",
							"                \"FirstName\": fname,\r\n",
							"                \"HighestCompletedLevelOfEducationDescriptor\": self.get_descriptor_string('LevelOfEducationDescriptor', value = random.choice(LEVELS_OF_EDUCATION)),\r\n",
							"                \"HispanicLatinoEthnicity\": random.choice(BOOLEAN),\r\n",
							"                \"LastSurname\": lname,\r\n",
							"                \"LoginId\": '{}{}'.format(fname[0],lname.lower()),\r\n",
							"                \"PersonalTitlePrefix\": 'Mr' if gender == 'Male' else 'Ms',\r\n",
							"                \"SexDescriptor\": self.get_descriptor_string('SexDescriptor', value = gender),\r\n",
							"                \"YearsOfPriorProfessionalExperience\": random.choice(range(50)),\r\n",
							"                \"Addresses\": self.create_address(),\r\n",
							"                \"AncestryEthnicOrigins\": [],\r\n",
							"                \"Credentials\": [\r\n",
							"                    {\r\n",
							"                        \"CredentialReference\": {\r\n",
							"                            \"CredentialIdentifier\": self.faker.random_number(digits = 10),\r\n",
							"                            \"StateOfIssueStateAbbreviationDescriptor\": self.get_descriptor_string('StateAbbreviationDescriptor', 'TX'),\r\n",
							"                            \"Link\": {\r\n",
							"                                \"rel\": \"Credential\",\r\n",
							"                                \"href\": \"/ed-fi/credentials/\" + self.faker.uuid4().replace('-','')\r\n",
							"                            }\r\n",
							"                        }\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"ElectronicMails\": [\r\n",
							"                    {\r\n",
							"                        \"ElectronicMailAddress\": \"{}{}@edfi.org\".format(fname,lname),\r\n",
							"                        \"ElectronicMailTypeDescriptor\": self.get_descriptor_string('ElectronicMailTypeDescriptor','Work')\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"IdentificationCodes\": [\r\n",
							"                    {\r\n",
							"                        \"StaffIdentificationSystemDescriptor\": self.get_descriptor_string('StaffIdentificationSystemDescriptor','State'),\r\n",
							"                        \"IdentificationCode\": self.faker.random_number(digits = self.unique_id_length)\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"IdentificationDocuments\": [],\r\n",
							"                \"InternationalAddresses\": self.create_address(),\r\n",
							"                \"Languages\": [],\r\n",
							"                \"OtherNames\": [self.faker.first_name_male() if gender == 'Male' else self.faker.first_name_female()],\r\n",
							"                \"PersonalIdentificationDocuments\": [\r\n",
							"                    {\r\n",
							"                        \"IdentificationDocumentUseDescriptor\": \"uri://ed-fi.org/IdentificationDocumentUseDescriptor#Personal Information Verification\",\r\n",
							"                        \"PersonalInformationVerificationDescriptor\": self.get_descriptor_string('PersonalInformationVerificationDescriptor', value = random.choice(PERSONAL_INFORMATION_VERIFICATION_DESCRIPTIONS))\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"Races\": [\r\n",
							"                    {\r\n",
							"                        \"RaceDescriptor\": self.get_descriptor_string('RaceDescriptor', value = random.choice(RACES))\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return staffs\r\n",
							"\r\n",
							"    def create_sessions(self, school):\r\n",
							"\r\n",
							"        return [{\r\n",
							"            \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"            \"SchoolReference\":{\r\n",
							"                \"SchoolId\":school['SchoolId'],\r\n",
							"                \"Link\":{\r\n",
							"                    \"rel\":\"School\",\r\n",
							"                    \"href\":\"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SchoolYearTypeReference\": {\r\n",
							"                \"SchoolYear\": self.school_year,\r\n",
							"                \"Link\": {\r\n",
							"                    \"rel\": \"SchoolYearType\",\r\n",
							"                    \"href\": \"/ed-fi/schoolYearTypes/{}\".format(school['_SchoolYears']['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SessionName\": \"{} - {} Fall Semester\".format(int(self.school_year) - 1, self.school_year ),\r\n",
							"            \"BeginDate\": \"{}-08-{}\".format(int(self.school_year) - 1, random.randint(1,30)),\r\n",
							"            \"EndDate\": \"{}-12-{}\".format(int(self.school_year) - 1, random.randint(1,30)),\r\n",
							"            \"TermDescriptor\": self.get_descriptor_string('TermDescriptor', 'Fall Semester'),\r\n",
							"            \"TotalInstructionalDays\": random.randint(60,130),\r\n",
							"            \"GradingPeriods\": [\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#First Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 1,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Second Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 2,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Third Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 3,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                }\r\n",
							"            ],\r\n",
							"            \"_etag\": self.faker.random_number(digits=10)\r\n",
							"        },\r\n",
							"        {\r\n",
							"            \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"            \"SchoolReference\":{\r\n",
							"                \"SchoolId\":school['SchoolId'],\r\n",
							"                \"Link\":{\r\n",
							"                    \"rel\":\"School\",\r\n",
							"                    \"href\":\"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SchoolYearTypeReference\": {\r\n",
							"                \"SchoolYear\": self.school_year,\r\n",
							"                \"Link\": {\r\n",
							"                    \"rel\": \"SchoolYearType\",\r\n",
							"                    \"href\": \"/ed-fi/schoolYearTypes/{}\".format(school['_SchoolYears']['Id'])\r\n",
							"                }\r\n",
							"            },\r\n",
							"            \"SessionName\": \"{} - {} Spring Semester\".format(int(self.school_year) - 1, self.school_year),\r\n",
							"            \"BeginDate\": \"{}-01-{}\".format(self.school_year, random.randint(1,30)),\r\n",
							"            \"EndDate\": \"{}-05-{}\".format(self.school_year, random.randint(1,30)),\r\n",
							"            \"TermDescriptor\": self.get_descriptor_string('TermDescriptor', 'Spring Semester'),\r\n",
							"            \"TotalInstructionalDays\": random.randint(60,130),\r\n",
							"            \"GradingPeriods\": [\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Fourth Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 4,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Fifth Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 5,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                {\r\n",
							"                    \"GradingPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"GradingPeriodDescriptor\": \"uri://ed-fi.org/GradingPeriodDescriptor#Sixth Six Weeks\",\r\n",
							"                    \"PeriodSequence\": 6,\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"GradingPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/gradingPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                }\r\n",
							"            ],\r\n",
							"            \"_etag\": self.faker.random_number(digits=10)\r\n",
							"        }]\r\n",
							"\r\n",
							"    def create_sections(self, school):\r\n",
							"        sections = []\r\n",
							"        for _ in range(self.number_sections_per_school):\r\n",
							"            semesterType = random.choice(['Spring', 'Fall'])\r\n",
							"            subjectName = random.choice(SUBJECT_NAMES)[1]\r\n",
							"            subjectNumber = random.randint(1,5)\r\n",
							"            sections.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"CourseOfferingReference\": {\r\n",
							"                    \"LocalCourseCode\": \"{}-{}\".format(subjectName[0:3].upper(), subjectNumber),\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"SessionName\": \"{} - {} {} Semester\".format(int(self.school_year) - 1, semesterType, self.school_year),\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"CourseOffering\",\r\n",
							"                        \"href\": \"/ed-fi/courseOfferings/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"LocationReference\": {\r\n",
							"                    \"ClassroomIdentificationCode\": self.faker.random_number(digits = 3),\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Location\",\r\n",
							"                        \"href\": \"/ed-fi/locations/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"LocationSchoolReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": \"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"SectionIdentifier\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"AvailableCredits\": random.randint(1,4),\r\n",
							"                \"EducationalEnvironmentDescriptor\": self.get_descriptor_string('EducationalEnvironmentDescriptor','Classroom'),\r\n",
							"                \"SectionName\": \"{} {}\".format(subjectName, subjectNumber),\r\n",
							"                \"SequenceOfCourse\": random.randint(1,5),\r\n",
							"                \"Characteristics\": [],\r\n",
							"                \"ClassPeriods\": [\r\n",
							"                {\r\n",
							"                    \"ClassPeriodReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"ClassPeriodName\": \"{} - Traditional\".format(random.randint(1,5)),\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"ClassPeriod\",\r\n",
							"                        \"href\": \"/ed-fi/classPeriods/{}\".format(self.faker.uuid4().replace('-',''))\r\n",
							"                    }\r\n",
							"                    }\r\n",
							"                }\r\n",
							"                ],\r\n",
							"                \"CourseLevelCharacteristics\": [],\r\n",
							"                \"OfferedGradeLevels\": [],\r\n",
							"                \"Programs\": [],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return sections\r\n",
							"\r\n",
							"    def create_student_section_associations(self, school):\r\n",
							"        student_section_associations = []\r\n",
							"        session = random.choice(school['_Sessions'])\r\n",
							"        for student in school['_Students']:\r\n",
							"            course = random.choice(school['_Courses'])\r\n",
							"            section = random.choice(school['_Sections'])\r\n",
							"            student_section_associations.append({\r\n",
							"                    \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                    \"SectionReference\": {\r\n",
							"                        \"LocalCourseCode\": course['CourseCode'],\r\n",
							"                        \"SchoolId\": school['SchoolId'],\r\n",
							"                        \"SchoolYear\": self.school_year,\r\n",
							"                        \"SectionIdentifier\": section['SectionIdentifier'],\r\n",
							"                        \"SessionName\": session['SessionName'],\r\n",
							"                        \"Link\": {\r\n",
							"                            \"rel\": \"Section\",\r\n",
							"                            \"href\": \"/ed-fi/sections/{}\".format(section['Id'])\r\n",
							"                        }\r\n",
							"                    },\r\n",
							"                    \"StudentReference\": {\r\n",
							"                        \"StudentUniqueId\": student['StudentUniqueId'],\r\n",
							"                        \"Link\": {\r\n",
							"                            \"rel\": \"Student\",\r\n",
							"                            \"href\": \"/ed-fi/students/{}\".format(student['Id'])\r\n",
							"                        }\r\n",
							"                    },\r\n",
							"                    \"BeginDate\": session['BeginDate'],\r\n",
							"                    \"EndDate\": session['EndDate'],\r\n",
							"                    \"HomeroomIndicator\": random.choice(BOOLEAN),\r\n",
							"                    \"_etag\": self.faker.random_number(digits = 10)\r\n",
							"                })\r\n",
							"        return student_section_associations\r\n",
							"\r\n",
							"    def create_staff_section_associations(self,school):\r\n",
							"        staff_section_associations = []\r\n",
							"        for staff in school['_Staffs']:\r\n",
							"            session = random.choice(school['_Sessions'])\r\n",
							"            section = random.choice(school['_Sections'])\r\n",
							"            staff_section_associations.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"SectionReference\": {\r\n",
							"                    \"LocalCourseCode\": section['CourseOfferingReference']['LocalCourseCode'],\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"SchoolYear\": self.school_year,\r\n",
							"                    \"SectionIdentifier\": section['SectionIdentifier'],\r\n",
							"                    \"SessionName\": session['SessionName'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Section\",\r\n",
							"                        \"href\": \"/ed-fi/sections/{}\".format(section['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"StaffReference\": {\r\n",
							"                    \"StaffUniqueId\": staff['StaffUniqueId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Staff\",\r\n",
							"                        \"href\": \"/ed-fi/staffs/{}\".format(staff['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"BeginDate\": session['BeginDate'],\r\n",
							"                \"ClassroomPositionDescriptor\": \"uri://ed-fi.org/ClassroomPositionDescriptor#Teacher of Record\",\r\n",
							"                \"EndDate\": session['EndDate'],\r\n",
							"                \"_etag\": self.faker.uuid4().replace('-','')\r\n",
							"            })\r\n",
							"        return staff_section_associations\r\n",
							"\r\n",
							"\r\n",
							"    def create_staff_school_associations(self, school):\r\n",
							"        staff_school_associations = []\r\n",
							"        for staff in school['_Staffs']:\r\n",
							"            staff_school_associations.append({\r\n",
							"                \"Id\": self.faker.uuid4().replace('-',''),\r\n",
							"                \"SchoolReference\": {\r\n",
							"                    \"SchoolId\": school['SchoolId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"School\",\r\n",
							"                        \"href\": \"/ed-fi/schools/{}\".format(school['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"StaffReference\": {\r\n",
							"                    \"StaffUniqueId\": staff['StaffUniqueId'],\r\n",
							"                    \"Link\": {\r\n",
							"                        \"rel\": \"Staff\",\r\n",
							"                        \"href\": \"/ed-fi/staffs/{}\".format(staff['Id'])\r\n",
							"                    }\r\n",
							"                },\r\n",
							"                \"ProgramAssignmentDescriptor\": self.get_descriptor_string('ProgramAssignmentDescriptor','Regular Education'),\r\n",
							"                \"AcademicSubjects\": [\r\n",
							"                    {\r\n",
							"                        \"AcademicSubjectDescriptor\": self.get_descriptor_string('AcademicSubjectDescriptor',random.choice(SUBJECT_NAMES)[0])\r\n",
							"                    }\r\n",
							"                ],\r\n",
							"                \"GradeLevels\": [\r\n",
							"                    {'GradeLevelDescriptor': self.get_descriptor_string('GradeLevelDescriptor',random.choice(GRADE_LEVEL))} for _ in range(4)\r\n",
							"            ],\r\n",
							"                \"_etag\": self.faker.random_number(digits=10)\r\n",
							"            })\r\n",
							"        return staff_school_associations\r\n",
							"\r\n",
							"    def format_edfi_data(self,data):\r\n",
							"        result = {\r\n",
							"            'Schools':[],\r\n",
							"            'Students':[],\r\n",
							"            'Calendars':[],\r\n",
							"            'Courses':[],\r\n",
							"            'StudentSchoolAssociations':[],\r\n",
							"            'Staffs':[],\r\n",
							"            'Sections': [],\r\n",
							"            'StaffSchoolAssociations':[],\r\n",
							"            'Sessions':[],\r\n",
							"            'StudentSectionAssociations':[],\r\n",
							"            'StaffSectionAssociations':[]\r\n",
							"\r\n",
							"        }\r\n",
							"        for school in data:\r\n",
							"            result['Schools'].append({key: school[key] for key in school if not (key.startswith('_')) })\r\n",
							"            result['Students'] += school['_Students']\r\n",
							"            result['Courses'] += school['_Courses']\r\n",
							"            result['StudentSchoolAssociations'] += school['_StudentAssociations']\r\n",
							"            result['Calendars'].append(school['_Calendars'])\r\n",
							"            result['Staffs'] += school['_Staffs']\r\n",
							"            result['Sections'] += school['_Sections']\r\n",
							"            result['StaffSchoolAssociations'] += school['_StaffSchoolAssociations']\r\n",
							"            result['Sessions'] += school['_Sessions']\r\n",
							"            result['StudentSectionAssociations'] += school['_StudentSectionAssociations']\r\n",
							"            result['StaffSectionAssociations'] += school['_StaffSectionAssociations']\r\n",
							"\r\n",
							"\r\n",
							"        return result\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_activity')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Process all roster data from stage1/M365/inbound/roster into stage 2 in delta lake format.\r\n",
							"(overwrites previously loaded data with the latest snapshot data)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"instrumentation_key = 'InstrumentationKey=5a4a6026-f008-4e9e-a48a-96a412d250d6'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"spark = SparkSession.builder.appName(\"OEA_M365_processing\").getOrCreate()\r\n",
							"\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.DEBUG) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string=instrumentation_key))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming activity data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process M365 activity data\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"csvDF = csvDF.dropDuplicates(['SignalId'])\r\n",
							"csvDF = csvDF.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"query.start(dest_path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming roster data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"s1_path = stage1 + '/M365/inbound/roster'\r\n",
							"s2_path = stage2 + '/M365'\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(entity, schema):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source={s1_path}, destination={s2_path}\")\r\n",
							"        csvDF = spark.readStream.csv(s1_path + '/*/' + entity + '/*.csv', header='false', schema=schema)\r\n",
							"\r\n",
							"        query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", s1_path + '/' + entity + '/_checkpoints')\r\n",
							"        query.start(s2_path + '/' + entity)\r\n",
							"\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        print(error)\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"s1_to_s2('AadUser', schema)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Batch process M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(source_path, schema, destination_path):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(source_path, header='false', schema=schema)\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(destination_path)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"def process_M365_roster(file_path):\r\n",
							"    #AadUser\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUser', schema, stage2 + '/M365/AadUser')\r\n",
							"    #AadUserPersonMapping\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUserPersonMapping', schema, stage2 + '/M365/AadUserPersonMapping')\r\n",
							"    #Course\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Course', schema, stage2 + '/M365/Course')\r\n",
							"    #CourseGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseGradeLevel', schema, stage2 + '/M365/CourseGradeLevel')\r\n",
							"    #CourseSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseSubject', schema, stage2 + '/M365/CourseSubject')\r\n",
							"    #Enrollment\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"    s1_to_s2(file_path + '/Enrollment', schema, stage2 + '/M365/Enrollment')\r\n",
							"    #Organization\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Organization', schema, stage2 + '/M365/Organization')\r\n",
							"    #Person\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Person', schema, stage2 + '/M365/Person')\r\n",
							"    #PersonDemographic\r\n",
							"    schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographic', schema, stage2 + '/M365/PersonDemographic')\r\n",
							"    #PersonDemographicEthnicity\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicEthnicity', schema, stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"    #PersonDemographicPersonFlag\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicPersonFlag', schema, stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"    #PersonDemographicRace\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicRace', schema, stage2 + '/M365/PersonDemographicRace')\r\n",
							"    #PersonEmailAddress\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonEmailAddress', schema, stage2 + '/M365/PersonEmailAddress')\r\n",
							"    #PersonIdentifier\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonIdentifier', schema, stage2 + '/M365/PersonIdentifier')\r\n",
							"    #PersonOrganizationRole\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonOrganizationRole', schema, stage2 + '/M365/PersonOrganizationRole')\r\n",
							"    #PersonPhoneNumber\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonPhoneNumber', schema, stage2 + '/M365/PersonPhoneNumber')\r\n",
							"    #PersonRelationship\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonRelationship', schema, stage2 + '/M365/PersonRelationship')\r\n",
							"    #RefDefinition\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"    s1_to_s2(file_path + '/RefDefinition', schema, stage2 + '/M365/RefDefinition')\r\n",
							"    #Section\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Section', schema, stage2 + '/M365/Section')\r\n",
							"    #SectionGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionGradeLevel', schema, stage2 + '/M365/SectionGradeLevel')\r\n",
							"    #SectionSession\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSession', schema, stage2 + '/M365/SectionSession')\r\n",
							"    #SectionSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSubject', schema, stage2 + '/M365/SectionSubject')\r\n",
							"    #Session\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Session', schema, stage2 + '/M365/Session')\r\n",
							"    #SourceSystem\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SourceSystem', schema, stage2 + '/M365/SourceSystem')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"inbound_path = stage1 + '/M365/inbound/roster'\r\n",
							"processed_path = stage1 + '/M365/processed/roster'\r\n",
							"\r\n",
							"logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"items = mssparkutils.fs.ls(inbound_path)\r\n",
							"for item in items:\r\n",
							"    if item.isDir:\r\n",
							"        process_M365_roster(item.path)\r\n",
							"        mssparkutils.fs.mv(item.path, processed_path, True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything to run it again"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_lib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Modules/M365"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Variables\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"s1_M365 = stage1 + '/M365'\r\n",
							"s2_M365 = stage2 + '/M365'\r\n",
							"s3_M365 = stage3 + '/M365'\r\n",
							"\r\n",
							"M365_module_path = oea_path + '/modules/M365'\r\n",
							"s1_M365_activity = s1_M365 + '/activity'\r\n",
							"s1_M365_roster = s1_M365 + '/roster'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Schemas"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Schema definitions\r\n",
							"M365_schemas = {}\r\n",
							"M365_schemas['TechActivity'] = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"M365_schemas['AadUser'] = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"M365_schemas['AadUserPersonMapping'] = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"M365_schemas['Course'] = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"M365_schemas['CourseGradeLevel'] = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"M365_schemas['CourseSubject'] = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"M365_schemas['Enrollment'] = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"M365_schemas['Organization'] = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"M365_schemas['Person'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"M365_schemas['PersonDemographic'] = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"M365_schemas['PersonDemographicEthnicity'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"M365_schemas['PersonDemographicPersonFlag'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"M365_schemas['PersonDemographicRace'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"M365_schemas['PersonEmailAddress'] = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"M365_schemas['PersonIdentifier'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"M365_schemas['PersonOrganizationRole'] = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"M365_schemas['PersonPhoneNumber'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"M365_schemas['PersonRelationship'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"M365_schemas['RefDefinition'] = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"M365_schemas['Section'] = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"M365_schemas['SectionGradeLevel'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"M365_schemas['SectionSession'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"M365_schemas['SectionSubject'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"M365_schemas['Session'] = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"M365_schemas['SourceSystem'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Functions"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Provides a small sample activity data set\r\n",
							"def get_sample_activity_data(entity):\r\n",
							"    df = spark.read.csv(M365_module_path + '/sample_data/small_samples/' + entity + '.csv', schema=M365_schemas[entity])\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process TechActivity data from stage1 into stage2 using structured streaming\r\n",
							"def process_activity(source_path=s1_M365_activity, dest_path=s2_M365):\r\n",
							"    df = spark.readStream.csv(source_path + '/*/*.csv', header='false', schema=M365_schemas['TechActivity'])\r\n",
							"    df = df.dropDuplicates(['SignalId'])\r\n",
							"    df = df.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"    query = df.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"    query.start(dest_path + '/TechActivity')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Resets all TechActivity processing. This is intended for use during initial testing - use with caution\r\n",
							"# This function does the following:\r\n",
							"#  - deletes the _checkpoints dir from stage1/M365/activity\r\n",
							"#  - deletes the delta table at stage2/M365/TechActivity\r\n",
							"def reset_activity_processing():\r\n",
							"    # Delete stage2/M365/TechActivity if it exists\r\n",
							"    items = mssparkutils.fs.ls(s2_M365)\r\n",
							"    #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"    for item in items:\r\n",
							"        if item.name == 'TechActivity':\r\n",
							"            mssparkutils.fs.rm(s2_M365 + '/TechActivity', True)\r\n",
							"\r\n",
							"    items = mssparkutils.fs.ls(s1_M365 + '/activity')\r\n",
							"    for item in items:\r\n",
							"        if item.name == '_checkpoints':\r\n",
							"            mssparkutils.fs.rm(s1_M365 + '/activity/_checkpoints', True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.utils import AnalysisException\r\n",
							"def process_roster_entity(path, entity):\r\n",
							"    try:\r\n",
							"        #logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(path + '/' + entity, header='false', schema=M365_schemas[entity])\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(s2_M365 + '/' + entity)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"\r\n",
							"def process_roster_date_folder(date_folder_path):\r\n",
							"    process_roster_entity(date_folder_path, 'AadUser')\r\n",
							"    process_roster_entity(date_folder_path, 'AadUserPersonMapping')\r\n",
							"    process_roster_entity(date_folder_path, 'Course')\r\n",
							"    process_roster_entity(date_folder_path, 'CourseGradeLevel')\r\n",
							"    process_roster_entity(date_folder_path, 'CourseSubject')\r\n",
							"    process_roster_entity(date_folder_path, 'Enrollment')\r\n",
							"    process_roster_entity(date_folder_path, 'Organization')\r\n",
							"    process_roster_entity(date_folder_path, 'Person')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonDemographic')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonDemographicEthnicity')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonDemographicPersonFlag')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonDemographicRace')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonEmailAddress')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonIdentifier')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonOrganizationRole')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonPhoneNumber')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonRelationship')\r\n",
							"    process_roster_entity(date_folder_path, 'RefDefinition')\r\n",
							"    process_roster_entity(date_folder_path, 'Section')\r\n",
							"    process_roster_entity(date_folder_path, 'SectionGradeLevel')\r\n",
							"    process_roster_entity(date_folder_path, 'SectionSession')\r\n",
							"    process_roster_entity(date_folder_path, 'SectionSubject')\r\n",
							"    process_roster_entity(date_folder_path, 'Session')\r\n",
							"    process_roster_entity(date_folder_path, 'SourceSystem')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def process_roster():\r\n",
							"    logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"    items = mssparkutils.fs.ls(s1_M365 + '/roster')\r\n",
							"    for item in items:\r\n",
							"        if item.isDir:\r\n",
							"            process_roster_date_folder(item.path)\r\n",
							"            mssparkutils.fs.mv(item.path, s1_M365 + '/roster_processed/', True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def reset_roster_processing():\r\n",
							"\r\n",
							"    if path_exists(stage2 + '/M365'):\r\n",
							"        # Delete roster delta tables (everything other than TechActivity)\r\n",
							"        items = mssparkutils.fs.ls(stage2 + '/M365')\r\n",
							"        #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"        for item in items:\r\n",
							"            if item.name != 'TechActivity':\r\n",
							"                mssparkutils.fs.rm(item.path, True)\r\n",
							"\r\n",
							"    if path_exists(stage1 + '/M365/roster_processed'):\r\n",
							"        # Move roster data back in to \"inbound\" folder\r\n",
							"        items = mssparkutils.fs.ls(stage1 + '/M365/roster_processed')\r\n",
							"        for item in items:\r\n",
							"            mssparkutils.fs.mv(item.path, stage1 + '/M365/roster', True)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_lib_backup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Process all roster data from stage1/M365/inbound/roster into stage 2 in delta lake format.\r\n",
							"(overwrites previously loaded data with the latest snapshot data)"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming activity data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process M365 activity data\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"csvDF = csvDF.dropDuplicates(['SignalId'])\r\n",
							"csvDF = csvDF.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"query.start(dest_path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming roster data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"s1_path = stage1 + '/M365/inbound/roster'\r\n",
							"s2_path = stage2 + '/M365'\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(entity, schema):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source={s1_path}, destination={s2_path}\")\r\n",
							"        csvDF = spark.readStream.csv(s1_path + '/*/' + entity + '/*.csv', header='false', schema=schema)\r\n",
							"\r\n",
							"        query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", s1_path + '/' + entity + '/_checkpoints')\r\n",
							"        query.start(s2_path + '/' + entity)\r\n",
							"\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        print(error)\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"s1_to_s2('AadUser', schema)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Batch process M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(source_path, schema, destination_path):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(source_path, header='false', schema=schema)\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(destination_path)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"def process_M365_roster(file_path):\r\n",
							"    #AadUser\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUser', schema, stage2 + '/M365/AadUser')\r\n",
							"    #AadUserPersonMapping\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUserPersonMapping', schema, stage2 + '/M365/AadUserPersonMapping')\r\n",
							"    #Course\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Course', schema, stage2 + '/M365/Course')\r\n",
							"    #CourseGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseGradeLevel', schema, stage2 + '/M365/CourseGradeLevel')\r\n",
							"    #CourseSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseSubject', schema, stage2 + '/M365/CourseSubject')\r\n",
							"    #Enrollment\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"    s1_to_s2(file_path + '/Enrollment', schema, stage2 + '/M365/Enrollment')\r\n",
							"    #Organization\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Organization', schema, stage2 + '/M365/Organization')\r\n",
							"    #Person\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Person', schema, stage2 + '/M365/Person')\r\n",
							"    #PersonDemographic\r\n",
							"    schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographic', schema, stage2 + '/M365/PersonDemographic')\r\n",
							"    #PersonDemographicEthnicity\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicEthnicity', schema, stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"    #PersonDemographicPersonFlag\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicPersonFlag', schema, stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"    #PersonDemographicRace\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicRace', schema, stage2 + '/M365/PersonDemographicRace')\r\n",
							"    #PersonEmailAddress\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonEmailAddress', schema, stage2 + '/M365/PersonEmailAddress')\r\n",
							"    #PersonIdentifier\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonIdentifier', schema, stage2 + '/M365/PersonIdentifier')\r\n",
							"    #PersonOrganizationRole\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonOrganizationRole', schema, stage2 + '/M365/PersonOrganizationRole')\r\n",
							"    #PersonPhoneNumber\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonPhoneNumber', schema, stage2 + '/M365/PersonPhoneNumber')\r\n",
							"    #PersonRelationship\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonRelationship', schema, stage2 + '/M365/PersonRelationship')\r\n",
							"    #RefDefinition\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"    s1_to_s2(file_path + '/RefDefinition', schema, stage2 + '/M365/RefDefinition')\r\n",
							"    #Section\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Section', schema, stage2 + '/M365/Section')\r\n",
							"    #SectionGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionGradeLevel', schema, stage2 + '/M365/SectionGradeLevel')\r\n",
							"    #SectionSession\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSession', schema, stage2 + '/M365/SectionSession')\r\n",
							"    #SectionSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSubject', schema, stage2 + '/M365/SectionSubject')\r\n",
							"    #Session\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Session', schema, stage2 + '/M365/Session')\r\n",
							"    #SourceSystem\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SourceSystem', schema, stage2 + '/M365/SourceSystem')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"inbound_path = stage1 + '/M365/inbound/roster'\r\n",
							"processed_path = stage1 + '/M365/processed/roster'\r\n",
							"\r\n",
							"logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"items = mssparkutils.fs.ls(inbound_path)\r\n",
							"for item in items:\r\n",
							"    if item.isDir:\r\n",
							"        process_M365_roster(item.path)\r\n",
							"        mssparkutils.fs.mv(item.path, processed_path, True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything to run it again"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Testing delta"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"student_schema = [\"OrderId\",\"OrderDate\", \"Region\", \"City\", \"Category\",\"Product\",\"Quantity\",\"UnitPrice\",\"TotalPrice\"]\r\n",
							"\r\n",
							"def load_data():\r\n",
							"    # N.B. this would actually be loading the real data from somewhere in the data lake, or elsewhere...\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            (1,\"01/01/2020\",\"East\",\"Boston\",\"Bars\",\"Carrot\",33,1.77,58.41),\r\n",
							"            (2,\"04/01/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",87,3.49,303.63),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (11,\"31/01/2020\",\"East\",\"Boston\",\"Cookies\",\"Arrowroot\",36,2.18,78.48),\r\n",
							"            (12,\"03/02/2020\",\"East\",\"Boston\",\"Cookies\",\"Chocolate Chip\",31,1.87,57.97),\r\n",
							"            (13,\"06/02/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",28,3.49,97.72)      \r\n",
							"        ],\r\n",
							"        orders_schema \r\n",
							"    )\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"students_schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", IntegerType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"  ])\r\n",
							"\r\n",
							"df1 = spark.createDataFrame(\r\n",
							"    [\r\n",
							"        ('John', 'Smith', 1, 'M'),\r\n",
							"        ('Jacob', 'Jones', 2, 'M'),\r\n",
							"        ('Jane', 'Johnson', 3, 'F'),\r\n",
							"        ('Jill', 'Jackson', 4, 'F')\r\n",
							"    ],\r\n",
							"    students_schema\r\n",
							")\r\n",
							"\r\n",
							"df2 = spark.createDataFrame(\r\n",
							"    [\r\n",
							"        ('John', 'Smith', 1, 'M'),\r\n",
							"        ('Jacob', 'Jones', 2, 'M'),\r\n",
							"        ('Jane', 'Johnson', 3, 'F')\r\n",
							"    ],\r\n",
							"    students_schema\r\n",
							")\r\n",
							"\r\n",
							"display(df2)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"students_schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", IntegerType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"  ])\r\n",
							"\r\n",
							"def load_data():\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            ('John', 'Smith', 1, 'M'),\r\n",
							"            ('Jacob', 'Jones', 2, 'M'),\r\n",
							"            ('Jane', 'Johnson', 3, 'F'),\r\n",
							"            ('Jill', 'Jackson', 4, 'F')                      \r\n",
							"        ],\r\n",
							"        students_schema\r\n",
							"    )\r\n",
							"    return df\r\n",
							"\r\n",
							"df = load_data()\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_process')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Modules/M365"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/OEA_Core"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/Modules/M365/M365_lib"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"process_activity()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_roster_v0p3_s1_to_s2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Process all roster data from stage1/M365/inbound/roster into stage 2 in delta lake format.\r\n",
							"(overwrites previously loaded data with the latest snapshot data)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"file_path = '/M365/inbound/roster/2021-06-15T04-04-12'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/AadUser', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/AadUser')\r\n",
							"#AadUserPersonMapping\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/AadUserPersonMapping', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/AadUserPersonMapping')\r\n",
							"#Course\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Course', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Course')\r\n",
							"#CourseGradeLevel\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/CourseGradeLevel', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/CourseGradeLevel')\r\n",
							"#CourseSubject\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/CourseSubject', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/CourseSubject')\r\n",
							"#Enrollment\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Enrollment', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Enrollment')\r\n",
							"#Organization\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Organization', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Organization')\r\n",
							"#Person\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Person', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Person')\r\n",
							"#PersonDemographic\r\n",
							"schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonDemographic', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographic')\r\n",
							"#PersonDemographicEthnicity\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonDemographicEthnicity', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"#PersonDemographicPersonFlag\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonDemographicPersonFlag', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"#PersonDemographicRace\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonDemographicRace', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicRace')\r\n",
							"#PersonEmailAddress\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonEmailAddress', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonEmailAddress')\r\n",
							"#PersonIdentifier\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonIdentifier', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonIdentifier')\r\n",
							"#PersonOrganizationRole\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonOrganizationRole', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonOrganizationRole')\r\n",
							"#PersonPhoneNumber\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonPhoneNumber', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonPhoneNumber')\r\n",
							"#PersonRelationship\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonRelationship', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonRelationship')\r\n",
							"#RefDefinition\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/RefDefinition', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/RefDefinition')\r\n",
							"#Section\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Section', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Section')\r\n",
							"#SectionGradeLevel\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/SectionGradeLevel', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionGradeLevel')\r\n",
							"#SectionSession\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/SectionSession', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionSession')\r\n",
							"#SectionSubject\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/SectionSubject', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionSubject')\r\n",
							"#Session\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Session', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Session')\r\n",
							"#SourceSystem\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/SourceSystem', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SourceSystem')\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_roster_v0p3_s1_to_s2_with_move_to_processed')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Process all roster data from stage1/M365/inbound/roster into stage 2 in delta lake format.\r\n",
							"(overwrites previously loaded data with the latest snapshot data)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"instrumentation_key = 'InstrumentationKey=5a4a6026-f008-4e9e-a48a-96a412d250d6'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"spark = SparkSession.builder.appName(\"OEA_M365_processing\").getOrCreate()\r\n",
							"\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.DEBUG) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string=instrumentation_key))"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming activity data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process M365 activity data\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"csvDF = csvDF.dropDuplicates(['SignalId'])\r\n",
							"csvDF = csvDF.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"query.start(dest_path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming roster data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"s1_path = stage1 + '/M365/inbound/roster'\r\n",
							"s2_path = stage2 + '/M365'\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(entity, schema):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source={s1_path}, destination={s2_path}\")\r\n",
							"        csvDF = spark.readStream.csv(s1_path + '/*/' + entity + '/*.csv', header='false', schema=schema)\r\n",
							"\r\n",
							"        query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", s1_path + '/' + entity + '/_checkpoints')\r\n",
							"        query.start(s2_path + '/' + entity)\r\n",
							"\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        print(error)\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"s1_to_s2('AadUser', schema)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Batch process M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": true,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(source_path, schema, destination_path):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(source_path, header='false', schema=schema)\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(destination_path)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"def process_M365_roster(file_path):\r\n",
							"    #AadUser\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUser', schema, stage2 + '/M365/AadUser')\r\n",
							"    #AadUserPersonMapping\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUserPersonMapping', schema, stage2 + '/M365/AadUserPersonMapping')\r\n",
							"    #Course\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Course', schema, stage2 + '/M365/Course')\r\n",
							"    #CourseGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseGradeLevel', schema, stage2 + '/M365/CourseGradeLevel')\r\n",
							"    #CourseSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseSubject', schema, stage2 + '/M365/CourseSubject')\r\n",
							"    #Enrollment\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"    s1_to_s2(file_path + '/Enrollment', schema, stage2 + '/M365/Enrollment')\r\n",
							"    #Organization\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Organization', schema, stage2 + '/M365/Organization')\r\n",
							"    #Person\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Person', schema, stage2 + '/M365/Person')\r\n",
							"    #PersonDemographic\r\n",
							"    schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographic', schema, stage2 + '/M365/PersonDemographic')\r\n",
							"    #PersonDemographicEthnicity\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicEthnicity', schema, stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"    #PersonDemographicPersonFlag\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicPersonFlag', schema, stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"    #PersonDemographicRace\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicRace', schema, stage2 + '/M365/PersonDemographicRace')\r\n",
							"    #PersonEmailAddress\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonEmailAddress', schema, stage2 + '/M365/PersonEmailAddress')\r\n",
							"    #PersonIdentifier\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonIdentifier', schema, stage2 + '/M365/PersonIdentifier')\r\n",
							"    #PersonOrganizationRole\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonOrganizationRole', schema, stage2 + '/M365/PersonOrganizationRole')\r\n",
							"    #PersonPhoneNumber\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonPhoneNumber', schema, stage2 + '/M365/PersonPhoneNumber')\r\n",
							"    #PersonRelationship\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonRelationship', schema, stage2 + '/M365/PersonRelationship')\r\n",
							"    #RefDefinition\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"    s1_to_s2(file_path + '/RefDefinition', schema, stage2 + '/M365/RefDefinition')\r\n",
							"    #Section\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Section', schema, stage2 + '/M365/Section')\r\n",
							"    #SectionGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionGradeLevel', schema, stage2 + '/M365/SectionGradeLevel')\r\n",
							"    #SectionSession\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSession', schema, stage2 + '/M365/SectionSession')\r\n",
							"    #SectionSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSubject', schema, stage2 + '/M365/SectionSubject')\r\n",
							"    #Session\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Session', schema, stage2 + '/M365/Session')\r\n",
							"    #SourceSystem\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SourceSystem', schema, stage2 + '/M365/SourceSystem')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"inbound_path = stage1 + '/M365/inbound/roster'\r\n",
							"processed_path = stage1 + '/M365/processed/roster'\r\n",
							"\r\n",
							"logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"items = mssparkutils.fs.ls(inbound_path)\r\n",
							"for item in items:\r\n",
							"    if item.isDir:\r\n",
							"        process_M365_roster(item.path)\r\n",
							"        mssparkutils.fs.mv(item.path, processed_path, True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything to run it again"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Testing delta"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"student_schema = [\"OrderId\",\"OrderDate\", \"Region\", \"City\", \"Category\",\"Product\",\"Quantity\",\"UnitPrice\",\"TotalPrice\"]\r\n",
							"\r\n",
							"def load_data():\r\n",
							"    # N.B. this would actually be loading the real data from somewhere in the data lake, or elsewhere...\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            (1,\"01/01/2020\",\"East\",\"Boston\",\"Bars\",\"Carrot\",33,1.77,58.41),\r\n",
							"            (2,\"04/01/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",87,3.49,303.63),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (11,\"31/01/2020\",\"East\",\"Boston\",\"Cookies\",\"Arrowroot\",36,2.18,78.48),\r\n",
							"            (12,\"03/02/2020\",\"East\",\"Boston\",\"Cookies\",\"Chocolate Chip\",31,1.87,57.97),\r\n",
							"            (13,\"06/02/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",28,3.49,97.72)      \r\n",
							"        ],\r\n",
							"        orders_schema \r\n",
							"    )\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"students_schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", IntegerType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"  ])\r\n",
							"\r\n",
							"df1 = spark.createDataFrame(\r\n",
							"    [\r\n",
							"        ('John', 'Smith', 1, 'M'),\r\n",
							"        ('Jacob', 'Jones', 2, 'M'),\r\n",
							"        ('Jane', 'Johnson', 3, 'F'),\r\n",
							"        ('Jill', 'Jackson', 4, 'F')\r\n",
							"    ],\r\n",
							"    students_schema\r\n",
							")\r\n",
							"\r\n",
							"df2 = spark.createDataFrame(\r\n",
							"    [\r\n",
							"        ('John', 'Smith', 1, 'M'),\r\n",
							"        ('Jacob', 'Jones', 2, 'M'),\r\n",
							"        ('Jane', 'Johnson', 3, 'F')\r\n",
							"    ],\r\n",
							"    students_schema\r\n",
							")\r\n",
							"\r\n",
							"display(df2)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"students_schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", IntegerType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"  ])\r\n",
							"\r\n",
							"def load_data():\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            ('John', 'Smith', 1, 'M'),\r\n",
							"            ('Jacob', 'Jones', 2, 'M'),\r\n",
							"            ('Jane', 'Johnson', 3, 'F'),\r\n",
							"            ('Jill', 'Jackson', 4, 'F')                      \r\n",
							"        ],\r\n",
							"        students_schema\r\n",
							"    )\r\n",
							"    return df\r\n",
							"\r\n",
							"df = load_data()\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_tests')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Modules/M365"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/OEA_Core\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/Modules/M365/M365_lib"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Move this to M365_process"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"process_activity()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"reset_activity_processing()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"process_roster()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"reset_roster_processing()"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#df = get_sample_activity_data('TechActivity')\r\n",
							"df = get_sample_activity_data('Section')\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_v0p3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"use_test_env = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"if use_test_env:\r\n",
							"    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\r\n",
							"    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\r\n",
							"    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\r\n",
							"else:\r\n",
							"    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1_m365_roster = stage1 + '/M365/roster/2021-06-15T04-04-12'\r\n",
							"stage1_m365_activity = stage1 + '/m365/DIPData/Activity/ApplicationUsage'\r\n",
							"\r\n",
							"# Process Roster data from stage 1 to stage 2\r\n",
							"# This includes:\r\n",
							"# - adding column names\r\n",
							"# - casting values into specific data types when appropriate\r\n",
							"\r\n",
							"def prep_stage1_M365(entity_name, sql_str):\r\n",
							"  df = spark.read.csv(stage1_m365_roster + '/' + entity_name, header='false')\r\n",
							"  if (df.count() > 0):\r\n",
							"    sqlContext.registerDataFrameAsTable(df, entity_name)\r\n",
							"    df = spark.sql(sql_str)\r\n",
							"    df.write.format('delta').mode('overwrite').save(stage2 + '/M365/' + entity_name)\r\n",
							"\r\n",
							"# Person\r\n",
							"sql_str = \"select _c0 Id, to_timestamp(_c1) FirstSeenDateTime, to_timestamp(_c2) LastSeenDateTime, _c3 GivenName, _c4 MiddleName, _c5 PreferredGivenName, _c6 PreferredMiddleName, _c7 PreferredSurname, _c8 Surname from Person\"\r\n",
							"prep_stage1_M365('Person', sql_str)\r\n",
							"# PersonDemographic\r\n",
							"sql_str = \"select _c0 PersonId, to_timestamp(_c1) FirstSeenDateTime, _c2 BirthCity, _c3 BirthCityCountryCode, _c4 BirthDate, _c5 BirthState, _c6 RefSexId from PersonDemographic\"\r\n",
							"prep_stage1_M365('PersonDemographic', sql_str)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#print(mssparkutils.fs.ls(stage1 + '/M365/roster'))\r\n",
							"folders = mssparkutils.fs.ls(stage1 + '/M365/roster')\r\n",
							"for folder in folders:\r\n",
							"    print(folder[1])"
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_v0p3_Delta')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"ff79d21e-0554-4995-a831-9400c0024e94": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"SignalType": "PostChannelMessage",
												"ChannelId": "19:20990825150f4da78ffdcd6a05cfa2e2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "5ddbce29-3ab9-405d-9350-05945a8a8542",
												"ClassId": "9ab188a9-815d-411d-83a3-a98b56a6c614",
												"StartTime": "2021-06-02T14:48:11Z",
												"AppName": "Teams",
												"SignalId": "AAMkAGRlM2ViNzg5LWMzOGEtNGYyMC05MTFhLWE5MzAxNTk3NjMzNwBGAAAAAAD6n_vRPyLsQ4uBYtj1MoVQBwBONi1figD3Spv1d4sN1luqAAAAAGZ3AABONi1figD3Spv1d4sN1luqAADDKaaJAAA=",
												"ActorRole": "Student"
											},
											{
												"SignalType": "ReplyChannelMessage",
												"ChannelId": "19:20990825150f4da78ffdcd6a05cfa2e2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "5ddbce29-3ab9-405d-9350-05945a8a8542",
												"ClassId": "9ab188a9-815d-411d-83a3-a98b56a6c614",
												"StartTime": "2021-06-02T16:36:46Z",
												"AppName": "Teams",
												"SignalId": "AAMkAGRlM2ViNzg5LWMzOGEtNGYyMC05MTFhLWE5MzAxNTk3NjMzNwBGAAAAAAD6n_vRPyLsQ4uBYtj1MoVQBwBONi1figD3Spv1d4sN1luqAAAAAGZ3AABONi1figD3Spv1d4sN1luqAADDKaaRAAA=",
												"ActorRole": "Student"
											},
											{
												"SignalType": "PostChannelMessage",
												"ChannelId": "19:1500af388f6647739e2300bdc1db53a2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "7951c64a-b45d-4adb-8842-5e7ab60d17e1",
												"ClassId": "5e0407f7-c973-4f25-9cef-302117b080c6",
												"StartTime": "2021-06-02T14:45:34Z",
												"AppName": "Teams",
												"SignalId": "AAMkADg2NDk3MmIwLTM5NDUtNGQwZS04NDQzLTM5MjViMjZiYzFiNABGAAAAAAAEYRCcXVmaTYyFMuIJ9RetBwDPLnXMbt9HQaxJaYXomUUhAAAAAAEnAADPLnXMbt9HQaxJaYXomUUhAADC-nCbAAA="
											},
											{
												"SignalType": "ReplyChannelMessage",
												"ChannelId": "19:1500af388f6647739e2300bdc1db53a2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "7951c64a-b45d-4adb-8842-5e7ab60d17e1",
												"ClassId": "5e0407f7-c973-4f25-9cef-302117b080c6",
												"StartTime": "2021-06-02T14:45:58Z",
												"AppName": "Teams",
												"SignalId": "AAMkADg2NDk3MmIwLTM5NDUtNGQwZS04NDQzLTM5MjViMjZiYzFiNABGAAAAAAAEYRCcXVmaTYyFMuIJ9RetBwDPLnXMbt9HQaxJaYXomUUhAAAAAAEnAADPLnXMbt9HQaxJaYXomUUhAADC-nCcAAA="
											},
											{
												"SignalType": "PostChannelMessage",
												"ChannelId": "19:1500af388f6647739e2300bdc1db53a2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "7951c64a-b45d-4adb-8842-5e7ab60d17e1",
												"ClassId": "5e0407f7-c973-4f25-9cef-302117b080c6",
												"StartTime": "2021-06-02T14:45:34Z",
												"AppName": "Teams",
												"SignalId": "AAMkADg2NDk3MmIwLTM5NDUtNGQwZS04NDQzLTM5MjViMjZiYzFiNABGAAAAAAAEYRCcXVmaTYyFMuIJ9RetBwDPLnXMbt9HQaxJaYXomUUhAAAAAAEnAADPLnXMbt9HQaxJaYXomUUhAADC-nCbAAA="
											}
										],
										"schema": {
											"SignalType": "string",
											"StartTime": "int",
											"UserAgent": "string",
											"SignalId": "string",
											"SisClassId": "string",
											"ClassId": "string",
											"ChannelId": "string",
											"AppName": "string",
											"ActorId": "string",
											"ActorRole": "string",
											"SchemaVersion": "string",
											"AssignmentId": "string",
											"SubmissionId": "string",
											"Action": "string",
											"DueDate": "int",
											"ClassCreationDate": "int",
											"Grade": "string",
											"SourceFileExtension": "string",
											"MeetingDuration": "int"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [
												"SignalType"
											],
											"seriesFieldKeys": [
												"StartTime"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"use_test_env = False"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"if use_test_env:\r\n",
							"    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\r\n",
							"    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\r\n",
							"    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\r\n",
							"else:\r\n",
							"    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1) First ingest all roster data (overwrite previously loaded data with the latest snapshot data)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/AadUser', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/AadUser')\r\n",
							"#AadUserPersonMapping\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/AadUserPersonMapping', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/AadUserPersonMapping')\r\n",
							"#Course\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Course', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Course')\r\n",
							"#CourseGradeLevel\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/CourseGradeLevel', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/CourseGradeLevel')\r\n",
							"#CourseSubject\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/CourseSubject', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/CourseSubject')\r\n",
							"#Enrollment\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Enrollment', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Enrollment')\r\n",
							"#Organization\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Organization', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Organization')\r\n",
							"#Person\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Person', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Person')\r\n",
							"#PersonDemographic\r\n",
							"schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonDemographic', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographic')\r\n",
							"#PersonDemographicEthnicity\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonDemographicEthnicity', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"#PersonDemographicPersonFlag\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonDemographicPersonFlag', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"#PersonDemographicRace\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonDemographicRace', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicRace')\r\n",
							"#PersonEmailAddress\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonEmailAddress', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonEmailAddress')\r\n",
							"#PersonIdentifier\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonIdentifier', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonIdentifier')\r\n",
							"#PersonOrganizationRole\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonOrganizationRole', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonOrganizationRole')\r\n",
							"#PersonPhoneNumber\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonPhoneNumber', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonPhoneNumber')\r\n",
							"#PersonRelationship\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonRelationship', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonRelationship')\r\n",
							"#RefDefinition\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/RefDefinition', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/RefDefinition')\r\n",
							"#Section\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Section', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Section')\r\n",
							"#SectionGradeLevel\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/SectionGradeLevel', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionGradeLevel')\r\n",
							"#SectionSession\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/SectionSession', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionSession')\r\n",
							"#SectionSubject\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/SectionSubject', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionSubject')\r\n",
							"#Session\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Session', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Session')\r\n",
							"#SourceSystem\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/SourceSystem', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SourceSystem')\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2) Now ingest the new activity data"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"\r\n",
							"#TechActivity\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"df = spark.read.csv(stage1 + '/M365/activity/2021-06-10/*.csv', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('append').save(stage2 + '/M365/TechActivity')\r\n",
							"#df.write.format('delta').mode('overwrite').save(stage2 + '/M365/TechActivity')\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Note that you can't rely on this having the latest for some reason (you can verify that by running a comparison query in SQL)\r\n",
							"df = spark.read.load('abfss://stage2@stoeacisd3gg1.dfs.core.windows.net/M365/TechActivity', format='delta')\r\n",
							"display(df)\r\n",
							"print(df.count())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MSInsights_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d0ea12a7-bb39-4758-b9aa-63ba5a632f58"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class MSInsights(BaseOEAModule):\r\n",
							"    \"\"\"\r\n",
							"    Provides data processing methods for MS Insights data.\r\n",
							"    Data is expected to be received via ADS into stage1np/ms_insights\r\n",
							"    The structure of the folders in stage1np will then be something like:\r\n",
							"        -> stage1np/ms_insights/activity/2021-06-02\r\n",
							"            -> stage1np/ms_insights/activity/2021-06-02/ApplicationUsage.csv\r\n",
							"        -> stage1np/ms_insights/roster/2021-06-02T06-05-11/\r\n",
							"            -> stage1np/ms_insights/roster/2021-06-02T06-05-11/AadUser\r\n",
							"            -> stage1np/ms_insights/roster/2021-06-02T06-05-11/Person\r\n",
							"            etc\r\n",
							"\r\n",
							"    In stage2, everything is written to stage2np/ms_insights and stage2p/ms_insights\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    def __init__(self, oea, source_folder='ms_insights'):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder)\r\n",
							"\r\n",
							"        self.stage1np_activity = self.stage1np + '/activity'\r\n",
							"        self.stage1np_roster = self.stage1np + '/roster'\r\n",
							"\r\n",
							"        self.schemas['TechActivity'] = [['SignalType', 'string', 'no-op'],\r\n",
							"                        ['StartTime', 'timestamp', 'no-op'],\r\n",
							"                        ['UserAgent', 'string', 'no-op'],\r\n",
							"                        ['SignalId', 'string', 'no-op'],\r\n",
							"                        ['SisClassId', 'string', 'no-op'],\r\n",
							"                        ['ClassId', 'string', 'no-op'],\r\n",
							"                        ['ChannelId', 'string', 'no-op'],\r\n",
							"                        ['AppName', 'string', 'no-op'],\r\n",
							"                        ['ActorId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['ActorRole', 'string', 'no-op'],\r\n",
							"                        ['SchemaVersion', 'string', 'no-op'],\r\n",
							"                        ['AssignmentId', 'string', 'no-op'],\r\n",
							"                        ['SubmissionId', 'string', 'no-op'],\r\n",
							"                        ['Action', 'string', 'no-op'],\r\n",
							"                        ['DueDate', 'timestamp', 'no-op'],\r\n",
							"                        ['ClassCreationDate', 'timestamp', 'no-op'],\r\n",
							"                        ['Grade', 'string', 'no-op'],\r\n",
							"                        ['SourceFileExtension', 'string', 'no-op'],\r\n",
							"                        ['MeetingDuration', 'integer', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['AadGroup'] = [['ObjectId', 'string', 'hash'],\r\n",
							"                        ['DisplayName', 'string', 'mask'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Mail', 'string', 'mask'],\r\n",
							"                        ['MailNickname', 'string', 'mask'],\r\n",
							"                        ['AnchorId', 'string', 'hash'],\r\n",
							"                        ['SectionId', 'string', 'no-op']]                           \r\n",
							"        self.schemas['AadGroupMembership'] = [['GroupObjectId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Role', 'string', 'no-op'],\r\n",
							"                        ['UserObjectId', 'string', 'hash-no-lookup']]  \r\n",
							"        self.schemas['AadUser'] = [['ObjectId', 'string', 'hash'],\r\n",
							"                        ['AnchorId', 'string', 'hash'],\r\n",
							"                        ['DisplayName', 'string', 'mask'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['GivenName', 'string', 'mask'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Mail', 'string', 'mask'],\r\n",
							"                        ['MailNickname', 'string', 'mask'],\r\n",
							"                        ['Role', 'string', 'no-op'],\r\n",
							"                        ['Surname', 'string', 'mask'],\r\n",
							"                        ['UserPrincipalName', 'string', 'hash'],\r\n",
							"                        ['StudentId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['TeacherId', 'string', 'hash-no-lookup']] \r\n",
							"        self.schemas['AadUserPersonMapping'] = [['ObjectId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup']] \r\n",
							"        self.schemas['Course'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['AcademicYearSessionId', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['IsActiveInSession', 'boolean', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op'],\r\n",
							"                        ['OrganizationId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['Code', 'string', 'no-op']] \r\n",
							"        self.schemas['CourseGradeLevel'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['CourseId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['RefGradeLevelId', 'string', 'no-op']] \r\n",
							"        self.schemas['CourseSubject'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['CourseId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['RefAcademicSubjectId', 'string', 'no-op']] \r\n",
							"        self.schemas['Enrollment'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['IsActiveInSession', 'boolean', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefSectionRoleId', 'string', 'no-op'],\r\n",
							"                        ['SectionId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['EntryDate', 'string', 'no-op'],\r\n",
							"                        ['ExitDate', 'string', 'no-op'],\r\n",
							"                        ['IsPrimaryStaffForSection', 'boolean', 'no-op']] \r\n",
							"        self.schemas['Organization'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op'],\r\n",
							"                        ['RefOrganizationTypeId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['Identifier', 'string', 'no-op'],\r\n",
							"                        ['ParentOrganizationId', 'string', 'no-op']] \r\n",
							"        self.schemas['Person'] = [['Id', 'string', 'hash'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['GivenName', 'string', 'mask'],\r\n",
							"                        ['MiddleName', 'string', 'mask'],\r\n",
							"                        ['PreferredGivenName', 'string', 'mask'],\r\n",
							"                        ['PreferredMiddleName', 'string', 'mask'],\r\n",
							"                        ['PreferredSurname', 'string', 'mask'],\r\n",
							"                        ['Surname', 'string', 'mask']] \r\n",
							"        self.schemas['PersonDemographic'] = [['PersonId', 'string', 'hash'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['BirthCity', 'string', 'mask'],\r\n",
							"                        ['BirthCountryCode', 'string', 'mask'],\r\n",
							"                        ['BirthDate', 'string', 'mask'],\r\n",
							"                        ['BirthState', 'string', 'mask'],\r\n",
							"                        ['RefSexId', 'string', 'mask']] \r\n",
							"        self.schemas['PersonDemographicEthnicity'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefEthnicityId', 'string', 'mask']] \r\n",
							"        self.schemas['PersonDemographicPersonFlag'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefPersonFlagId', 'string', 'mask']] \r\n",
							"        self.schemas['PersonDemographicRace'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefRaceId', 'string', 'mask']] \r\n",
							"        self.schemas['PersonEmailAddress'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['EmailAddress', 'string', 'mask'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['PriorityOrder', 'short', 'no-op'],\r\n",
							"                        ['RefEmailAddressTypeId', 'string', 'no-op']] \r\n",
							"        self.schemas['PersonIdentifier'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Identifier', 'string', 'mask'],\r\n",
							"                        ['IsPresentInSource', 'boolean', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefIdentifierTypeId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op']] \r\n",
							"        self.schemas['PersonOrganizationRole'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['IsActiveInSession', 'boolean', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['OrganizationId', 'string', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefRoleId', 'string', 'no-op'],\r\n",
							"                        ['SessionId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['IsPrimary', 'boolean', 'no-op'],\r\n",
							"                        ['RefGradeLevelId', 'string', 'no-op'],\r\n",
							"                        ['RoleEndDate', 'string', 'mask'],\r\n",
							"                        ['RoleStartDate', 'string', 'mask']] \r\n",
							"        self.schemas['PersonPhoneNumber'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['PhoneNumber', 'string', 'mask'],\r\n",
							"                        ['PriorityOrder', 'short', 'no-op'],\r\n",
							"                        ['RefPhoneNumberTypeId', 'string', 'no-op']] \r\n",
							"        self.schemas['PersonRelationship'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['PersonId', 'string', 'hash-no-lookup'],\r\n",
							"                        ['RefPersonRelationshipId', 'string', 'no-op'],\r\n",
							"                        ['RelatedPersonId', 'string', 'hash-no-lookup']] \r\n",
							"        self.schemas['RefDefinition'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['Code', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Namespace', 'string', 'no-op'],\r\n",
							"                        ['RefType', 'string', 'no-op'],\r\n",
							"                        ['SortOrder', 'short', 'no-op']] \r\n",
							"        self.schemas['Section'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op'],\r\n",
							"                        ['OrganizationId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['Code', 'string', 'no-op'],\r\n",
							"                        ['CourseId', 'string', 'no-op'],\r\n",
							"                        ['Location', 'string', 'no-op']] \r\n",
							"        self.schemas['SectionGradeLevel'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['RefGradeLevelId', 'string', 'no-op'],\r\n",
							"                        ['SectionId', 'string', 'no-op']] \r\n",
							"        self.schemas['SectionSession'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['IsActiveInSession', 'boolean', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['SectionId', 'string', 'no-op'],\r\n",
							"                        ['SessionId', 'string', 'no-op']] \r\n",
							"        self.schemas['SectionSubject'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['RefAcademicSubjectId', 'string', 'no-op'],\r\n",
							"                        ['SectionId', 'string', 'no-op']] \r\n",
							"        self.schemas['Session'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['EndDate', 'string', 'no-op'],\r\n",
							"                        ['ExternalId', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op'],\r\n",
							"                        ['RefAcademicYearId', 'string', 'no-op'],\r\n",
							"                        ['RefSessionTypeId', 'string', 'no-op'],\r\n",
							"                        ['SourceSystemId', 'string', 'no-op'],\r\n",
							"                        ['StartDate', 'string', 'no-op'],\r\n",
							"                        ['ParentSessionId', 'string', 'no-op']] \r\n",
							"        self.schemas['SourceSystem'] = [['Id', 'string', 'no-op'],\r\n",
							"                        ['FirstSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['LastSeenDateTime', 'timestamp', 'no-op'],\r\n",
							"                        ['Name', 'string', 'no-op']] \r\n",
							"    \r\n",
							"    def process_activity(self):\r\n",
							"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights activity data from: \" + self.stage1np_activity)\r\n",
							"\r\n",
							"        spark_schema = self.oea.to_spark_schema(self.schemas['TechActivity'])\r\n",
							"        df = spark.readStream.csv(self.stage1np_activity + '/*/*.csv', header='false', schema=spark_schema)\r\n",
							"        df = df.dropDuplicates(['SignalId'])\r\n",
							"        df = df.withColumn('year', F.year(F.col('StartTime'))).withColumn('month', F.month(F.col('StartTime')))\r\n",
							"\r\n",
							"        df_pseudo, df_lookup = self.oea.pseudonymize(df, self.schemas['TechActivity'])\r\n",
							"\r\n",
							"        query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_activity + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"        query = query.start(self.stage2p + '/TechActivity')\r\n",
							"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
							"        logger.info(query.lastProgress)\r\n",
							"\r\n",
							"    def reset_activity_processing(self):\r\n",
							"        \"\"\" Resets all TechActivity processing. This is intended for use during initial testing - use with caution.\r\n",
							"            - deletes the _checkpoints dir from stage1/M365/activity\r\n",
							"            - deletes the delta table at stage2/ms_insights/TechActivity\r\n",
							"        \"\"\"\r\n",
							"        self.oea.rm_if_exists(self.stage2p + '/TechActivity')\r\n",
							"        self.oea.rm_if_exists(self.stage1np_activity + '/_checkpoints')\r\n",
							"        logger.info(f\"Deleted {self.stage2p + '/TechActivity'} and {self.stage1np_activity + '/_checkpoints'}\")  \r\n",
							"\r\n",
							"    def _process_roster_entity(self, path, entity):\r\n",
							"        try:\r\n",
							"            logger.debug(f\"Processing roster entity: path={path}, entity={entity}\")\r\n",
							"            spark_schema = self.oea.to_spark_schema(self.schemas[entity])\r\n",
							"            df = spark.read.csv(path + '/' + entity, header='false', schema=spark_schema)\r\n",
							"            df_pseudo, df_lookup = self.oea.pseudonymize(df, self.schemas[entity])\r\n",
							"\r\n",
							"            if len(df_pseudo.columns) > 0: \r\n",
							"                df_pseudo.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2p + '/' + entity)\r\n",
							"            if len(df_lookup.columns) > 0: \r\n",
							"                df_lookup.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/' + entity + '_lookup')\r\n",
							"\r\n",
							"        except (AnalysisException) as error:\r\n",
							"            logger.exception(str(error))\r\n",
							"\r\n",
							"    def _process_roster_date_folder(self, date_folder_path):\r\n",
							"        folders = self.oea.get_folders(date_folder_path)\r\n",
							"        for table_name in folders:\r\n",
							"            self._process_roster_entity(date_folder_path, table_name)\r\n",
							"\r\n",
							"    def process_roster(self):\r\n",
							"        \"\"\" Processes all roster data in stage1 and writes out to stage2 and stage2p \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights roster data from: \" + self.stage1np)\r\n",
							"\r\n",
							"        items = mssparkutils.fs.ls(self.stage1np + '/roster')\r\n",
							"        for item in items:\r\n",
							"            if item.isDir:\r\n",
							"                self._process_roster_date_folder(item.path)\r\n",
							"                mssparkutils.fs.mv(item.path, self.stage1np + '/roster_processed/', True)\r\n",
							"\r\n",
							"    def reset_roster_processing(self):\r\n",
							"        \"\"\" Resets all stage1 to stage2 processing of roster data. \"\"\"\r\n",
							"        # cleanup stage2np\r\n",
							"        if self.oea.path_exists(self.stage2np):\r\n",
							"            # Delete roster delta tables (everything other than TechActivity)\r\n",
							"            items = mssparkutils.fs.ls(self.stage2np)\r\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"            for item in items:\r\n",
							"                if item.name != 'TechActivity':\r\n",
							"                    mssparkutils.fs.rm(item.path, True)\r\n",
							"        # cleanup stage2p\r\n",
							"        if self.oea.path_exists(self.stage2p):\r\n",
							"            # Delete roster delta tables (everything other than TechActivity)\r\n",
							"            items = mssparkutils.fs.ls(self.stage2p)\r\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"            for item in items:\r\n",
							"                if item.name != 'TechActivity':\r\n",
							"                    mssparkutils.fs.rm(item.path, True)                    \r\n",
							"\r\n",
							"        if self.oea.path_exists(self.stage1np + '/roster_processed'):\r\n",
							"            # Move roster data back in to \"inbound\" folder\r\n",
							"            items = mssparkutils.fs.ls(self.stage1np + '/roster_processed')\r\n",
							"            for item in items:\r\n",
							"                print(item.path)\r\n",
							"                mssparkutils.fs.mv(item.path, self.stage1np + '/roster', True)\r\n",
							"        logger.info(\"Done. Removed roster data from stage2 and moved everything in stage1 from roster_processed folder back into stage1np/ms_insights/roster\")\r\n",
							"  \r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Main')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "1) Implementation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/OEA_Core"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/Modules/M365/M365_lib"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"process_activity()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"df = spark.sql(\"SELECT * FROM tutorial_01.person\")\n",
							"# df.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a6cd93af-efd4-4fa1-a25f-ce4f53acc793"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3dc1c569-c4c6-4dd7-bb3f-4eded507376b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"spark.sql(\"DROP DATABASE IF EXISTS tutorial_01 CASCADE\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark3v3",
						"name": "spark3v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"import pytest"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"orders_schema = [\"OrderId\",\"OrderDate\", \"Region\", \"City\", \"Category\",\"Product\",\"Quantity\",\"UnitPrice\",\"TotalPrice\"]"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def load_data():\r\n",
							"    # N.B. this would actually be loading the real data from somewhere in the data lake, or elsewhere...\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            (1,\"01/01/2020\",\"East\",\"Boston\",\"Bars\",\"Carrot\",33,1.77,58.41),\r\n",
							"            (2,\"04/01/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",87,3.49,303.63),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (11,\"31/01/2020\",\"East\",\"Boston\",\"Cookies\",\"Arrowroot\",36,2.18,78.48),\r\n",
							"            (12,\"03/02/2020\",\"East\",\"Boston\",\"Cookies\",\"Chocolate Chip\",31,1.87,57.97),\r\n",
							"            (13,\"06/02/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",28,3.49,97.72)      \r\n",
							"        ],\r\n",
							"        orders_schema \r\n",
							"    )\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def remove_duplicate_orders(df):\r\n",
							"    # Swap these lines over to fail test\r\n",
							"    #return df \r\n",
							"    #return df.distinct()\r\n",
							"    return df.dropDuplicates([\"OrderId\"])"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Tests\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def mytest():\r\n",
							"    df = load_data()\r\n",
							"    df = remove_duplicate_orders(df)\r\n",
							"    #Assert\r\n",
							"    expected_orders = 6\r\n",
							"    number_of_orders = df.count()\r\n",
							"    assert number_of_orders == 6, f'Expected {expected_orders} order after remove_duplicate_orders() but {number_of_orders} returned.'"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark3v3",
						"name": "spark3v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /testing/Notebook2"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df = load_data()\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#mssparkutils.notebook.run(\"notebook path\", <timeoutSeconds>, <parameterMap>)\r\n",
							"#mssparkutils.notebook.run(\"folder/Sample1\", 90, {\"input\": 20 })\r\n",
							"mssparkutils.notebook.run(\"Notebook2\")\r\n",
							"df = load_data()\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_Core')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"instrumentation_key = 'InstrumentationKey=5a4a6026-f008-4e9e-a48a-96a412d250d6'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\r\n",
							"oea_path = 'abfss://oea-framework@' + storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"spark = SparkSession.builder.appName(\"OEA_Framework\").getOrCreate()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Imports\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from notebookutils import mssparkutils\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"from pyspark.sql.utils import AnalysisException, FileNotFoundException"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Setup logging"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.DEBUG) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string=instrumentation_key))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Function lib"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Returns true if the path exists\r\n",
							"def path_exists(path):\r\n",
							"    tableExists = False\r\n",
							"    try:\r\n",
							"        items = mssparkutils.fs.ls(path)\r\n",
							"        tableExists = True\r\n",
							"    except Exception as e:\r\n",
							"        # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\r\n",
							"        pass\r\n",
							"    return tableExists\r\n",
							"\r\n",
							"def file_exists(path):\r\n",
							"    return folder_exists(path)\r\n",
							"\r\n",
							"def folder_exists(path):\r\n",
							"    return folder_exists(path)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_Utils')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Microsoft Spark Utilities\r\n",
							"[Intro to Microsoft Spark Utilities](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"# to list out all of the available commands\r\n",
							"mssparkutils.fs.help()\r\n",
							"\r\n",
							"# an example of deleting a directory (and everything within it)\r\n",
							"mssparkutils.fs.rm('abfss://stage1@stoeacisd3gg1.dfs.core.windows.net/tutorial_01', True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# delete from M365 stage2, move files from \"processed\" back into \"inbound\" in stage1\r\n",
							"from notebookutils import mssparkutils\r\n",
							"mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"mssparkutils.fs.mv('abfss://stage1@stoeacisd3gg1.dfs.core.windows.net/M365/processed/roster/2021-06-15T04-04-12', stage1 + '/M365/inbound/roster', True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Logging example. More info at: https://github.com/balakreshnan/Samples2021/blob/main/Synapseworkspace/opencensuslog.md#azure-synapse-spark-logs-runtime-errors-to-application-insights\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.INFO) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=<insert instrumentation key here>'))\r\n",
							"\r\n",
							"logger.info(\"OEA info msg\")\r\n",
							"logger.debug(\"OEA: debug msg\")\r\n",
							"logger.debug(\"OEA: warning msg\")\r\n",
							"logger.error(\"OEA: error msg\")\r\n",
							"logger.exception(\"OEA: exception msg\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Check on existence of a folder in the data lake.\r\n",
							"\r\n",
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"def folderExists(path, foldername):\r\n",
							"    items = mssparkutils.fs.ls(path)\r\n",
							"    tableExists = False\r\n",
							"    for item in items:\r\n",
							"        if item.name == foldername:\r\n",
							"            tableExists = True\r\n",
							"    return tableExists\r\n",
							"\r\n",
							"#print(folderExists(stage1 + '/tutorial_01/activity/2021-06-10', 'TechActivity'))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Create an empty delta table if one doesn't yet exist\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import *\r\n",
							"\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"if not DeltaTable.isDeltaTable(spark, stage2 + '/tutorial_01/TechActivity'):\r\n",
							"    emptyDF = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\r\n",
							"    emptyDF.write.format('delta').mode('overwrite').save(stage2 + '/tutorial_01/TechActivity')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							"\r\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Structured Streaming\r\n",
							"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"#https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').start(dest_path)\r\n",
							"\r\n",
							"query.stop()\r\n",
							"print(query.id)\r\n",
							"print(query.explain())\r\n",
							"print(spark.streams.active)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_connector')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark4v3p1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 4,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "4",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "8ca98359-6b2f-4de4-8551-eea07156a68c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark4v3p1",
						"name": "spark4v3p1",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark4v3p1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# OEA connector\n",
							"This notebook provides a way for invoking methods on the OEA framework or supporting modules from a pipeline.\n",
							"\n",
							"When setting up a new module, be sure to include a new cell below that imports that module, so that its methods can be invoked by pipelines."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# These values should be passed in from the pipeline that is using this notebook as an activity.\r\n",
							"# Note that kwargs allows you to pass in a dict of params, but the dict has to specified as a string when invoked from a pipeline.\r\n",
							"# Also note that you can refer to attributes of an object in the params, for example: {'path':oea.stage2np}\r\n",
							"object_name = 'oea'\r\n",
							"method_name = ''\r\n",
							"kwargs = '{}'"
						],
						"outputs": [],
						"execution_count": 78
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /ContosoSIS_py"
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.create_lake_db(2, 'contoso_sis')"
						],
						"outputs": [],
						"execution_count": 76
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.drop_sql_db('sqls2_contoso_sis')"
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea.create_sql_db()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"obj = eval(object_name)\r\n",
							"kwargs = eval(kwargs)\r\n",
							"m = getattr(obj, method_name)\r\n",
							"m(**kwargs)"
						],
						"outputs": [],
						"execution_count": 39
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6410c1e1-ef95-44b8-8e40-08c5efed98c7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"from delta.tables import DeltaTable\n",
							"from notebookutils import mssparkutils\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType, ShortType\n",
							"from pyspark.sql import functions as F\n",
							"from pyspark.sql.utils import AnalysisException\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\n",
							"import pandas as pd\n",
							"import sys\n",
							"import re\n",
							"import json\n",
							"import datetime\n",
							"import random\n",
							"import io\n",
							"\n",
							"logger = logging.getLogger('OEA')\n",
							"\n",
							"class OEA:\n",
							"    def __init__(self, storage_account='', instrumentation_key='', salt='', logging_level=logging.DEBUG):\n",
							"        if storage_account:\n",
							"            self.storage_account = storage_account\n",
							"        else:\n",
							"            oea_id = mssparkutils.env.getWorkspaceName()[8:] # extracts the OEA id for this OEA instance from the synapse workspace name (based on OEA naming convention)\n",
							"            self.storage_account = 'stoea' + oea_id # sets the name of the storage account based on OEA naming convention\n",
							"        self.serverless_sql_endpoint = mssparkutils.env.getWorkspaceName() + '-ondemand.sql.azuresynapse.net'\n",
							"        self._initialize_logger(instrumentation_key, logging_level)\n",
							"        self.salt = salt\n",
							"        self.stage1np = 'abfss://stage1np@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.stage2np = 'abfss://stage2np@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.stage2p = 'abfss://stage2p@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.stage3np = 'abfss://stage3np@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.stage3p = 'abfss://stage3p@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.framework_path = 'abfss://oea-framework@' + self.storage_account + '.dfs.core.windows.net'\n",
							"        self.registered_modules = {}\n",
							"\n",
							"        logger.debug(\"OEA initialized.\")\n",
							"\n",
							"    def path(self, container_name, directory_path=None):\n",
							"        if directory_path:\n",
							"            return f'abfss://{container_name}@{self.storage_account}.dfs.core.windows.net/{directory_path}'\n",
							"        else:\n",
							"            return f'abfss://{container_name}@{self.storage_account}.dfs.core.windows.net'            \n",
							"\n",
							"    def _initialize_logger(self, instrumentation_key, logging_level):\n",
							"        logging.lastResort = None\n",
							"        # the logger will print an error like \"ValueError: I/O operation on closed file\" because we're trying to have log messages also print to stdout\n",
							"        # and apparently this causes issues on some of the spark executor nodes. The bottom line is that we don't want these logging errors to get printed in the notebook output.\n",
							"        logging.raiseExceptions = False\n",
							"        logger.setLevel(logging_level)\n",
							"\n",
							"        handler = logging.StreamHandler(sys.stdout)\n",
							"        handler.setLevel(logging_level)\n",
							"        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
							"        handler.setFormatter(formatter)\n",
							"        logger.addHandler(handler)\n",
							"\n",
							"        if instrumentation_key:\n",
							"            # Setup logging to go to app insights (more info here: https://github.com/balakreshnan/Samples2021/blob/main/Synapseworkspace/opencensuslog.md#azure-synapse-spark-logs-runtime-errors-to-application-insights)\n",
							"            self.logger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=' + instrumentation_key))\n",
							"\n",
							"\n",
							"    def load(self, folder, table, stage=None, data_format='delta'):\n",
							"        \"\"\" Loads a dataframe based on the path specified in the given args \"\"\"\n",
							"        if stage is None: stage = self.stage2p\n",
							"        path = f\"{stage}/{folder}/{table}\"\n",
							"        try:\n",
							"            df = spark.read.load(f\"{stage}/{folder}/{table}\", format=data_format)\n",
							"            return df        \n",
							"        except AnalysisException as e:\n",
							"            raise ValueError(\"Failed to load. Are you sure you have the right path?\\nMore info below:\\n\" + str(e))\n",
							"\n",
							"    def load_from_stage1(self, path_and_filename, data_format='csv'):\n",
							"        \"\"\" Loads a dataframe with data from stage1, based on the path specified in the given args \"\"\"\n",
							"        path = f\"{self.stage1np}/{path_and_filename}\"\n",
							"        df = spark.read.load(path, format=data_format)\n",
							"        return df        \n",
							"\n",
							"    def load_sample_from_csv_file(self, path_and_filename, header=True, stage=None):\n",
							"        \"\"\" Loads a sample from the specified csv file and returns a pandas dataframe.\n",
							"            Ex: print(load_sample_from_csv_file('/student_data/students.csv'))\n",
							"        \"\"\"\n",
							"        if stage is None: stage = self.stage1np\n",
							"        csv_str = mssparkutils.fs.head(f\"{stage}/{path_and_filename}\") # https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#preview-file-content\n",
							"        complete_lines = re.match(r\".*\\n\", csv_str, re.DOTALL).group(0)\n",
							"        if header: header = 0 # for info on why this is needed: https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.read_csv.html\n",
							"        else: header = None\n",
							"        pdf = pd.read_csv(io.StringIO(complete_lines), sep=',', header=header)\n",
							"        return pdf\n",
							"\n",
							"    def print_stage(self, path):\n",
							"        \"\"\" Prints out the highlevel contents of the specified stage.\"\"\"\n",
							"        msg = path + \"\\n\"\n",
							"        folders = self.get_folders(path)\n",
							"        for folder_name in folders:\n",
							"            entities = self.get_folders(path + '/' + folder_name)\n",
							"            msg += f\"{folder_name}: {entities}\\n\"\n",
							"        print(msg)            \n",
							"\n",
							"    def fix_column_names(self, df):\n",
							"        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\n",
							"        df_with_valid_column_names = df.select([F.col(col).alias(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", col)) for col in df.columns])\n",
							"        return df_with_valid_column_names\n",
							"\n",
							"    def to_spark_schema(self, schema):#: list[list[str]]):\n",
							"        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \n",
							"            Example:\n",
							"            schemas['Person'] = [['Id','string','hash'],\n",
							"                                    ['CreateDate','timestamp','no-op'],\n",
							"                                    ['LastModifiedDate','timestamp','no-op']]\n",
							"            to_spark_schema(schemas['Person'])\n",
							"        \"\"\"\n",
							"        fields = []\n",
							"        for col_name, dtype, op in schema:\n",
							"            fields.append(StructField(col_name, globals()[dtype.lower().capitalize() + \"Type\"](), True))\n",
							"        spark_schema = StructType(fields)\n",
							"        return spark_schema\n",
							"\n",
							"    def ingest_incremental_csv_data(self, source_system, tablename, schema, partition_by, primary_key='id', has_header=True):\n",
							"        \"\"\" Processes incremental batch data from stage1 into stage2 \"\"\"\n",
							"        source_path = f'{self.stage1np}/{source_system}/{tablename}'\n",
							"        p_destination_path = f'{self.stage2p}/{source_system}/{tablename}_pseudo'\n",
							"        np_destination_path = f'{self.stage2np}/{source_system}/{tablename}_lookup'\n",
							"        logger.info(f'Processing incremental data from: {source_path} and writing out to: {p_destination_path}')\n",
							"\n",
							"        spark_schema = self.to_spark_schema(schema)\n",
							"        if has_header: header_flag = 'true'\n",
							"        else: header_flag = 'false'\n",
							"        df = spark.readStream.csv(source_path + '/**/*.csv', header=header_flag, schema=spark_schema)\n",
							"        #df = spark.read.csv(source_path + '/**/*.csv', header=header_flag, schema=spark_schema)\n",
							"        #display(df)\n",
							"        df = df.dropDuplicates([primary_key])\n",
							"        df_pseudo, df_lookup = self.pseudonymize(df, schema)\n",
							"\n",
							"        if len(df_pseudo.columns) == 0:\n",
							"            logger.info('No data to be written to stage2p')\n",
							"        else:        \n",
							"            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_p').partitionBy(partition_by)\n",
							"            query = query.start(p_destination_path)\n",
							"            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
							"            logger.info(query.lastProgress)\n",
							"\n",
							"        if len(df_lookup.columns) == 0:\n",
							"            logger.info('No data to be written to stage2np')\n",
							"        else:\n",
							"            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_np').partitionBy(partition_by)\n",
							"            query2 = query2.start(np_destination_path)\n",
							"            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
							"            logger.info(query2.lastProgress)        \n",
							"\n",
							"    def pseudonymize(self, df, schema): #: list[list[str]]):\n",
							"        \"\"\" Performs pseudonymization of the given dataframe based on the provided schema.\n",
							"            For example, if the given df is for an entity called person, \n",
							"            2 dataframes will be returned, one called person that has hashed ids and masked fields, \n",
							"            and one called person_lookup that contains the original person_id, person_id_pseudo,\n",
							"            and the non-masked values for columns marked to be masked.\"\"\"\n",
							"        \n",
							"        df_pseudo = df_lookup = df\n",
							"\n",
							"        for col_name, dtype, op in schema:\n",
							"            if op == \"hash-no-lookup\" or op == \"hnl\":\n",
							"                # This means that the lookup can be performed against a different table so no lookup is needed.\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
							"                df_lookup = df_lookup.drop(col_name)           \n",
							"            elif op == \"hash\" or op == 'h':\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\n",
							"                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256))\n",
							"            elif op == \"mask\" or op == 'm':\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\n",
							"            elif op == \"partition-by\":\n",
							"                pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\n",
							"            elif op == \"no-op\" or op == 'x':\n",
							"                df_lookup = df_lookup.drop(col_name)\n",
							"\n",
							"        df_pseudo = self.fix_column_names(df_pseudo)\n",
							"        df_lookup = self.fix_column_names(df_lookup)\n",
							"\n",
							"        return (df_pseudo, df_lookup)\n",
							"\n",
							"    # Returns true if the path exists\n",
							"    def path_exists(self, path):\n",
							"        tableExists = False\n",
							"        try:\n",
							"            items = mssparkutils.fs.ls(path)\n",
							"            tableExists = True\n",
							"        except Exception as e:\n",
							"            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\n",
							"            pass\n",
							"        return tableExists\n",
							"\n",
							"    def ls(self, path):\n",
							"        folders = []\n",
							"        files = []\n",
							"        try:\n",
							"            items = mssparkutils.fs.ls(path)\n",
							"            for item in items:\n",
							"                if item.isFile:\n",
							"                    files.append(item.name)\n",
							"                elif item.isDir:\n",
							"                    folders.append(item.name)\n",
							"        except Exception as e:\n",
							"            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
							"        return (folders, files)\n",
							"\n",
							"    def print_stage(self, path):\n",
							"        print(path)\n",
							"        folders = self.get_folders(path)\n",
							"        for folder_name in folders:\n",
							"            entities = self.get_folders(path + '/' + folder_name)\n",
							"            print(f\"{folder_name}: {entities}\")\n",
							"\n",
							"    # Return the list of folders found in the given path.\n",
							"    def get_folders(self, path):\n",
							"        dirs = []\n",
							"        try:\n",
							"            items = mssparkutils.fs.ls(path)\n",
							"            for item in items:\n",
							"                #print(item.name, item.isDir, item.isFile, item.path, item.size)\n",
							"                if item.isDir:\n",
							"                    dirs.append(item.name)\n",
							"        except Exception as e:\n",
							"            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\n",
							"        return dirs\n",
							"\n",
							"    def get_latest_folder(self, path):\n",
							"        folders = oea.get_folders(path)\n",
							"        if len(folders) > 0: return folders[-1]\n",
							"        else: return None\n",
							"\n",
							"    # Remove a folder if it exists (defaults to use of recursive removal).\n",
							"    def rm_if_exists(self, path, recursive_remove=True):\n",
							"        try:\n",
							"            mssparkutils.fs.rm(path, recursive_remove)\n",
							"        except Exception as e:\n",
							"            pass\n",
							"\n",
							"    def pop_from_path(self, path):\n",
							"        \"\"\" Pops the last arg in a path and returns the path and the last arg as a tuple.\n",
							"            pop_from_path('abfss://stage2@xyz.dfs.core.windows.net/ms_insights/test.csv') # returns ('abfss://stage2@xyz.dfs.core.windows.net/ms_insights', 'test.csv')\n",
							"        \"\"\"\n",
							"        m = re.match(r\"(.*)\\/([^/]+)\", path)\n",
							"        return (m.group(1), m.group(2))\n",
							"\n",
							"    def parse_source_path(self, path):\n",
							"        \"\"\" Parses a path that looks like this: abfss://stage2p@stoeacisd3ggimpl3.dfs.core.windows.net/ms_insights\n",
							"            and returns a dictionary like this: {'stage_num': '2', 'ss': 'ms_insights'}\n",
							"            Note that it will also return a 'stage_num' of 2 if the path is stage2p or stage2np - this is by design because the spark db with the s2 prefix will be used for data in stage2 and stage2p.\n",
							"        \"\"\"\n",
							"        m = re.match(r\".*:\\/\\/stage(?P<stage_num>\\d+)[n]?[p]?@[^/]+\\/(?P<ss>[^/]+)\", path)\n",
							"        return m.groupdict()\n",
							"    \n",
							"    def create_lake_db(self, stage_num, source_dir, source_format='DELTA'):\n",
							"        \"\"\" Creates a spark db that points to data in the given stage under the specified source directory (assumes that every folder in the source_dir is a table).\n",
							"            Example: create_lake_db(2, 'contoso_sis')\n",
							"            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
							"        \"\"\"\n",
							"        db_name = f's{stage_num}_{source_dir}'\n",
							"        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
							"        self.create_lake_views(db_name, self.path(f'stage{stage_num}p', source_dir), source_format)\n",
							"        self.create_lake_views(db_name, self.path(f'stage{stage_num}np', source_dir), source_format)\n",
							"        result = \"Database created: \" + db_name\n",
							"        logger.info(result)\n",
							"        return result        \n",
							"\n",
							"    def create_lake_views(self, db_name, source_path, source_format):\n",
							"        dirs = self.get_folders(source_path)\n",
							"        for table_name in dirs:\n",
							"            spark.sql(f\"create table if not exists {db_name}.{table_name} using {source_format} location '{source_path}/{table_name}'\")\n",
							"\n",
							"    def drop_lake_db(self, db_name):\n",
							"        spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
							"        result = \"Database dropped: \" + db_name\n",
							"        logger.info(result)\n",
							"        return result       \n",
							"\n",
							"    def create_sql_db(self, stage_num, source_dir, source_format='DELTA'):\n",
							"        \"\"\" Creates the script for creating db based on the given path (assumes that every folder in the given path is a table).\n",
							"            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
							"        \"\"\"\n",
							"        cmd = \"\"\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '@{variables('sqlDBName')}') \n",
							"                BEGIN\n",
							"                CREATE DATABASE @{variables('sqlDBName')}; \n",
							"                END;'\n",
							"        \"\"\"\n",
							"        print(cmd)\n",
							"        result = \"Database created: \" + db_name\n",
							"        logger.info(result)\n",
							"        return result\n",
							"\n",
							"    def drop_sql_db(self, db_name):\n",
							"        print('Copy and paste this into a sql script and execute it:\\n')\n",
							"        print(f'DROP {db_name}')\n",
							"\n",
							"    # List installed packages\n",
							"    def list_packages(self):\n",
							"        import pkg_resources\n",
							"        for d in pkg_resources.working_set:\n",
							"            print(d)\n",
							"\n",
							"    def print_schema_starter(self, entity_name, df):\n",
							"        \"\"\" Prints a starter schema that can be modified as needed when developing the oea schema for a new module. \"\"\"\n",
							"        st = f\"self.schemas['{entity_name}'] = [\"\n",
							"        for col in df.schema:\n",
							"            st += f\"['{col.name}', '{str(col.dataType)[:-4].lower()}', 'no-op'],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\"\n",
							"        return st[:-11] + ']'\n",
							"\n",
							"    def write_rows_as_csv(data, folder, filename, container=None):\n",
							"        \"\"\" Writes a dictionary as a csv to the specified location. This is helpful when creating test data sets and landing them in stage1np.\n",
							"            data = [{'id':'1','fname':'John'}, {'id':'1','fname':'Jane'}]\n",
							"        \"\"\"\n",
							"        if container == None: container = self.stage1np\n",
							"        pdf = pd.DataFrame(data)\n",
							"        mssparkutils.fs.put(f\"{container}/{folder}/{filename}\", pdf.to_csv(index=False), True) # True indicates overwrite mode  \n",
							"\n",
							"    def write_rowset_as_csv(data, folder, container=None):\n",
							"        \"\"\" Writes out as csv rows the passed in data. The inbound data should be in a format like this:\n",
							"            data = { 'students':[{'id':'1','fname':'John'}], 'courses':[{'id':'31', 'name':'Math'}] }\n",
							"        \"\"\"\n",
							"        if container == None: container = self.stage1np\n",
							"        for entity_name, value in data.items():\n",
							"            pdf = pd.DataFrame(value)\n",
							"            mssparkutils.fs.put(f\"{container}/{folder}/{entity_name}.csv\", pdf.to_csv(index=False), True) # True indicates overwrite mode         \n",
							"\n",
							"class BaseOEAModule:\n",
							"    \"\"\" Provides data processing methods for Contoso SIS data (the student information system for the fictional Contoso school district).  \"\"\"\n",
							"    def __init__(self, source_folder, pseudonymize = True):\n",
							"        self.source_folder = source_folder\n",
							"        self.pseudonymize = pseudonymize\n",
							"        self.stage1np = f\"{oea.stage1np}/{source_folder}\"\n",
							"        self.stage2np = f\"{oea.stage2np}/{source_folder}\"\n",
							"        self.stage2p = f\"{oea.stage2p}/{source_folder}\"\n",
							"        self.stage3np = f\"{oea.stage3np}/{source_folder}\"\n",
							"        self.stage3p = f\"{oea.stage3p}/{source_folder}\"\n",
							"        self.module_path = f\"{oea.framework_path}/modules/{source_folder}\"\n",
							"        self.schemas = {}\n",
							"\n",
							"    def _process_entity_from_stage1(self, path, entity_name, format='csv', write_mode='overwrite', header='true'):\n",
							"        spark_schema = oea.to_spark_schema(self.schemas[entity_name])\n",
							"        df = spark.read.format(format).load(f\"{self.stage1np}/{path}/{entity_name}\", header=header, schema=spark_schema)\n",
							"\n",
							"        if self.pseudonymize:\n",
							"            df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas[entity_name])\n",
							"            df_pseudo.write.format('delta').mode(write_mode).save(f\"{self.stage2p}/{entity_name}\")\n",
							"            if len(df_lookup.columns) > 0:\n",
							"                df_lookup.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}_lookup\")\n",
							"        else:\n",
							"            df = oea.fix_column_names(df)   \n",
							"            df.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}\")\n",
							"\n",
							"    def delete_stage1(self):\n",
							"        oea.rm_if_exists(self.stage1np)\n",
							"\n",
							"    def delete_stage2(self):\n",
							"        oea.rm_if_exists(self.stage2np)\n",
							"        oea.rm_if_exists(self.stage2p)\n",
							"\n",
							"    def delete_stage3(self):\n",
							"        oea.rm_if_exists(self.stage3np)\n",
							"        oea.rm_if_exists(self.stage3p)                \n",
							"\n",
							"    def delete_all_stages(self):\n",
							"        self.delete_stage1()\n",
							"        self.delete_stage2()\n",
							"        self.delete_stage3()\n",
							"\n",
							"    def create_stage2_lake_db(self, format='DELTA'):\n",
							"        oea.create_lake_db(self.stage2p, format)\n",
							"        oea.create_lake_db(self.stage2np, format)\n",
							"\n",
							"    def create_stage3_lake_db(self, format='DELTA'):\n",
							"        oea.create_lake_db(self.stage3p, format)\n",
							"        oea.create_lake_db(self.stage3np, format)\n",
							"\n",
							"    def copy_test_data_to_stage1(self):\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data', self.stage1np, True)   \n",
							"\n",
							"class DataLakeWriter:\n",
							"    def __init__(self, root_destination):\n",
							"        self.root_destination = root_destination\n",
							"\n",
							"    def write(self, path_and_filename, data_str, format='csv'):\n",
							"        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist\n",
							"\n",
							"oea = OEA()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_utils_test1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Microsoft Spark Utilities\r\n",
							"[Intro to Microsoft Spark Utilities](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python)"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"# to list out all of the available commands\r\n",
							"mssparkutils.fs.help()\r\n",
							"\r\n",
							"# an example of deleting a directory (and everything within it)\r\n",
							"mssparkutils.fs.rm('abfss://stage1@stoeacisd3gg1.dfs.core.windows.net/tutorial_01', True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OurSIS')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "1) Implementation/Custom_Modules"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Reference')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Docs"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Run a notebook from a notebook\r\n",
							"this allows variables to be set and functions to be defined\r\n",
							"\r\n",
							"https://docs.microsoft.com/en-us/azure/synapse-analytics/synapse-notebook-activity?tabs=classical#run-another-synapse-notebook\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /testing/Notebook2"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/corgis_explore')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark4v3p1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "55aeabca-90e4-49f0-af2b-50c395684c73"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark4v3p1",
						"name": "spark4v3p1",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark4v3p1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"   def load_from_stage1(self, path_and_filename, data_format='csv'):\r\n",
							"        \"\"\" Loads a dataframe with data from stage1, based on the path specified in the given args \"\"\"\r\n",
							"        path = f\"{self.stage1np}/{path_and_filename}\"\r\n",
							"        df = spark.read.load(path, format=data_format)\r\n",
							"        return df   "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea = OEA()\r\n",
							"df = oea.load_from_stage1(\"corgis2/graduates/graduates.csv\")\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/corgis_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a26b369c-61bf-4993-9323-51028b14af36"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class Corgis(BaseOEAModule):\r\n",
							"    def __init__(self, oea, source_folder='corgis', pseudonymize = True):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder, pseudonymize)\r\n",
							"        self.schemas['studentattendance'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['school_year', 'integer', 'no-op'],\r\n",
							"                                            ['school_id', 'string', 'no-op'],\r\n",
							"                                            ['attendance_date', 'timestamp', 'no-op'],\r\n",
							"                                            ['all_day', 'string', 'no-op'],\r\n",
							"                                            ['Period', 'short', 'no-op'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['AttendanceCode', 'string', 'no-op'],\r\n",
							"                                            ['PresenceFlag', 'boolean', 'no-op'],\r\n",
							"                                            ['attendance_status', 'string', 'no-op'],\r\n",
							"                                            ['attendance_type', 'string', 'no-op'],\r\n",
							"                                            ['attendance_sequence', 'short', 'no-op']]\r\n",
							"\r\n",
							"\r\n",
							"                                            \r\n",
							"    def process_data_from_stage1(self):\r\n",
							"        self._process_entity_from_stage1('studentattendance', 'csv', 'overwrite', 'true')\r\n",
							"        self._process_entity_from_stage1('studentsectionmark', 'csv', 'overwrite', 'true')\r\n",
							"\r\n",
							"    def copy_test_data_to_stage1(self):\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentattendance.csv', self.stage1np + '/studentattendance/studentattendance.csv', True)\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentsectionmark.csv', self.stage1np + '/studentsectionmark/studentsectionmark.csv', True)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/data_generation_example')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "51146d45-a8cf-4f0b-ae23-b65dee3adc54"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data Generation Example\r\n",
							"This notebook demonstrates how to use the EdFiDataGenerator to generate test student data in the Ed-Fi format for as many schools as specified.\r\n",
							"\r\n",
							"To generate test Ed-Fi data, simple run this notebook.\r\n",
							"The test data will be generated in json format and written to stage1np/test_data in your data lake."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run OEA_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run DataGen_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea = OEA()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dg = EdFiDataGenerator()\r\n",
							"writer = DataLakeWriter(oea.stage1np + '/test_data')\r\n",
							"dg.generate_data(2, writer)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/example_modules_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b8653d8f-0199-45e6-ae80-594e463dbb43"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class ContosoSIS(BaseOEAModule):\n",
							"    def __init__(self, oea, source_folder='contoso_sis', pseudonymize = True):\n",
							"        BaseOEAModule.__init__(self, oea, source_folder, pseudonymize)\n",
							"        self.schemas['studentattendance'] = [['id', 'string', 'no-op'],\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\n",
							"                                            ['school_year', 'integer', 'no-op'],\n",
							"                                            ['school_id', 'string', 'no-op'],\n",
							"                                            ['attendance_date', 'timestamp', 'no-op'],\n",
							"                                            ['all_day', 'string', 'no-op'],\n",
							"                                            ['Period', 'short', 'no-op'],\n",
							"                                            ['section_id', 'string', 'no-op'],\n",
							"                                            ['AttendanceCode', 'string', 'no-op'],\n",
							"                                            ['PresenceFlag', 'boolean', 'no-op'],\n",
							"                                            ['attendance_status', 'string', 'no-op'],\n",
							"                                            ['attendance_type', 'string', 'no-op'],\n",
							"                                            ['attendance_sequence', 'short', 'no-op']]\n",
							"\n",
							"        self.schemas['studentsectionmark'] = [['id', 'string', 'no-op'],\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\n",
							"                                            ['section_id', 'string', 'no-op'],\n",
							"                                            ['school_year', 'string', 'no-op'],\n",
							"                                            ['term_id', 'string', 'no-op'],\n",
							"                                            ['numeric_grade_earned', 'short', 'no-op'],\n",
							"                                            ['alpha_grade_earned', 'string', 'no-op'],\n",
							"                                            ['is_final_grade', 'string', 'no-op'],\n",
							"                                            ['credits_attempted', 'short', 'no-op'],\n",
							"                                            ['credits_earned', 'short', 'no-op'],\n",
							"                                            ['grad_credit_type', 'string', 'no-op']]\n",
							"\n",
							"    def process_latest_from_stage1(self):\n",
							"        latest = oea.get_latest_folder(self.stage1np)\n",
							"        if latest == None:\n",
							"            logger.info(\"No folders found in: \" + self.stage1np)\n",
							"        else:\n",
							"            self._process_entity_from_stage1(latest, 'studentattendance', 'csv', 'overwrite', 'true')\n",
							"            self._process_entity_from_stage1(latest, 'studentsectionmark', 'csv', 'overwrite', 'true')\n",
							"\n",
							"class M365(BaseOEAModule):\n",
							"    \"\"\"\n",
							"    Provides data processing methods for MS Insights data v0.2 format.\n",
							"    \"\"\"\n",
							"\n",
							"    def __init__(self, oea, source_folder='m365', pseudonymize = False):\n",
							"        BaseOEAModule.__init__(self, oea, source_folder, pseudonymize)\n",
							"\n",
							"        self.schemas['Activity'] = [['SignalType', 'string', 'no-op'],\n",
							"                                            ['StartTime', 'timestamp', 'no-op'],\n",
							"                                            ['UserAgent', 'string', 'no-op'],\n",
							"                                            ['SignalId', 'string', 'no-op'],\n",
							"                                            ['SISClassId', 'string', 'no-op'],\n",
							"                                            ['OfficeClassId', 'string', 'no-op'],\n",
							"                                            ['ChannelId', 'string', 'no-op'],\n",
							"                                            ['AppName', 'string', 'no-op'],\n",
							"                                            ['ActorId', 'string', 'hash-no-lookup'],\n",
							"                                            ['ActorRole', 'string', 'no-op'],\n",
							"                                            ['SchemaVersion', 'string', 'no-op'],\n",
							"                                            ['AssignmentId', 'string', 'no-op'],\n",
							"                                            ['SubmissionId', 'string', 'no-op'],\n",
							"                                            ['Action', 'string', 'no-op'],\n",
							"                                            ['AssginmentDueDate', 'string', 'no-op'],\n",
							"                                            ['ClassCreationDate', 'string', 'no-op'],\n",
							"                                            ['Grade', 'string', 'no-op'],\n",
							"                                            ['SourceFileExtension', 'string', 'no-op'],\n",
							"                                            ['MeetingDuration', 'string', 'no-op']]\n",
							"        self.schemas['Calendar'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['Name', 'string', 'no-op'],\n",
							"                                            ['Description', 'string', 'no-op'],\n",
							"                                            ['SchoolYear', 'integer', 'no-op'],\n",
							"                                            ['IsCurrent', 'boolean', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'no-op'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['OrgId', 'string', 'no-op']]\n",
							"        self.schemas['Course'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['Name', 'string', 'no-op'],\n",
							"                                            ['Code', 'string', 'no-op'],\n",
							"                                            ['Description', 'string', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'no-op'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['CalendarId', 'string', 'no-op']]\n",
							"        self.schemas['Org'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['Name', 'string', 'no-op'],\n",
							"                                            ['Identifier', 'string', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'no-op'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['ParentOrgId', 'string', 'no-op'],\n",
							"                                            ['RefOrgTypeId', 'string', 'no-op'],\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\n",
							"        self.schemas['Person'] = [['Id', 'string', 'hash'],\n",
							"                                            ['FirstName', 'string', 'mask'],\n",
							"                                            ['MiddleName', 'string', 'mask'],\n",
							"                                            ['LastName', 'string', 'mask'],\n",
							"                                            ['GenerationCode', 'string', 'no-op'],\n",
							"                                            ['Prefix', 'string', 'no-op'],\n",
							"                                            ['EnabledUser', 'string', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'hash'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\n",
							"        self.schemas['PersonIdentifier'] = [['Id', 'string', 'hash'],\n",
							"                                            ['Identifier', 'string', 'hash'],\n",
							"                                            ['Description', 'string', 'no-op'],\n",
							"                                            ['RefIdentifierTypeId', 'string', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'hash'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['PersonId', 'string', 'hash'],\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\n",
							"        self.schemas['RefDefinition'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['RefType', 'string', 'no-op'],\n",
							"                                            ['Namespace', 'string', 'no-op'],\n",
							"                                            ['Code', 'string', 'no-op'],\n",
							"                                            ['SortOrder', 'integer', 'no-op'],\n",
							"                                            ['Description', 'string', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op']]\n",
							"        self.schemas['Section'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['Name', 'string', 'no-op'],\n",
							"                                            ['Code', 'string', 'no-op'],\n",
							"                                            ['Location', 'string', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'no-op'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['CourseId', 'string', 'no-op'],\n",
							"                                            ['RefSectionTypeId', 'string', 'no-op'],\n",
							"                                            ['SessionId', 'string', 'no-op'],\n",
							"                                            ['OrgId', 'string', 'no-op']]\n",
							"        self.schemas['Session'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['Name', 'string', 'no-op'],\n",
							"                                            ['BeginDate', 'timestamp', 'no-op'],\n",
							"                                            ['EndDate', 'timestamp', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'no-op'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['CalendarId', 'string', 'no-op'],\n",
							"                                            ['ParentSessionId', 'string', 'no-op'],\n",
							"                                            ['RefSessionTypeId', 'string', 'no-op']]\n",
							"        self.schemas['StaffOrgAffiliation'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['IsPrimary', 'boolean', 'no-op'],\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'no-op'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['OrgId', 'string', 'no-op'],\n",
							"                                            ['PersonId', 'string', 'hash'],\n",
							"                                            ['RefStaffOrgRoleId', 'string', 'no-op']]\n",
							"        self.schemas['StaffSectionMembership'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['IsPrimaryStaffForSection', 'boolean', 'no-op'],\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'no-op'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['PersonId', 'string', 'hash'],\n",
							"                                            ['RefStaffSectionRoleId', 'string', 'no-op'],\n",
							"                                            ['SectionId', 'string', 'no-op']]\n",
							"        self.schemas['StudentOrgAffiliation'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['IsPrimary', 'boolean', 'no-op'],\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'no-op'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['OrgId', 'string', 'no-op'],\n",
							"                                            ['PersonId', 'string', 'hash'],\n",
							"                                            ['RefGradeLevelId', 'string', 'no-op'],\n",
							"                                            ['RefStudentOrgRoleId', 'string', 'no-op'],\n",
							"                                            ['RefEnrollmentStatusId', 'string', 'no-op']]\n",
							"        self.schemas['StudentSectionMembership'] = [['Id', 'string', 'no-op'],\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\n",
							"                                            ['ExternalId', 'string', 'no-op'],\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\n",
							"                                            ['PersonId', 'string', 'hash'],\n",
							"                                            ['RefGradeLevelWhenCourseTakenId', 'string', 'no-op'],\n",
							"                                            ['RefStudentSectionRoleId', 'string', 'no-op'],\n",
							"                                            ['SectionId', 'string', 'no-op']]\n",
							"    \n",
							"    def process_activity_data_from_stage1(self):\n",
							"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \n",
							"            https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
							"        \"\"\"\n",
							"        latest = oea.get_latest_folder(self.stage1np)\n",
							"        if latest == None:\n",
							"            logger.info(\"No activity data available to process in folder: \" + self.stage1np)\n",
							"            return\n",
							"\n",
							"        logger.info(\"Processing activity data from: \" + self.stage1np + '/' + latest)\n",
							"\n",
							"        spark_schema = self.oea.to_spark_schema(self.schemas['Activity'])\n",
							"        df = spark.read.csv(self.stage1np + '/' + latest + '/Activity/*.csv', header='false', schema=spark_schema) \n",
							"        sqlContext.registerDataFrameAsTable(df, 'Activity')\n",
							"        sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(self.oea.stage2np + '/m365/PersonIdentifier'), 'PersonIdentifier')\n",
							"        sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(self.oea.stage2np + '/m365/RefDefinition'), 'RefDefinition')\n",
							"\n",
							"        df = spark.sql( \n",
							"            \"select act.SignalType, act.StartTime, act.UserAgent, act.SignalId, act.SISClassId, act.OfficeClassId, act.ChannelId, \\\n",
							"            act.AppName, act.ActorId, act.ActorRole, act.SchemaVersion, act.AssignmentId, act.SubmissionId, act.Action, act.AssginmentDueDate, \\\n",
							"            act.ClassCreationDate, act.Grade, act.SourceFileExtension, act.MeetingDuration, pi.PersonId \\\n",
							"            from PersonIdentifier pi, RefDefinition rd, Activity act \\\n",
							"            where \\\n",
							"                pi.RefIdentifierTypeId = rd.Id \\\n",
							"                and rd.RefType = 'RefIdentifierType' \\\n",
							"                and rd.Code = 'ActiveDirectoryId' \\\n",
							"                and pi.Identifier = act.ActorId\")\n",
							"\n",
							"        df = df.dropDuplicates(['SignalId'])\n",
							"        df = df.withColumn('year', F.year(F.col('StartTime'))).withColumn('month', F.month(F.col('StartTime')))\n",
							"        df = self.oea.fix_column_names(df)\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/TechActivity')\n",
							"\n",
							"    def reset_activity_processing(self):\n",
							"        \"\"\" Resets all TechActivity processing. This is intended for use during initial testing - use with caution. \"\"\"\n",
							"        self.oea.rm_if_exists(self.stage2p + '/TechActivity')\n",
							"        self.oea.rm_if_exists(self.stage2np + '/TechActivity')\n",
							"        logger.info(f\"Deleted TechActivity from stage2\")  \n",
							"\n",
							"    def _process_roster_entity(self, path):\n",
							"        try:\n",
							"            base_path, filename = self.oea.pop_from_path(path)\n",
							"            entity = filename[:-4]\n",
							"            logger.debug(f\"Processing roster entity: path={path}, entity={entity}\")\n",
							"            spark_schema = self.oea.to_spark_schema(self.schemas[entity])\n",
							"            df = spark.read.csv(path, header='false', schema=spark_schema)\n",
							"            df = self.oea.fix_column_names(df)\n",
							"            df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/' + entity)\n",
							"\n",
							"        except (AnalysisException) as error:\n",
							"            logger.exception(str(error))\n",
							"\n",
							"    def process_latest_roster_from_stage1(self):\n",
							"        latest = oea.get_latest_folder(self.stage1np)\n",
							"        if latest == None:\n",
							"            logger.info(\"There is no roster data in stage1 to process.\")\n",
							"        else:\n",
							"            folder_to_process = self.stage1np + '/' + latest\n",
							"            items = mssparkutils.fs.ls(folder_to_process)\n",
							"            logger.info(\"Processing latest roster data from: \" + folder_to_process)\n",
							"            for item in items:\n",
							"                if item.name != 'Activity':     \n",
							"                    self._process_entity_from_stage1(latest, item.name, 'csv', 'overwrite', 'false')\n",
							"\n",
							"    def reset_roster_processing(self):\n",
							"        \"\"\" Resets all stage1 to stage2 processing of roster data. \"\"\"\n",
							"        # cleanup stage2np\n",
							"        if self.oea.path_exists(self.stage2np):\n",
							"            # Delete roster tables (everything other than TechActivity)\n",
							"            items = mssparkutils.fs.ls(self.stage2np)\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\n",
							"            for item in items:\n",
							"                if item.name != 'TechActivity':\n",
							"                    mssparkutils.fs.rm(item.path, True)\n",
							"        # cleanup stage2p\n",
							"        if self.oea.path_exists(self.stage2p):\n",
							"            # Delete roster tables (everything other than TechActivity)\n",
							"            items = mssparkutils.fs.ls(self.stage2p)\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\n",
							"            for item in items:\n",
							"                if item.name != 'TechActivity':\n",
							"                    mssparkutils.fs.rm(item.path, True)    \n",
							"  \n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/generate_data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "946b3f7f-754d-4b83-b695-a40148269c0f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run OEA_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"%run M365DataGenerator_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"oea = OEA()\r\n",
							"oea.rm_if_exists(oea.stage1np + '/generated/m365_test')\r\n",
							"oea.rm_if_exists(oea.stage1np + '/generated/contoso_sis_test')\r\n",
							"data_generator = M365DataGenerator(activity_max_per_person=15)\r\n",
							"data_lake_writer = DataLakeWriter(oea.stage1np)\r\n",
							"data_generator.generate_data(num_of_schools=2, writer=data_lake_writer)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"pdf = oea.load_sample_from_csv_file('generated/m365/Activity0p2.csv', header=False)\r\n",
							"print(pdf)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"df = spark.sql(\"SELECT * FROM tutorial_01.person\")\n",
							"# df.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/setup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# setup the OEA framework"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"instrumentation_key = 'InstrumentationKey=5a4a6026-f008-4e9e-a48a-96a412d250d6'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"spark = SparkSession.builder.appName(\"OEA_M365_processing\").getOrCreate()\r\n",
							"\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.DEBUG) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string=instrumentation_key))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming activity data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process M365 activity data\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"csvDF = csvDF.dropDuplicates(['SignalId'])\r\n",
							"csvDF = csvDF.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"query.start(dest_path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming roster data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"s1_path = stage1 + '/M365/inbound/roster'\r\n",
							"s2_path = stage2 + '/M365'\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(entity, schema):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source={s1_path}, destination={s2_path}\")\r\n",
							"        csvDF = spark.readStream.csv(s1_path + '/*/' + entity + '/*.csv', header='false', schema=schema)\r\n",
							"\r\n",
							"        query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", s1_path + '/' + entity + '/_checkpoints')\r\n",
							"        query.start(s2_path + '/' + entity)\r\n",
							"\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        print(error)\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"s1_to_s2('AadUser', schema)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Batch process M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": true,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(source_path, schema, destination_path):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(source_path, header='false', schema=schema)\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(destination_path)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"def process_M365_roster(file_path):\r\n",
							"    #AadUser\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUser', schema, stage2 + '/M365/AadUser')\r\n",
							"    #AadUserPersonMapping\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUserPersonMapping', schema, stage2 + '/M365/AadUserPersonMapping')\r\n",
							"    #Course\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Course', schema, stage2 + '/M365/Course')\r\n",
							"    #CourseGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseGradeLevel', schema, stage2 + '/M365/CourseGradeLevel')\r\n",
							"    #CourseSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseSubject', schema, stage2 + '/M365/CourseSubject')\r\n",
							"    #Enrollment\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"    s1_to_s2(file_path + '/Enrollment', schema, stage2 + '/M365/Enrollment')\r\n",
							"    #Organization\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Organization', schema, stage2 + '/M365/Organization')\r\n",
							"    #Person\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Person', schema, stage2 + '/M365/Person')\r\n",
							"    #PersonDemographic\r\n",
							"    schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographic', schema, stage2 + '/M365/PersonDemographic')\r\n",
							"    #PersonDemographicEthnicity\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicEthnicity', schema, stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"    #PersonDemographicPersonFlag\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicPersonFlag', schema, stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"    #PersonDemographicRace\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicRace', schema, stage2 + '/M365/PersonDemographicRace')\r\n",
							"    #PersonEmailAddress\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonEmailAddress', schema, stage2 + '/M365/PersonEmailAddress')\r\n",
							"    #PersonIdentifier\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonIdentifier', schema, stage2 + '/M365/PersonIdentifier')\r\n",
							"    #PersonOrganizationRole\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonOrganizationRole', schema, stage2 + '/M365/PersonOrganizationRole')\r\n",
							"    #PersonPhoneNumber\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonPhoneNumber', schema, stage2 + '/M365/PersonPhoneNumber')\r\n",
							"    #PersonRelationship\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonRelationship', schema, stage2 + '/M365/PersonRelationship')\r\n",
							"    #RefDefinition\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"    s1_to_s2(file_path + '/RefDefinition', schema, stage2 + '/M365/RefDefinition')\r\n",
							"    #Section\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Section', schema, stage2 + '/M365/Section')\r\n",
							"    #SectionGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionGradeLevel', schema, stage2 + '/M365/SectionGradeLevel')\r\n",
							"    #SectionSession\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSession', schema, stage2 + '/M365/SectionSession')\r\n",
							"    #SectionSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSubject', schema, stage2 + '/M365/SectionSubject')\r\n",
							"    #Session\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Session', schema, stage2 + '/M365/Session')\r\n",
							"    #SourceSystem\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SourceSystem', schema, stage2 + '/M365/SourceSystem')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"inbound_path = stage1 + '/M365/inbound/roster'\r\n",
							"processed_path = stage1 + '/M365/processed/roster'\r\n",
							"\r\n",
							"logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"items = mssparkutils.fs.ls(inbound_path)\r\n",
							"for item in items:\r\n",
							"    if item.isDir:\r\n",
							"        process_M365_roster(item.path)\r\n",
							"        mssparkutils.fs.mv(item.path, processed_path, True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything to run it again"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stream_tutorial')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Structured streaming tutorial"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"file_path = '/M365/inbound/roster/2021-06-15T04-04-12'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 25
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tutorial_01')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": true
							}
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#step 1 - initial ingestion (specify schema)\r\n",
							"\r\n",
							"df = spark.read.csv(stage1 + '/tutorial_01/roster/2021-06-02/Person', header='false')\r\n",
							"sqlContext.registerDataFrameAsTable(df, 'Person')\r\n",
							"sql_str = \"select _c0 Id, to_timestamp(_c1) FirstSeenDateTime, to_timestamp(_c2) LastSeenDateTime, _c3 GivenName, _c4 MiddleName, _c5 PreferredGivenName, _c6 PreferredMiddleName, _c7 PreferredSurname, _c8 Surname from Person\"\r\n",
							"df = spark.sql(sql_str)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/tutorial_01/Person')\r\n",
							"\r\n",
							"# now show the current state of the Person table\r\n",
							"df = spark.read.format(\"delta\").load(stage2 + '/tutorial_01/Person')\r\n",
							"df.show(100)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# step 2 - update table with new snapshot data\r\n",
							"\r\n",
							"# Ingest next snapshot of Person records, with the following changes:\r\n",
							"# Added 2 new students: Andrew Hudson, Amber Buchanan\r\n",
							"# Deleted 1 student: Erika Riley\r\n",
							"# Modified 1 student: Joseph Kelley was modified to be Joey Kelley\r\n",
							"df = spark.read.csv(stage1 + '/tutorial_01/roster/2021-06-10/Person', header='false')\r\n",
							"sqlContext.registerDataFrameAsTable(df, 'Person')\r\n",
							"sql_str = \"select _c0 Id, to_timestamp(_c1) FirstSeenDateTime, to_timestamp(_c2) LastSeenDateTime, _c3 GivenName, _c4 MiddleName, _c5 PreferredGivenName, _c6 PreferredMiddleName, _c7 PreferredSurname, _c8 Surname from Person\"\r\n",
							"df = spark.sql(sql_str)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/tutorial_01/Person')\r\n",
							"# now show the current state of the Person table\r\n",
							"df = spark.read.format(\"delta\").load(stage2 + '/tutorial_01/Person')\r\n",
							"df.show(30)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# step 3 - update table with delta data (upsert)\r\n",
							"# Ingest delta data consisting of a new student \"Sheldon Bergeron\", and an update to \"Shelby Berger\" (lastnamt changed to \"Bergeron\")\r\n",
							"# https://docs.delta.io/latest/api/python/index.html?highlight=whenmatchedupdateall#delta.tables.DeltaTable.merge\r\n",
							"from delta.tables import *\r\n",
							"\r\n",
							"updatesDF = spark.read.csv(stage1 + '/tutorial_01/roster/2021-06-11/Person', header='false')\r\n",
							"sqlContext.registerDataFrameAsTable(updatesDF, 'Person')\r\n",
							"sql_str = \"select _c0 Id, to_timestamp(_c1) FirstSeenDateTime, to_timestamp(_c2) LastSeenDateTime, _c3 GivenName, _c4 MiddleName, _c5 PreferredGivenName, _c6 PreferredMiddleName, _c7 PreferredSurname, _c8 Surname from Person\"\r\n",
							"updatesDF = spark.sql(sql_str)\r\n",
							"#display(updatesDF)\r\n",
							"deltaTable = DeltaTable.forPath(spark, stage2 + '/tutorial_01/Person')\r\n",
							"\r\n",
							"deltaTable.alias(\"Person\").merge(\r\n",
							"    updatesDF.alias(\"updates\"),\r\n",
							"    \"Person.Id = updates.Id\") \\\r\n",
							"  .whenMatchedUpdate(set = \r\n",
							"      {\r\n",
							"      \"FirstSeenDateTime\": \"updates.FirstSeenDateTime\",\r\n",
							"      \"LastSeenDateTime\": \"updates.LastSeenDateTime\",\r\n",
							"      \"GivenName\": \"updates.GivenName\",\r\n",
							"      \"MiddleName\": \"updates.MiddleName\",\r\n",
							"      \"PreferredGivenName\": \"updates.PreferredGivenName\",\r\n",
							"      \"PreferredMiddleName\": \"updates.PreferredMiddleName\",\r\n",
							"      \"PreferredSurname\": \"updates.PreferredSurname\",\r\n",
							"      \"Surname\": \"updates.Surname\"\r\n",
							"    } \r\n",
							"  ) \\\r\n",
							"  .whenNotMatchedInsert(values =\r\n",
							"    {\r\n",
							"      \"Id\": \"updates.Id\",\r\n",
							"      \"FirstSeenDateTime\": \"updates.FirstSeenDateTime\",\r\n",
							"      \"LastSeenDateTime\": \"updates.LastSeenDateTime\",\r\n",
							"      \"GivenName\": \"updates.GivenName\",\r\n",
							"      \"MiddleName\": \"updates.MiddleName\",\r\n",
							"      \"PreferredGivenName\": \"updates.PreferredGivenName\",\r\n",
							"      \"PreferredMiddleName\": \"updates.PreferredMiddleName\",\r\n",
							"      \"PreferredSurname\": \"updates.PreferredSurname\",\r\n",
							"      \"Surname\": \"updates.Surname\"\r\n",
							"    } \r\n",
							"  ) \\\r\n",
							"  .execute()\r\n",
							"# now show the current state of the Person table\r\n",
							"df = spark.read.format(\"delta\").load(stage2 + '/tutorial_01/Person')\r\n",
							"df.show(30)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# now show the current state of the Person table\r\n",
							"df = spark.read.format(\"delta\").load(stage2 + '/tutorial_01/Person')\r\n",
							"df.show(50)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# step 4: describe history and time travel\r\n",
							"\r\n",
							"from delta.tables import DeltaTable\r\n",
							"deltaTable = DeltaTable.forPath(spark, stage2 + '/tutorial_01/Person')\r\n",
							"print(\"######## Describe history for the table ######\")\r\n",
							"deltaTable.history().show()\r\n",
							"\r\n",
							"print(\"######## Show an earlier version of the table ######\")\r\n",
							"df1 = spark.read.format('delta').option('versionAsOf', 1).load(stage2 + '/tutorial_01/Person')\r\n",
							"df1.show(30)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Create spark db to use with Power BI\r\n",
							"#spark.sql(\"CREATE TABLE events USING DELTA LOCATION '\" + stage2 + \"/tutorial_01/Person'\")\r\n",
							"\r\n",
							"spark.sql('CREATE DATABASE IF NOT EXISTS tutorial_02')\r\n",
							"spark.sql(\"create table if not exists tutorial_02.Person using DELTA location '\" + stage2 + \"/tutorial_01/Person'\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": true
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.sql(\"SELECT * FROM tutorial_01.person\")\r\n",
							"df.show(10)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Activity data\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# insert new activity data\r\n",
							"# todo: port the example below, as described here: https://docs.delta.io/0.8.0/delta-update.html#-merge-in-dedup\r\n",
							"deltaTable.alias(\"logs\").merge(\r\n",
							"    newDedupedLogs.alias(\"newDedupedLogs\"),\r\n",
							"    \"logs.uniqueId = newDedupedLogs.uniqueId\") \\\r\n",
							"  .whenNotMatchedInsertAll() \\\r\n",
							"  .execute()\r\n",
							"\r\n",
							"from delta.tables import DeltaTable\r\n",
							"deltaTable = DeltaTable.forPath(spark, dest_path)\r\n",
							"deltaTable.alias(\"TechActivity\").merge(updatesDF.alias(\"updates\"), \"TechActivity.SignalId = updates.SignalId\") \\\r\n",
							"    .whenNotMatchedInsertAll() \\\r\n",
							"    .execute()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset the tutorial"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete all data from stage2 - to reset the exercise and start over\r\n",
							"from notebookutils import mssparkutils\r\n",
							"mssparkutils.fs.rm(stage2 + '/tutorial_01', True)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": true
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"# to get a list of all the commands available\r\n",
							"mssparkutils.fs.help()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		}
	]
}