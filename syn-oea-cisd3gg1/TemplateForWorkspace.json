{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "syn-oea-cisd3gg1"
		},
		"EdFi_Ods_Production_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'EdFi_Ods_Production'"
		},
		"LS_ADLS_OEA_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'LS_ADLS_OEA'"
		},
		"syn-oea-cisd3gg1-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'syn-oea-cisd3gg1-WorkspaceDefaultSqlServer'"
		},
		"LS_ADLS_OEA_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeacisd3gg1.dfs.core.windows.net"
		},
		"LS_HTTP_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{linkedService().baseURL}"
		},
		"LS_HTTP_Corgis_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://corgis-edu.github.io"
		},
		"LS_HTTP_Github_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com"
		},
		"syn-oea-cisd3gg1-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stoeacisd3gg1.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Clever_data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Clever_land_in_s1",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Clever_land_in_s1",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Clever_s1_into_s2",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Clever_land_in_s1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Clever_s1_into_s2",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Clever_s2_to_s3",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Clever_s1_into_s2",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Clever_s2_to_s3",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "0) Master Orchestration"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-15T15:28:41Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Clever_land_in_s1')]",
				"[concat(variables('workspaceId'), '/pipelines/Clever_s1_into_s2')]",
				"[concat(variables('workspaceId'), '/pipelines/Clever_s2_to_s3')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Clever_land_in_s1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Lands data from Clever into the inbound folder in stage1, under a folder with the execution datetime.",
				"activities": [
					{
						"name": "copy Clever data",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "set exec_date_time",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFolderPath": "stage1/clever",
									"wildcardFileName": "*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".csv"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "data_lake_csv",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "test-env",
									"directory": "stage1/Clever"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "data_lake_csv",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1",
									"directory": {
										"value": "Clever/inbound/@{variables('exec_date_time')}",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "set exec_date_time",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "exec_date_time",
							"value": {
								"value": "@{formatDateTime(convertTimeZone(utcnow(), 'UTC', 'Eastern Standard Time'), 'yyyy-MM-ddTHHmm_ss')}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"exec_date_time": {
						"type": "String"
					}
				},
				"folder": {
					"name": "1) Land in s1"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-15T13:43:17Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/data_lake_csv')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Clever_s1_into_s2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "list_dir_content",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "data_lake_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1",
									"directory": "Clever/inbound"
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "For each file in inbound folder",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "list_dir_content",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('list_dir_content').output.childitems",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Clever_s1_to_s2",
									"type": "SynapseNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebook": {
											"referenceName": "Clever_s1_to_s2",
											"type": "NotebookReference"
										},
										"parameters": {
											"folder_to_process": {
												"value": {
													"value": "inbound/@{item().name}",
													"type": "Expression"
												},
												"type": "string"
											}
										},
										"snapshot": true
									}
								},
								{
									"name": "Move_Files",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "Clever_s1_to_s2",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "Move_files",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"source_filesystem": "stage1",
											"source_path": {
												"value": "Clever/inbound/@{item().name}",
												"type": "Expression"
											},
											"destination_filesystem": "stage1",
											"destination_path": {
												"value": "Clever/processed/@{item().name}",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "2) Process from s1 into s2"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-16T20:23:32Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/data_lake_binary')]",
				"[concat(variables('workspaceId'), '/notebooks/Clever_s1_to_s2')]",
				"[concat(variables('workspaceId'), '/pipelines/Move_files')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Clever_s2_to_s3')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "placholder",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Clever_land_in_s1",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "3) Process from s2 into s3"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-15T15:27:48Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Clever_land_in_s1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_roster_s1_into_s2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "folder_metadata",
						"description": "Gets the list of files in the inbound folder in stage1.",
						"type": "GetMetadata",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "data_lake_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1",
									"directory": "M365/inbound/roster"
								}
							},
							"fieldList": [
								"childItems"
							],
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "BinaryReadSettings"
							}
						}
					},
					{
						"name": "for each file in inbound folder",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "folders",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('folders').output.Value",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "M365_s1_to_s2",
									"type": "SynapseNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebook": {
											"referenceName": "M365_roster_v0p3_s1_to_s2",
											"type": "NotebookReference"
										},
										"parameters": {
											"storage_account": {
												"value": "stoeacisd3gg1",
												"type": "string"
											},
											"file_path": {
												"value": {
													"value": "/M365/inbound/roster/@{item().name}",
													"type": "Expression"
												},
												"type": "string"
											}
										},
										"snapshot": true
									}
								},
								{
									"name": "Move_Files",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "M365_s1_to_s2",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "Move_files",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"source_filesystem": "stage1",
											"source_path": {
												"value": "M365/inbound/roster/@{item().name}",
												"type": "Expression"
											},
											"destination_filesystem": "stage1",
											"destination_path": {
												"value": "M365/processed/roster/@{item().name}",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "folders",
						"type": "Filter",
						"dependsOn": [
							{
								"activity": "folder_metadata",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('folder_metadata').output.childitems",
								"type": "Expression"
							},
							"condition": {
								"value": "@equals(item().type, 'Folder')",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "2) Process from s1 into s2"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-16T20:23:32Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/data_lake_binary')]",
				"[concat(variables('workspaceId'), '/notebooks/M365_roster_v0p3_s1_to_s2')]",
				"[concat(variables('workspaceId'), '/pipelines/Move_files')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Move_files')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Moves files from specified source to destination, adding a folder with the current datetime to the destination path.",
				"activities": [
					{
						"name": "move_files",
						"description": "Moves files from the specified source container and path to the destination container and path.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"deleteFilesAfterCompletion": true
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false,
							"preserve": [
								"ACL",
								"Owner",
								"Group",
								"Attributes"
							],
							"skipErrorFile": {
								"fileMissing": false,
								"fileForbidden": false,
								"invalidFileName": false
							}
						},
						"inputs": [
							{
								"referenceName": "data_lake_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.source_filesystem",
										"type": "Expression"
									},
									"directory": "@pipeline().parameters.source_path"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "data_lake_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.destination_filesystem",
										"type": "Expression"
									},
									"directory": {
										"value": "@{pipeline().parameters.destination_path}",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "Delete1",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "move_files",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "data_lake_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "@pipeline().parameters.source_filesystem",
									"directory": "@pipeline().parameters.source_path"
								}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"source_filesystem": {
						"type": "string"
					},
					"source_path": {
						"type": "string"
					},
					"destination_filesystem": {
						"type": "string"
					},
					"destination_path": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Utils"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-15T00:29:38Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/data_lake_binary')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Move_files_add_timestamp')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Moves files from specified source to destination, adding a folder with the current datetime to the destination path.",
				"activities": [
					{
						"name": "move_files",
						"description": "Moves files from the specified source container and path to the destination container and path.",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Set variable1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFolderPath": {
										"value": "@pipeline().parameters.source_path",
										"type": "Expression"
									},
									"deleteFilesAfterCompletion": true
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false,
							"preserve": [
								"ACL",
								"Owner",
								"Group",
								"Attributes"
							],
							"skipErrorFile": {
								"fileMissing": false,
								"fileForbidden": false,
								"invalidFileName": false
							}
						},
						"inputs": [
							{
								"referenceName": "data_lake_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.source_filesystem",
										"type": "Expression"
									},
									"directory": "not_applicable"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "data_lake_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": {
										"value": "@pipeline().parameters.destination_filesystem",
										"type": "Expression"
									},
									"directory": {
										"value": "@{pipeline().parameters.destination_path}/@{variables('exec_date_time')}",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "Set variable1",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "exec_date_time",
							"value": {
								"value": "@{formatDateTime(convertTimeZone(utcnow(), 'UTC', 'Eastern Standard Time'), 'yyyy-MM-ddTHHmm_ss')}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"source_filesystem": {
						"type": "string"
					},
					"source_path": {
						"type": "string"
					},
					"destination_filesystem": {
						"type": "string"
					},
					"destination_path": {
						"type": "string"
					}
				},
				"variables": {
					"exec_date_time": {
						"type": "String"
					}
				},
				"folder": {
					"name": "Utils"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-14T23:12:08Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/data_lake_binary')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_ex1_data_ingestion')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Basic example of pulling data from an http endpoint and landing it in the data lake.",
				"activities": [
					{
						"name": "ingest studentattendance",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "DS_HTTP_binary",
								"type": "DatasetReference",
								"parameters": {
									"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentattendance.csv"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ADLS_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"filename": "contoso_sis/example1/studentattendance.csv"
								}
							}
						]
					},
					{
						"name": "ingest studentdemographics",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "DS_HTTP_binary",
								"type": "DatasetReference",
								"parameters": {
									"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentdemographics.csv"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ADLS_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"filename": "contoso_sis/example1/studentdemographics.csv"
								}
							}
						]
					},
					{
						"name": "ingest studentsectionmark",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "DS_HTTP_binary",
								"type": "DatasetReference",
								"parameters": {
									"URL": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentsectionmark.csv"
								}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ADLS_binary",
								"type": "DatasetReference",
								"parameters": {
									"filesystem": "stage1np",
									"filename": "contoso_sis/example1/studentsectionmark.csv"
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_HTTP_binary')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_ex2_data_ingestion')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This is an example of a pipeline that retrieves data from multiple HTTP endpoints, using parameters to list the URL's to retrieve data from.",
				"activities": [
					{
						"name": "get data for each endpoint",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Set currentDateTime",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.endpoints",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "ingest data from url",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "BinarySource",
											"storeSettings": {
												"type": "HttpReadSettings",
												"requestMethod": "GET"
											},
											"formatSettings": {
												"type": "BinaryReadSettings"
											}
										},
										"sink": {
											"type": "BinarySink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											}
										},
										"enableStaging": false,
										"logSettings": {
											"enableCopyActivityLog": true,
											"copyActivityLogSettings": {
												"logLevel": "Info",
												"enableReliableLogging": false
											},
											"logLocationSettings": {
												"linkedServiceName": {
													"referenceName": "LS_ADLS_OEA",
													"type": "LinkedServiceReference"
												},
												"path": "oea-framework/pipeline_logs"
											}
										}
									},
									"inputs": [
										{
											"referenceName": "DS_HTTP_binary",
											"type": "DatasetReference",
											"parameters": {
												"URL": {
													"value": "@item().source",
													"type": "Expression"
												}
											}
										}
									],
									"outputs": [
										{
											"referenceName": "DS_ADLS_binary",
											"type": "DatasetReference",
											"parameters": {
												"filesystem": {
													"value": "@pipeline().parameters.sinkFilesystem",
													"type": "Expression"
												},
												"filename": {
													"value": "@{item().destinationDirectory}/@{variables('currentDateTime')}/@{item().destinationFilename}",
													"type": "Expression"
												}
											}
										}
									]
								}
							]
						}
					},
					{
						"name": "Set currentDateTime",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "currentDateTime",
							"value": {
								"value": "@{formatDateTime(convertTimeZone(utcnow(), 'UTC', 'Eastern Standard Time'), 'yyyy-MM-ddTHHmm_ss')}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"endpoints": {
						"type": "array",
						"defaultValue": [
							{
								"source": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentattendance.csv",
								"destinationDirectory": "contoso_sis",
								"destinationFilename": "studentattendance/studentattendance.csv"
							},
							{
								"source": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentdemographics.csv",
								"destinationDirectory": "contoso_sis",
								"destinationFilename": "studentdemographics/studentdemographics.csv"
							},
							{
								"source": "https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentsectionmark.csv",
								"destinationDirectory": "contoso_sis",
								"destinationFilename": "studentsectionmark/studentsectionmark.csv"
							}
						]
					},
					"sinkFilesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					}
				},
				"variables": {
					"currentDateTime": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_HTTP_binary')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ADLS_binary')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Push_to_SDS')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "placholder",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Clever_land_in_s1",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "4) Outbound"
				},
				"annotations": [],
				"lastPublishTime": "2021-07-15T15:28:15Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Clever_land_in_s1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ADLS_binary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used for landing data in the data lake.\nDefaults to landing data in stage1np.\nNote that you can specify a full path in the filename param (eg, to land a file in a specific folder filename param can be 'contoso_sis/students/students.csv').\n",
				"linkedServiceName": {
					"referenceName": "LS_ADLS_OEA",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string",
						"defaultValue": "stage1np"
					},
					"filename": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().filename",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_ADLS_OEA')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_Corgis_binary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_HTTP_Corgis",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"relativeURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation",
						"relativeUrl": {
							"value": "@dataset().relativeURL",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_HTTP_Corgis')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_Github_binary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_HTTP_Github",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"relativeURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation",
						"relativeUrl": {
							"value": "@dataset().relativeURL",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_HTTP_Github')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_HTTP_binary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Retrieves data from an http endpoint.\nThe data can be in any format - the binary dataset allows us to pull any payload without affecting it.",
				"linkedServiceName": {
					"referenceName": "LS_HTTP",
					"type": "LinkedServiceReference",
					"parameters": {
						"baseURL": {
							"value": "@dataset().URL",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"URL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_HTTP')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/data_lake_binary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "syn-oea-cisd3gg1-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/syn-oea-cisd3gg1-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/data_lake_csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "syn-oea-cisd3gg1-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/syn-oea-cisd3gg1-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/data_lake_test1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "syn-oea-cisd3gg1-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"filesystem": {
						"type": "string"
					},
					"directory": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": {
							"value": "@dataset().directory",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().filesystem",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/syn-oea-cisd3gg1-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/EdFi_Ods_Production')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('EdFi_Ods_Production_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_ADLS_OEA')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Connection to the OEA data lake",
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('LS_ADLS_OEA_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('LS_ADLS_OEA_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_HTTP')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Connection to an HTTP endpoint.\nThe baseURL parameter must be passed in from the dataset that utilizes this linked service.",
				"parameters": {
					"baseURL": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('LS_HTTP_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_HTTP_Corgis')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('LS_HTTP_Corgis_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_HTTP_Github')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Connection for retrieving datasets from github.",
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('LS_HTTP_Github_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-cisd3gg1-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('syn-oea-cisd3gg1-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/syn-oea-cisd3gg1-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('syn-oea-cisd3gg1-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Daily_at_1am')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Clever_data",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2021-07-16T20:32:00",
						"timeZone": "Eastern Standard Time",
						"schedule": {
							"minutes": [
								0
							],
							"hours": [
								1
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Clever_data')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 11')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage1/M365/activity/2021-06-10/ApplicationUsage.Part001.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION='2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 12')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage1np/corgis2/graduates/graduates.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION='2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 13')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage1np/contoso_sis/studentattendance.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION='2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage1/M365/roster/2021-06-15T04-04-12/Person/part-00000-90ae57f8-0d60-4c5a-bc4e-9866483ef367-c000.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION='2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage2/M365/Person/part-00000-b8157474-2acc-4d3d-a095-bc5cf6553591-c000.snappy.parquet',\n        FORMAT='PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage2/tutorial_01/Person/part-00000-45776a54-6c81-4904-8a7c-d577eddebbb7-c000.snappy.parquet',\n        FORMAT='PARQUET'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage1/tutorial_01/roster/2021-06-11/Person/part-00000-eacccad9-25c4-4ffc-a662-7ecce7b2d86b-c000.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION='2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 6')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "tutorial_01",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 9')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [Id]\n,[FirstSeenDateTime]\n,[LastSeenDateTime]\n,[GivenName]\n,[MiddleName]\n,[PreferredGivenName]\n,[PreferredMiddleName]\n,[PreferredSurname]\n,[Surname]\n FROM [dbo].[Person]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "tutorial_sql2",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ms_TechActivity')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Limitations of SQL Serverless with Delta Lake: \n-- https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n\n\n-- 0) Get info on inferred data types\n-- Notes: When querying the delta lake via T-SQL, the data types will be inferred based on what is specified in parquet.\n--        Parquet does not give you a way to specify column size, so strings are inferred as varchar(8000) which is not optimal.\nEXEC sp_describe_first_result_set N'\n\tSELECT * FROM \n    OPENROWSET(\n        BULK ''https://stoeacisd3ggimpl3.dfs.core.windows.net/stage2/ms_insights/TechActivity'',\n        FORMAT=''DELTA''\n    ) AS result';\n\n\n-- 1) Create a db via SQL serverless pool\nCREATE DATABASE s2_ms_insights;\nGO\nUSE s2_ms_insights;\nGO\n\n-- Always use UTF-8 collations.\n-- https://techcommunity.microsoft.com/t5/azure-synapse-analytics/always-use-utf-8-collations-to-read-utf-8-text-in-serverless-sql/ba-p/1883633\nALTER DATABASE s2_ms_insights \n    COLLATE Latin1_General_100_BIN2_UTF8;\n\n-- 2) set location info\nCREATE EXTERNAL DATA SOURCE DeltaLakeStorage\nWITH ( location = 'https://stoeacisd3ggimpl3.dfs.core.windows.net/stage2' );\nGO\nCREATE EXTERNAL FILE FORMAT DeltaLakeFormat WITH ( FORMAT_TYPE = DELTA );\nGO\n\n-- 3) Create an external table\n-- for info on data types: https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver15\n-- Note: this does not work for the columns used in partitioning the data because you always get null values (in this example you always get null for year and month). \n-- This is a known limitation: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#partitioning-column-returns-null-values\nCREATE EXTERNAL TABLE TechActivity (\n     SignalType varchar(200),\n     StartTime datetime2,\n     UserAgent varchar(200),\n     SignalId varchar(200),\n     SisClassId varchar(200),\n     ClassId varchar(200),\n     ChannelId varchar(200),\n     AppName varchar(200),\n     ActorId varchar(200),\n     ActorRole varchar(200),\n     SchemaVersion varchar(50),\n     AssignmentId varchar(200),\n     SubmissionId varchar(200),\n     [Action] varchar(200),\n     DueDate datetime2,\n     ClassCreationDate datetime2,\n     Grade varchar(50),\n     SourceFileExtension varchar(50),\n     MeetingDuration int,\n     [year] smallint,\n     [month] tinyint\n) WITH (\n    LOCATION = '/ms_insights/TechActivity',\n    DATA_SOURCE = DeltaLakeStorage,\n    FILE_FORMAT = DeltaLakeFormat\n);\n\n\nselect * from TechActivity where year is not null\n\n--EXEC sp_describe_first_result_set N'\n\t--SELECT * FROM TechActivity AS result';\n\ndrop external table TechActivity2\n\nCREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET );\nGO\n\n-- This will perform the select and copy the data to the specified LOCATION\nCREATE EXTERNAL TABLE TechActivity2\n  WITH (\n    LOCATION = '/ms_insights/TechActivity2',\n    DATA_SOURCE = DeltaLakeStorage,\n    FILE_FORMAT = ParquetFormat\n)\n  AS select \n     [SignalType] VARCHAR (200),\n     StartTime\n     from \n        OPENROWSET(BULK 'https://stoeacisd3ggimpl3.dfs.core.windows.net/stage2/ms_insights/TechActivity/year=2021/**',\n        FORMAT='PARQUET') AS [r]\n\n\n\n-- Create a partitioned view\n-- https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-views#delta-lake-partitioned-views\nCREATE OR ALTER VIEW TechActivityView\nAS SELECT \n    CAST(SignalType as varchar(200)) as SignalType,\n    CAST(StartTime as datetime2) as StartTime,\n    CAST(UserAgent as varchar(200)) as UserAgent,\n    CAST(SignalId as varchar(200)) as SignalId,\n    CAST(SisClassId as varchar(200)) as SisClassId,\n    CAST(ClassId as varchar(200)) as ClassId,\n    CAST(ChannelId as varchar(200)) as ChannelId,\n    CAST(AppName as varchar(200)) as AppName,\n    CAST(ActorId as varchar(200)) as ActorId,\n    CAST(ActorRole as varchar(200)) as ActorRole,\n    CAST(SchemaVersion as varchar(50)) as SchemaVersion,\n    CAST(AssignmentId as varchar(200)) as AssignmentId,\n    CAST(SubmissionId as varchar(200)) as SubmissionId,\n    CAST([Action] as varchar(200)) as [Action],\n    CAST(DueDate as datetime2) as DueDate,\n    CAST(ClassCreationDate as datetime2) as ClassCreationDate,\n    CAST(Grade as varchar(50)) as Grade,\n    CAST(SourceFileExtension as varchar(50)) as SourceFileExtension,\n    CAST(MeetingDuration as int) as MeetingDuration,\n    CAST([year] as smallint) as [year],\n    CAST([month] as tinyint) as [month]\nFROM  \n    OPENROWSET(\n        BULK 'ms_insights/TechActivity',\n        DATA_SOURCE = 'DeltaLakeStorage',\n        FORMAT='DELTA'\n    ) AS [r]\n\nSELECT count(*) FROM TechActivityView\n\ndrop view TechActivityView2\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/query_delta')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage2/tutorial_01/Person',\n        FORMAT='DELTA'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tutorial_01')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Tutorial on use of SQL serverless pool.\n-- For more info: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview\n\n-- 1) Create a db via SQL serverless pool\nCREATE DATABASE tutorial_sql_01;\nGO\nUSE tutorial_sql_01;\nGO\n\nCREATE EXTERNAL DATA SOURCE DeltaLakeStorage\nWITH ( location = 'https://stoeacisd3gg1.dfs.core.windows.net/stage2' );\nGO\nCREATE EXTERNAL FILE FORMAT DeltaLakeFormat WITH ( FORMAT_TYPE = DELTA );\nGO\n\n-- 2) Create an external table\n-- for info on data types: https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver15\nCREATE EXTERNAL TABLE Person (\n     Id varchar(50),\n     FirstSeenDateTime datetime,\n     LastSeenDateTime datetime,\n     GivenName varchar(70),\n     MiddleName varchar(70),\n     PreferredGivenName varchar(70),\n     PreferredMiddleName varchar(70),\n     PreferredSurname varchar(70),\n     Surname varchar(70)\n) WITH (\n    LOCATION = '/tutorial_01/Person',\n    DATA_SOURCE = DeltaLakeStorage,\n    FILE_FORMAT = DeltaLakeFormat\n);\n\n-- 3) Create a view\ncreate or alter view PersonView\nas\nselect *\nfrom openrowset(\n           bulk '/tutorial_01/Person',\n           data_source = 'DeltaLakeStorage',\n           format = 'delta'\n    ) with (\n        Id varchar(50),\n        FirstSeenDateTime datetime,\n        LastSeenDateTime datetime,\n        GivenName varchar(70),\n        MiddleName varchar(70),\n        PreferredGivenName varchar(70),\n        PreferredMiddleName varchar(70),\n        PreferredSurname varchar(70),\n        Surname varchar(70)\n        ) as rows\n\n-- 4) use CETAS to export select statement with OPENROWSET result to  storage\n-- more info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas\n-- Note: CETAS does not support writing out as Delta Lake format\nCREATE EXTERNAL FILE FORMAT ParquetFormat WITH ( FORMAT_TYPE = PARQUET );\n\nCREATE EXTERNAL TABLE PersonAthruE\nWITH (\n    LOCATION = '/tutorial_01/PersonAthruE',\n    DATA_SOURCE = DeltaLakeStorage,\n    FILE_FORMAT = ParquetFormat\n)  \nAS\nSELECT *\nFROM\n    OPENROWSET(\n           bulk '/tutorial_01/Person',\n           data_source = 'DeltaLakeStorage',\n           format = 'delta'\n    ) AS [r]\nWHERE Surname LIKE '[A-E]%'\nGO\n\n\n-- 5) Create stored procs (note that you won't see the stored proc listed in Synapse studio, but you will see it in SSMS)\n-- more info: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-stored-procedures\nCREATE PROCEDURE drop_external_table_if_exists @name SYSNAME\nAS BEGIN\n    IF (0 <> (SELECT COUNT(*) FROM sys.external_tables WHERE name = @name))\n    BEGIN\n        DECLARE @drop_stmt NVARCHAR(200) = N'DROP EXTERNAL TABLE ' + @name; \n        EXEC sp_executesql @tsql = @drop_stmt;\n    END\nEND\n\n-- execute the stored proc\nEXEC drop_external_table_if_exists PersonAthruE\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "tutorial_sql_02",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Clever_s1_to_s2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 0,
				"nbformat_minor": 0,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Setup Clever data\n",
							"\n",
							"\n",
							"\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\n",
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\n",
							"stage_anon = 'abfss://stage-anon@' + storage_account + '.dfs.core.windows.net'\n",
							"\n",
							"folder_to_process = 'inbound'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/Clever/' + folder_to_process, header='true')\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/Clever')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Contoso ISD')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Packages"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ContosoISD_example')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "af00ccc2-3a3e-425a-bca1-7dec7511cb02"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ContosoISD Example\r\n",
							"This example demonstrates how to use the OEA framework and modules to process incoming data, perform data prep, and view the data in an example Power BI dashboard.\r\n",
							"\r\n",
							"# Running the example\r\n",
							"1) Select your spark pool in the \"Attach to\" dropdown list above.\r\n",
							"\r\n",
							"2) Click on \"Publish\" in the top nav bar (and wait a few seconds for the notification that says \"Publishing completed\").\r\n",
							"\r\n",
							"3) Click on \"Run all\" at the top of this tab (and wait for the processing to complete - which can take around 5 to 10 minutes).\r\n",
							"\r\n",
							"4) Open the dashboard in Power BI desktop and point it to your newly setup data lake (you can download the pbix from here: [techInequityDashboardContoso v2.pbix](https://github.com/microsoft/OpenEduAnalytics/blob/main/packages/ContosoISD/power_bi/techInequityDashboardContoso%20v2.pbix) )\r\n",
							"\r\n",
							"# More info\r\n",
							"See [OEA Solution Guide](https://github.com/microsoft/OpenEduAnalytics/blob/main/docs/OpenEduAnalyticsSolutionGuide.pdf) for more details on this example."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /example_modules_py"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 0) Initialize the OEA framework and modules needed.\r\n",
							"oea = OEA()\r\n",
							"m365 = M365(oea)\r\n",
							"contoso_sis = ContosoSIS(oea, 'contoso_sis', False)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 1) Land data into stage1 of your data lake, from multiple source systems (this example copies in test data sets that came with the OEA installation).\r\n",
							"contoso_sis.copy_test_data_to_stage1()\r\n",
							"m365.copy_test_data_to_stage1()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 2) Process the raw data (csv format) from stage1 into stage2 (adds schema details and writes out in parquet format).\r\n",
							"#    [Note: we're not performing pseudonymization in this example, so everything is written to container stage2np.]\r\n",
							"m365.process_roster_data_from_stage1()\r\n",
							"contoso_sis.process_data_from_stage1()\r\n",
							"m365.process_activity_data_from_stage1()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 3) Run additional prep on the data to create a unified dataset that can be used in a Power BI report\r\n",
							"\r\n",
							"# Process sectionmark data. Convert id values to use the Person.Id and Section.Id values set in the m365 data.\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentsectionmark'), 'SectionMark')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Person'), 'Person')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Section'), 'Section')\r\n",
							"df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\r\n",
							"sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\r\n",
							"sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\r\n",
							"from SectionMark sm, Person p, Section s \\\r\n",
							"where sm.student_id = p.ExternalId \\\r\n",
							"and sm.section_id = s.ExternalId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/SectionMark')\r\n",
							"\r\n",
							"# Repeat the above process, this time for student attendance\r\n",
							"# Convert id values to use the Person.Id, Org.Id and Section.Id values\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/contoso_sis/studentattendance'), 'Attendance')\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Org'), 'Org')\r\n",
							"df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\r\n",
							"att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\r\n",
							"att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\r\n",
							"from Attendance att, Org o, Person p, Section s \\\r\n",
							"where att.student_id = p.ExternalId \\\r\n",
							"and att.school_id = o.ExternalId \\\r\n",
							"and att.section_id = s.ExternalId\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np +'/ContosoISD/Attendance')\r\n",
							"\r\n",
							"# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\r\n",
							"sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(oea.stage2np + '/m365/Course'), 'Course')\r\n",
							"df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\r\n",
							"df.write.format('parquet').mode('overwrite').save(oea.stage2np + '/ContosoISD/Course')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# 4) Create spark db's that point to the data in the data lake to allow for connecting via Power BI through use of the Serverless SQL endpoint.\r\n",
							"contoso_sis.create_stage2_db('PARQUET')\r\n",
							"m365.create_stage2_db('PARQUET')\r\n",
							"\r\n",
							"spark.sql('CREATE DATABASE IF NOT EXISTS s2_ContosoISD')\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Activity using PARQUET location '\" + oea.stage2np + \"/m365/TechActivity'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Calendar using PARQUET location '\" + oea.stage2np + \"/m365/Calendar'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Org using PARQUET location '\" + oea.stage2np + \"/m365/Org'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Person using PARQUET location '\" + oea.stage2np + \"/m365/Person'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.PersonIdentifier using PARQUET location '\" + oea.stage2np + \"/m365/PersonIdentifier'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.RefDefinition using PARQUET location '\" + oea.stage2np + \"/m365/RefDefinition'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Section using PARQUET location '\" + oea.stage2np + \"/m365/Section'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Session using PARQUET location '\" + oea.stage2np + \"/m365/Session'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StaffOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StaffOrgAffiliation'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StaffSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StaffSectionMembership'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StudentOrgAffiliation'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StudentSectionMembership'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Course using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Course'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.Attendance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Attendance'\")\r\n",
							"spark.sql(\"create table if not exists s2_ContosoISD.SectionMark using PARQUET location '\" + oea.stage2np + \"/ContosoISD/SectionMark'\")\r\n",
							"\r\n",
							"print(f\"Created spark db's.\\nYou can now open the 'techInequityDashboardContoso v2.pbix' dashboard and change the datasource to point to: {oea.serverless_sql_endpoint}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything\r\n",
							"You can uncomment line 11 in the cell below and run the cell to reset everything and walk through the process again from the top.\r\n",
							"\r\n",
							"Note: remember to comment out line 11 again to prevent accidental resetting of the example"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def reset_all_processing():\r\n",
							"    contoso_sis.delete_all_stages()\r\n",
							"    m365.delete_all_stages()\r\n",
							"    oea.rm_if_exists(oea.stage2np + '/ContosoISD')\r\n",
							"\r\n",
							"    oea.drop_db('s2_contoso_sis')\r\n",
							"    oea.drop_db('s2_contosoisd')\r\n",
							"    oea.drop_db('s2_m365')\r\n",
							"\r\n",
							"# Uncomment the following line and run this cell to reset everything if you want to walk through the process again.\r\n",
							"#reset_all_processing()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Lake Map')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark1",
						"name": "spark1",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"<img align=\"right\" height=\"50\" src=\"https://microsoft.github.io/OpenEduAnalytics/assets/images/oea-logo-nobg.png\">\r\n",
							"&nbsp\r\n",
							"&nbsp\r\n",
							"\r\n",
							"---\r\n",
							"\r\n",
							"# Data Lake Map\r\n",
							"This notebook provides documentation on the structure of the data lake and details on what data is brought in and how it's processed.\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# M365\r\n",
							"\r\n",
							"M365 data is ingested through the Insights Premium product, and setup via the SDS admin pages.\r\n",
							"\r\n",
							"## Roster data\r\n",
							"This is info about how roster data is received and processed.\r\n",
							"\r\n",
							"## Activity data\r\n",
							"This is info about how roster data is received and processed."
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_activity')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Process all roster data from stage1/M365/inbound/roster into stage 2 in delta lake format.\r\n",
							"(overwrites previously loaded data with the latest snapshot data)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"instrumentation_key = 'InstrumentationKey=5a4a6026-f008-4e9e-a48a-96a412d250d6'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"spark = SparkSession.builder.appName(\"OEA_M365_processing\").getOrCreate()\r\n",
							"\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.DEBUG) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string=instrumentation_key))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming activity data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process M365 activity data\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"csvDF = csvDF.dropDuplicates(['SignalId'])\r\n",
							"csvDF = csvDF.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"query.start(dest_path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming roster data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"s1_path = stage1 + '/M365/inbound/roster'\r\n",
							"s2_path = stage2 + '/M365'\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(entity, schema):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source={s1_path}, destination={s2_path}\")\r\n",
							"        csvDF = spark.readStream.csv(s1_path + '/*/' + entity + '/*.csv', header='false', schema=schema)\r\n",
							"\r\n",
							"        query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", s1_path + '/' + entity + '/_checkpoints')\r\n",
							"        query.start(s2_path + '/' + entity)\r\n",
							"\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        print(error)\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"s1_to_s2('AadUser', schema)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Batch process M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(source_path, schema, destination_path):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(source_path, header='false', schema=schema)\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(destination_path)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"def process_M365_roster(file_path):\r\n",
							"    #AadUser\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUser', schema, stage2 + '/M365/AadUser')\r\n",
							"    #AadUserPersonMapping\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUserPersonMapping', schema, stage2 + '/M365/AadUserPersonMapping')\r\n",
							"    #Course\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Course', schema, stage2 + '/M365/Course')\r\n",
							"    #CourseGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseGradeLevel', schema, stage2 + '/M365/CourseGradeLevel')\r\n",
							"    #CourseSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseSubject', schema, stage2 + '/M365/CourseSubject')\r\n",
							"    #Enrollment\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"    s1_to_s2(file_path + '/Enrollment', schema, stage2 + '/M365/Enrollment')\r\n",
							"    #Organization\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Organization', schema, stage2 + '/M365/Organization')\r\n",
							"    #Person\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Person', schema, stage2 + '/M365/Person')\r\n",
							"    #PersonDemographic\r\n",
							"    schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographic', schema, stage2 + '/M365/PersonDemographic')\r\n",
							"    #PersonDemographicEthnicity\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicEthnicity', schema, stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"    #PersonDemographicPersonFlag\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicPersonFlag', schema, stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"    #PersonDemographicRace\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicRace', schema, stage2 + '/M365/PersonDemographicRace')\r\n",
							"    #PersonEmailAddress\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonEmailAddress', schema, stage2 + '/M365/PersonEmailAddress')\r\n",
							"    #PersonIdentifier\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonIdentifier', schema, stage2 + '/M365/PersonIdentifier')\r\n",
							"    #PersonOrganizationRole\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonOrganizationRole', schema, stage2 + '/M365/PersonOrganizationRole')\r\n",
							"    #PersonPhoneNumber\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonPhoneNumber', schema, stage2 + '/M365/PersonPhoneNumber')\r\n",
							"    #PersonRelationship\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonRelationship', schema, stage2 + '/M365/PersonRelationship')\r\n",
							"    #RefDefinition\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"    s1_to_s2(file_path + '/RefDefinition', schema, stage2 + '/M365/RefDefinition')\r\n",
							"    #Section\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Section', schema, stage2 + '/M365/Section')\r\n",
							"    #SectionGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionGradeLevel', schema, stage2 + '/M365/SectionGradeLevel')\r\n",
							"    #SectionSession\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSession', schema, stage2 + '/M365/SectionSession')\r\n",
							"    #SectionSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSubject', schema, stage2 + '/M365/SectionSubject')\r\n",
							"    #Session\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Session', schema, stage2 + '/M365/Session')\r\n",
							"    #SourceSystem\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SourceSystem', schema, stage2 + '/M365/SourceSystem')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"inbound_path = stage1 + '/M365/inbound/roster'\r\n",
							"processed_path = stage1 + '/M365/processed/roster'\r\n",
							"\r\n",
							"logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"items = mssparkutils.fs.ls(inbound_path)\r\n",
							"for item in items:\r\n",
							"    if item.isDir:\r\n",
							"        process_M365_roster(item.path)\r\n",
							"        mssparkutils.fs.mv(item.path, processed_path, True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything to run it again"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_lib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Modules/M365"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Variables\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"s1_M365 = stage1 + '/M365'\r\n",
							"s2_M365 = stage2 + '/M365'\r\n",
							"s3_M365 = stage3 + '/M365'\r\n",
							"\r\n",
							"M365_module_path = oea_path + '/modules/M365'\r\n",
							"s1_M365_activity = s1_M365 + '/activity'\r\n",
							"s1_M365_roster = s1_M365 + '/roster'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Schemas"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Schema definitions\r\n",
							"M365_schemas = {}\r\n",
							"M365_schemas['TechActivity'] = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"M365_schemas['AadUser'] = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"M365_schemas['AadUserPersonMapping'] = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"M365_schemas['Course'] = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"M365_schemas['CourseGradeLevel'] = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"M365_schemas['CourseSubject'] = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"M365_schemas['Enrollment'] = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"M365_schemas['Organization'] = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"M365_schemas['Person'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"M365_schemas['PersonDemographic'] = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"M365_schemas['PersonDemographicEthnicity'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"M365_schemas['PersonDemographicPersonFlag'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"M365_schemas['PersonDemographicRace'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"M365_schemas['PersonEmailAddress'] = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"M365_schemas['PersonIdentifier'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"M365_schemas['PersonOrganizationRole'] = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"M365_schemas['PersonPhoneNumber'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"M365_schemas['PersonRelationship'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"M365_schemas['RefDefinition'] = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"M365_schemas['Section'] = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"M365_schemas['SectionGradeLevel'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"M365_schemas['SectionSession'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"M365_schemas['SectionSubject'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"M365_schemas['Session'] = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"M365_schemas['SourceSystem'] = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Functions"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Provides a small sample activity data set\r\n",
							"def get_sample_activity_data(entity):\r\n",
							"    df = spark.read.csv(M365_module_path + '/sample_data/small_samples/' + entity + '.csv', schema=M365_schemas[entity])\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process TechActivity data from stage1 into stage2 using structured streaming\r\n",
							"def process_activity(source_path=s1_M365_activity, dest_path=s2_M365):\r\n",
							"    df = spark.readStream.csv(source_path + '/*/*.csv', header='false', schema=M365_schemas['TechActivity'])\r\n",
							"    df = df.dropDuplicates(['SignalId'])\r\n",
							"    df = df.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"    query = df.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"    query.start(dest_path + '/TechActivity')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Resets all TechActivity processing. This is intended for use during initial testing - use with caution\r\n",
							"# This function does the following:\r\n",
							"#  - deletes the _checkpoints dir from stage1/M365/activity\r\n",
							"#  - deletes the delta table at stage2/M365/TechActivity\r\n",
							"def reset_activity_processing():\r\n",
							"    # Delete stage2/M365/TechActivity if it exists\r\n",
							"    items = mssparkutils.fs.ls(s2_M365)\r\n",
							"    #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"    for item in items:\r\n",
							"        if item.name == 'TechActivity':\r\n",
							"            mssparkutils.fs.rm(s2_M365 + '/TechActivity', True)\r\n",
							"\r\n",
							"    items = mssparkutils.fs.ls(s1_M365 + '/activity')\r\n",
							"    for item in items:\r\n",
							"        if item.name == '_checkpoints':\r\n",
							"            mssparkutils.fs.rm(s1_M365 + '/activity/_checkpoints', True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.utils import AnalysisException\r\n",
							"def process_roster_entity(path, entity):\r\n",
							"    try:\r\n",
							"        #logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(path + '/' + entity, header='false', schema=M365_schemas[entity])\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(s2_M365 + '/' + entity)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"\r\n",
							"def process_roster_date_folder(date_folder_path):\r\n",
							"    process_roster_entity(date_folder_path, 'AadUser')\r\n",
							"    process_roster_entity(date_folder_path, 'AadUserPersonMapping')\r\n",
							"    process_roster_entity(date_folder_path, 'Course')\r\n",
							"    process_roster_entity(date_folder_path, 'CourseGradeLevel')\r\n",
							"    process_roster_entity(date_folder_path, 'CourseSubject')\r\n",
							"    process_roster_entity(date_folder_path, 'Enrollment')\r\n",
							"    process_roster_entity(date_folder_path, 'Organization')\r\n",
							"    process_roster_entity(date_folder_path, 'Person')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonDemographic')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonDemographicEthnicity')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonDemographicPersonFlag')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonDemographicRace')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonEmailAddress')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonIdentifier')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonOrganizationRole')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonPhoneNumber')\r\n",
							"    process_roster_entity(date_folder_path, 'PersonRelationship')\r\n",
							"    process_roster_entity(date_folder_path, 'RefDefinition')\r\n",
							"    process_roster_entity(date_folder_path, 'Section')\r\n",
							"    process_roster_entity(date_folder_path, 'SectionGradeLevel')\r\n",
							"    process_roster_entity(date_folder_path, 'SectionSession')\r\n",
							"    process_roster_entity(date_folder_path, 'SectionSubject')\r\n",
							"    process_roster_entity(date_folder_path, 'Session')\r\n",
							"    process_roster_entity(date_folder_path, 'SourceSystem')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def process_roster():\r\n",
							"    logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"    items = mssparkutils.fs.ls(s1_M365 + '/roster')\r\n",
							"    for item in items:\r\n",
							"        if item.isDir:\r\n",
							"            process_roster_date_folder(item.path)\r\n",
							"            mssparkutils.fs.mv(item.path, s1_M365 + '/roster_processed/', True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def reset_roster_processing():\r\n",
							"\r\n",
							"    if path_exists(stage2 + '/M365'):\r\n",
							"        # Delete roster delta tables (everything other than TechActivity)\r\n",
							"        items = mssparkutils.fs.ls(stage2 + '/M365')\r\n",
							"        #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"        for item in items:\r\n",
							"            if item.name != 'TechActivity':\r\n",
							"                mssparkutils.fs.rm(item.path, True)\r\n",
							"\r\n",
							"    if path_exists(stage1 + '/M365/roster_processed'):\r\n",
							"        # Move roster data back in to \"inbound\" folder\r\n",
							"        items = mssparkutils.fs.ls(stage1 + '/M365/roster_processed')\r\n",
							"        for item in items:\r\n",
							"            mssparkutils.fs.mv(item.path, stage1 + '/M365/roster', True)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_lib_backup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Process all roster data from stage1/M365/inbound/roster into stage 2 in delta lake format.\r\n",
							"(overwrites previously loaded data with the latest snapshot data)"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming activity data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process M365 activity data\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"csvDF = csvDF.dropDuplicates(['SignalId'])\r\n",
							"csvDF = csvDF.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"query.start(dest_path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming roster data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"s1_path = stage1 + '/M365/inbound/roster'\r\n",
							"s2_path = stage2 + '/M365'\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(entity, schema):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source={s1_path}, destination={s2_path}\")\r\n",
							"        csvDF = spark.readStream.csv(s1_path + '/*/' + entity + '/*.csv', header='false', schema=schema)\r\n",
							"\r\n",
							"        query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", s1_path + '/' + entity + '/_checkpoints')\r\n",
							"        query.start(s2_path + '/' + entity)\r\n",
							"\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        print(error)\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"s1_to_s2('AadUser', schema)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Batch process M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(source_path, schema, destination_path):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(source_path, header='false', schema=schema)\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(destination_path)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"def process_M365_roster(file_path):\r\n",
							"    #AadUser\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUser', schema, stage2 + '/M365/AadUser')\r\n",
							"    #AadUserPersonMapping\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUserPersonMapping', schema, stage2 + '/M365/AadUserPersonMapping')\r\n",
							"    #Course\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Course', schema, stage2 + '/M365/Course')\r\n",
							"    #CourseGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseGradeLevel', schema, stage2 + '/M365/CourseGradeLevel')\r\n",
							"    #CourseSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseSubject', schema, stage2 + '/M365/CourseSubject')\r\n",
							"    #Enrollment\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"    s1_to_s2(file_path + '/Enrollment', schema, stage2 + '/M365/Enrollment')\r\n",
							"    #Organization\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Organization', schema, stage2 + '/M365/Organization')\r\n",
							"    #Person\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Person', schema, stage2 + '/M365/Person')\r\n",
							"    #PersonDemographic\r\n",
							"    schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographic', schema, stage2 + '/M365/PersonDemographic')\r\n",
							"    #PersonDemographicEthnicity\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicEthnicity', schema, stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"    #PersonDemographicPersonFlag\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicPersonFlag', schema, stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"    #PersonDemographicRace\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicRace', schema, stage2 + '/M365/PersonDemographicRace')\r\n",
							"    #PersonEmailAddress\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonEmailAddress', schema, stage2 + '/M365/PersonEmailAddress')\r\n",
							"    #PersonIdentifier\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonIdentifier', schema, stage2 + '/M365/PersonIdentifier')\r\n",
							"    #PersonOrganizationRole\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonOrganizationRole', schema, stage2 + '/M365/PersonOrganizationRole')\r\n",
							"    #PersonPhoneNumber\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonPhoneNumber', schema, stage2 + '/M365/PersonPhoneNumber')\r\n",
							"    #PersonRelationship\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonRelationship', schema, stage2 + '/M365/PersonRelationship')\r\n",
							"    #RefDefinition\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"    s1_to_s2(file_path + '/RefDefinition', schema, stage2 + '/M365/RefDefinition')\r\n",
							"    #Section\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Section', schema, stage2 + '/M365/Section')\r\n",
							"    #SectionGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionGradeLevel', schema, stage2 + '/M365/SectionGradeLevel')\r\n",
							"    #SectionSession\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSession', schema, stage2 + '/M365/SectionSession')\r\n",
							"    #SectionSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSubject', schema, stage2 + '/M365/SectionSubject')\r\n",
							"    #Session\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Session', schema, stage2 + '/M365/Session')\r\n",
							"    #SourceSystem\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SourceSystem', schema, stage2 + '/M365/SourceSystem')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"inbound_path = stage1 + '/M365/inbound/roster'\r\n",
							"processed_path = stage1 + '/M365/processed/roster'\r\n",
							"\r\n",
							"logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"items = mssparkutils.fs.ls(inbound_path)\r\n",
							"for item in items:\r\n",
							"    if item.isDir:\r\n",
							"        process_M365_roster(item.path)\r\n",
							"        mssparkutils.fs.mv(item.path, processed_path, True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything to run it again"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Testing delta"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"student_schema = [\"OrderId\",\"OrderDate\", \"Region\", \"City\", \"Category\",\"Product\",\"Quantity\",\"UnitPrice\",\"TotalPrice\"]\r\n",
							"\r\n",
							"def load_data():\r\n",
							"    # N.B. this would actually be loading the real data from somewhere in the data lake, or elsewhere...\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            (1,\"01/01/2020\",\"East\",\"Boston\",\"Bars\",\"Carrot\",33,1.77,58.41),\r\n",
							"            (2,\"04/01/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",87,3.49,303.63),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (11,\"31/01/2020\",\"East\",\"Boston\",\"Cookies\",\"Arrowroot\",36,2.18,78.48),\r\n",
							"            (12,\"03/02/2020\",\"East\",\"Boston\",\"Cookies\",\"Chocolate Chip\",31,1.87,57.97),\r\n",
							"            (13,\"06/02/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",28,3.49,97.72)      \r\n",
							"        ],\r\n",
							"        orders_schema \r\n",
							"    )\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"students_schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", IntegerType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"  ])\r\n",
							"\r\n",
							"df1 = spark.createDataFrame(\r\n",
							"    [\r\n",
							"        ('John', 'Smith', 1, 'M'),\r\n",
							"        ('Jacob', 'Jones', 2, 'M'),\r\n",
							"        ('Jane', 'Johnson', 3, 'F'),\r\n",
							"        ('Jill', 'Jackson', 4, 'F')\r\n",
							"    ],\r\n",
							"    students_schema\r\n",
							")\r\n",
							"\r\n",
							"df2 = spark.createDataFrame(\r\n",
							"    [\r\n",
							"        ('John', 'Smith', 1, 'M'),\r\n",
							"        ('Jacob', 'Jones', 2, 'M'),\r\n",
							"        ('Jane', 'Johnson', 3, 'F')\r\n",
							"    ],\r\n",
							"    students_schema\r\n",
							")\r\n",
							"\r\n",
							"display(df2)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"students_schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", IntegerType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"  ])\r\n",
							"\r\n",
							"def load_data():\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            ('John', 'Smith', 1, 'M'),\r\n",
							"            ('Jacob', 'Jones', 2, 'M'),\r\n",
							"            ('Jane', 'Johnson', 3, 'F'),\r\n",
							"            ('Jill', 'Jackson', 4, 'F')                      \r\n",
							"        ],\r\n",
							"        students_schema\r\n",
							"    )\r\n",
							"    return df\r\n",
							"\r\n",
							"df = load_data()\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_process')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Modules/M365"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/OEA_Core"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/Modules/M365/M365_lib"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"process_activity()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_roster_v0p3_s1_to_s2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Process all roster data from stage1/M365/inbound/roster into stage 2 in delta lake format.\r\n",
							"(overwrites previously loaded data with the latest snapshot data)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"file_path = '/M365/inbound/roster/2021-06-15T04-04-12'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/AadUser', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/AadUser')\r\n",
							"#AadUserPersonMapping\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/AadUserPersonMapping', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/AadUserPersonMapping')\r\n",
							"#Course\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Course', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Course')\r\n",
							"#CourseGradeLevel\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/CourseGradeLevel', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/CourseGradeLevel')\r\n",
							"#CourseSubject\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/CourseSubject', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/CourseSubject')\r\n",
							"#Enrollment\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Enrollment', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Enrollment')\r\n",
							"#Organization\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Organization', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Organization')\r\n",
							"#Person\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Person', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Person')\r\n",
							"#PersonDemographic\r\n",
							"schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonDemographic', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographic')\r\n",
							"#PersonDemographicEthnicity\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonDemographicEthnicity', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"#PersonDemographicPersonFlag\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonDemographicPersonFlag', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"#PersonDemographicRace\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonDemographicRace', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicRace')\r\n",
							"#PersonEmailAddress\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonEmailAddress', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonEmailAddress')\r\n",
							"#PersonIdentifier\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonIdentifier', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonIdentifier')\r\n",
							"#PersonOrganizationRole\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonOrganizationRole', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonOrganizationRole')\r\n",
							"#PersonPhoneNumber\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonPhoneNumber', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonPhoneNumber')\r\n",
							"#PersonRelationship\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/PersonRelationship', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonRelationship')\r\n",
							"#RefDefinition\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/RefDefinition', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/RefDefinition')\r\n",
							"#Section\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Section', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Section')\r\n",
							"#SectionGradeLevel\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/SectionGradeLevel', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionGradeLevel')\r\n",
							"#SectionSession\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/SectionSession', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionSession')\r\n",
							"#SectionSubject\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/SectionSubject', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionSubject')\r\n",
							"#Session\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/Session', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Session')\r\n",
							"#SourceSystem\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"df = spark.read.csv(stage1 + file_path + '/SourceSystem', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SourceSystem')\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_roster_v0p3_s1_to_s2_with_move_to_processed')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Process all roster data from stage1/M365/inbound/roster into stage 2 in delta lake format.\r\n",
							"(overwrites previously loaded data with the latest snapshot data)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"instrumentation_key = 'InstrumentationKey=5a4a6026-f008-4e9e-a48a-96a412d250d6'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"spark = SparkSession.builder.appName(\"OEA_M365_processing\").getOrCreate()\r\n",
							"\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.DEBUG) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string=instrumentation_key))"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming activity data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process M365 activity data\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"csvDF = csvDF.dropDuplicates(['SignalId'])\r\n",
							"csvDF = csvDF.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"query.start(dest_path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming roster data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"s1_path = stage1 + '/M365/inbound/roster'\r\n",
							"s2_path = stage2 + '/M365'\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(entity, schema):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source={s1_path}, destination={s2_path}\")\r\n",
							"        csvDF = spark.readStream.csv(s1_path + '/*/' + entity + '/*.csv', header='false', schema=schema)\r\n",
							"\r\n",
							"        query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", s1_path + '/' + entity + '/_checkpoints')\r\n",
							"        query.start(s2_path + '/' + entity)\r\n",
							"\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        print(error)\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"s1_to_s2('AadUser', schema)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Batch process M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": true,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(source_path, schema, destination_path):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(source_path, header='false', schema=schema)\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(destination_path)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"def process_M365_roster(file_path):\r\n",
							"    #AadUser\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUser', schema, stage2 + '/M365/AadUser')\r\n",
							"    #AadUserPersonMapping\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUserPersonMapping', schema, stage2 + '/M365/AadUserPersonMapping')\r\n",
							"    #Course\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Course', schema, stage2 + '/M365/Course')\r\n",
							"    #CourseGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseGradeLevel', schema, stage2 + '/M365/CourseGradeLevel')\r\n",
							"    #CourseSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseSubject', schema, stage2 + '/M365/CourseSubject')\r\n",
							"    #Enrollment\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"    s1_to_s2(file_path + '/Enrollment', schema, stage2 + '/M365/Enrollment')\r\n",
							"    #Organization\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Organization', schema, stage2 + '/M365/Organization')\r\n",
							"    #Person\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Person', schema, stage2 + '/M365/Person')\r\n",
							"    #PersonDemographic\r\n",
							"    schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographic', schema, stage2 + '/M365/PersonDemographic')\r\n",
							"    #PersonDemographicEthnicity\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicEthnicity', schema, stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"    #PersonDemographicPersonFlag\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicPersonFlag', schema, stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"    #PersonDemographicRace\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicRace', schema, stage2 + '/M365/PersonDemographicRace')\r\n",
							"    #PersonEmailAddress\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonEmailAddress', schema, stage2 + '/M365/PersonEmailAddress')\r\n",
							"    #PersonIdentifier\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonIdentifier', schema, stage2 + '/M365/PersonIdentifier')\r\n",
							"    #PersonOrganizationRole\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonOrganizationRole', schema, stage2 + '/M365/PersonOrganizationRole')\r\n",
							"    #PersonPhoneNumber\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonPhoneNumber', schema, stage2 + '/M365/PersonPhoneNumber')\r\n",
							"    #PersonRelationship\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonRelationship', schema, stage2 + '/M365/PersonRelationship')\r\n",
							"    #RefDefinition\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"    s1_to_s2(file_path + '/RefDefinition', schema, stage2 + '/M365/RefDefinition')\r\n",
							"    #Section\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Section', schema, stage2 + '/M365/Section')\r\n",
							"    #SectionGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionGradeLevel', schema, stage2 + '/M365/SectionGradeLevel')\r\n",
							"    #SectionSession\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSession', schema, stage2 + '/M365/SectionSession')\r\n",
							"    #SectionSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSubject', schema, stage2 + '/M365/SectionSubject')\r\n",
							"    #Session\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Session', schema, stage2 + '/M365/Session')\r\n",
							"    #SourceSystem\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SourceSystem', schema, stage2 + '/M365/SourceSystem')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"inbound_path = stage1 + '/M365/inbound/roster'\r\n",
							"processed_path = stage1 + '/M365/processed/roster'\r\n",
							"\r\n",
							"logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"items = mssparkutils.fs.ls(inbound_path)\r\n",
							"for item in items:\r\n",
							"    if item.isDir:\r\n",
							"        process_M365_roster(item.path)\r\n",
							"        mssparkutils.fs.mv(item.path, processed_path, True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything to run it again"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Testing delta"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"student_schema = [\"OrderId\",\"OrderDate\", \"Region\", \"City\", \"Category\",\"Product\",\"Quantity\",\"UnitPrice\",\"TotalPrice\"]\r\n",
							"\r\n",
							"def load_data():\r\n",
							"    # N.B. this would actually be loading the real data from somewhere in the data lake, or elsewhere...\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            (1,\"01/01/2020\",\"East\",\"Boston\",\"Bars\",\"Carrot\",33,1.77,58.41),\r\n",
							"            (2,\"04/01/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",87,3.49,303.63),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (11,\"31/01/2020\",\"East\",\"Boston\",\"Cookies\",\"Arrowroot\",36,2.18,78.48),\r\n",
							"            (12,\"03/02/2020\",\"East\",\"Boston\",\"Cookies\",\"Chocolate Chip\",31,1.87,57.97),\r\n",
							"            (13,\"06/02/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",28,3.49,97.72)      \r\n",
							"        ],\r\n",
							"        orders_schema \r\n",
							"    )\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"students_schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", IntegerType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"  ])\r\n",
							"\r\n",
							"df1 = spark.createDataFrame(\r\n",
							"    [\r\n",
							"        ('John', 'Smith', 1, 'M'),\r\n",
							"        ('Jacob', 'Jones', 2, 'M'),\r\n",
							"        ('Jane', 'Johnson', 3, 'F'),\r\n",
							"        ('Jill', 'Jackson', 4, 'F')\r\n",
							"    ],\r\n",
							"    students_schema\r\n",
							")\r\n",
							"\r\n",
							"df2 = spark.createDataFrame(\r\n",
							"    [\r\n",
							"        ('John', 'Smith', 1, 'M'),\r\n",
							"        ('Jacob', 'Jones', 2, 'M'),\r\n",
							"        ('Jane', 'Johnson', 3, 'F')\r\n",
							"    ],\r\n",
							"    students_schema\r\n",
							")\r\n",
							"\r\n",
							"display(df2)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"students_schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", IntegerType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"  ])\r\n",
							"\r\n",
							"def load_data():\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            ('John', 'Smith', 1, 'M'),\r\n",
							"            ('Jacob', 'Jones', 2, 'M'),\r\n",
							"            ('Jane', 'Johnson', 3, 'F'),\r\n",
							"            ('Jill', 'Jackson', 4, 'F')                      \r\n",
							"        ],\r\n",
							"        students_schema\r\n",
							"    )\r\n",
							"    return df\r\n",
							"\r\n",
							"df = load_data()\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_tests')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Modules/M365"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/OEA_Core\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/Modules/M365/M365_lib"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Move this to M365_process"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"process_activity()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"reset_activity_processing()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"process_roster()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"reset_roster_processing()"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#df = get_sample_activity_data('TechActivity')\r\n",
							"df = get_sample_activity_data('Section')\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_v0p3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"use_test_env = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"if use_test_env:\r\n",
							"    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\r\n",
							"    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\r\n",
							"    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\r\n",
							"else:\r\n",
							"    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1_m365_roster = stage1 + '/M365/roster/2021-06-15T04-04-12'\r\n",
							"stage1_m365_activity = stage1 + '/m365/DIPData/Activity/ApplicationUsage'\r\n",
							"\r\n",
							"# Process Roster data from stage 1 to stage 2\r\n",
							"# This includes:\r\n",
							"# - adding column names\r\n",
							"# - casting values into specific data types when appropriate\r\n",
							"\r\n",
							"def prep_stage1_M365(entity_name, sql_str):\r\n",
							"  df = spark.read.csv(stage1_m365_roster + '/' + entity_name, header='false')\r\n",
							"  if (df.count() > 0):\r\n",
							"    sqlContext.registerDataFrameAsTable(df, entity_name)\r\n",
							"    df = spark.sql(sql_str)\r\n",
							"    df.write.format('delta').mode('overwrite').save(stage2 + '/M365/' + entity_name)\r\n",
							"\r\n",
							"# Person\r\n",
							"sql_str = \"select _c0 Id, to_timestamp(_c1) FirstSeenDateTime, to_timestamp(_c2) LastSeenDateTime, _c3 GivenName, _c4 MiddleName, _c5 PreferredGivenName, _c6 PreferredMiddleName, _c7 PreferredSurname, _c8 Surname from Person\"\r\n",
							"prep_stage1_M365('Person', sql_str)\r\n",
							"# PersonDemographic\r\n",
							"sql_str = \"select _c0 PersonId, to_timestamp(_c1) FirstSeenDateTime, _c2 BirthCity, _c3 BirthCityCountryCode, _c4 BirthDate, _c5 BirthState, _c6 RefSexId from PersonDemographic\"\r\n",
							"prep_stage1_M365('PersonDemographic', sql_str)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#print(mssparkutils.fs.ls(stage1 + '/M365/roster'))\r\n",
							"folders = mssparkutils.fs.ls(stage1 + '/M365/roster')\r\n",
							"for folder in folders:\r\n",
							"    print(folder[1])"
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/M365_v0p3_Delta')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"ff79d21e-0554-4995-a831-9400c0024e94": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"SignalType": "PostChannelMessage",
												"ChannelId": "19:20990825150f4da78ffdcd6a05cfa2e2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "5ddbce29-3ab9-405d-9350-05945a8a8542",
												"ClassId": "9ab188a9-815d-411d-83a3-a98b56a6c614",
												"StartTime": "2021-06-02T14:48:11Z",
												"AppName": "Teams",
												"SignalId": "AAMkAGRlM2ViNzg5LWMzOGEtNGYyMC05MTFhLWE5MzAxNTk3NjMzNwBGAAAAAAD6n_vRPyLsQ4uBYtj1MoVQBwBONi1figD3Spv1d4sN1luqAAAAAGZ3AABONi1figD3Spv1d4sN1luqAADDKaaJAAA=",
												"ActorRole": "Student"
											},
											{
												"SignalType": "ReplyChannelMessage",
												"ChannelId": "19:20990825150f4da78ffdcd6a05cfa2e2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "5ddbce29-3ab9-405d-9350-05945a8a8542",
												"ClassId": "9ab188a9-815d-411d-83a3-a98b56a6c614",
												"StartTime": "2021-06-02T16:36:46Z",
												"AppName": "Teams",
												"SignalId": "AAMkAGRlM2ViNzg5LWMzOGEtNGYyMC05MTFhLWE5MzAxNTk3NjMzNwBGAAAAAAD6n_vRPyLsQ4uBYtj1MoVQBwBONi1figD3Spv1d4sN1luqAAAAAGZ3AABONi1figD3Spv1d4sN1luqAADDKaaRAAA=",
												"ActorRole": "Student"
											},
											{
												"SignalType": "PostChannelMessage",
												"ChannelId": "19:1500af388f6647739e2300bdc1db53a2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "7951c64a-b45d-4adb-8842-5e7ab60d17e1",
												"ClassId": "5e0407f7-c973-4f25-9cef-302117b080c6",
												"StartTime": "2021-06-02T14:45:34Z",
												"AppName": "Teams",
												"SignalId": "AAMkADg2NDk3MmIwLTM5NDUtNGQwZS04NDQzLTM5MjViMjZiYzFiNABGAAAAAAAEYRCcXVmaTYyFMuIJ9RetBwDPLnXMbt9HQaxJaYXomUUhAAAAAAEnAADPLnXMbt9HQaxJaYXomUUhAADC-nCbAAA="
											},
											{
												"SignalType": "ReplyChannelMessage",
												"ChannelId": "19:1500af388f6647739e2300bdc1db53a2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "7951c64a-b45d-4adb-8842-5e7ab60d17e1",
												"ClassId": "5e0407f7-c973-4f25-9cef-302117b080c6",
												"StartTime": "2021-06-02T14:45:58Z",
												"AppName": "Teams",
												"SignalId": "AAMkADg2NDk3MmIwLTM5NDUtNGQwZS04NDQzLTM5MjViMjZiYzFiNABGAAAAAAAEYRCcXVmaTYyFMuIJ9RetBwDPLnXMbt9HQaxJaYXomUUhAAAAAAEnAADPLnXMbt9HQaxJaYXomUUhAADC-nCcAAA="
											},
											{
												"SignalType": "PostChannelMessage",
												"ChannelId": "19:1500af388f6647739e2300bdc1db53a2@thread.tacv2",
												"SchemaVersion": "1.1",
												"ActorId": "7951c64a-b45d-4adb-8842-5e7ab60d17e1",
												"ClassId": "5e0407f7-c973-4f25-9cef-302117b080c6",
												"StartTime": "2021-06-02T14:45:34Z",
												"AppName": "Teams",
												"SignalId": "AAMkADg2NDk3MmIwLTM5NDUtNGQwZS04NDQzLTM5MjViMjZiYzFiNABGAAAAAAAEYRCcXVmaTYyFMuIJ9RetBwDPLnXMbt9HQaxJaYXomUUhAAAAAAEnAADPLnXMbt9HQaxJaYXomUUhAADC-nCbAAA="
											}
										],
										"schema": {
											"SignalType": "string",
											"StartTime": "int",
											"UserAgent": "string",
											"SignalId": "string",
											"SisClassId": "string",
											"ClassId": "string",
											"ChannelId": "string",
											"AppName": "string",
											"ActorId": "string",
											"ActorRole": "string",
											"SchemaVersion": "string",
											"AssignmentId": "string",
											"SubmissionId": "string",
											"Action": "string",
											"DueDate": "int",
											"ClassCreationDate": "int",
											"Grade": "string",
											"SourceFileExtension": "string",
											"MeetingDuration": "int"
										}
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [
												"SignalType"
											],
											"seriesFieldKeys": [
												"StartTime"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"use_test_env = False"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"if use_test_env:\r\n",
							"    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\r\n",
							"    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\r\n",
							"    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\r\n",
							"else:\r\n",
							"    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1) First ingest all roster data (overwrite previously loaded data with the latest snapshot data)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/AadUser', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/AadUser')\r\n",
							"#AadUserPersonMapping\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/AadUserPersonMapping', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/AadUserPersonMapping')\r\n",
							"#Course\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Course', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Course')\r\n",
							"#CourseGradeLevel\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/CourseGradeLevel', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/CourseGradeLevel')\r\n",
							"#CourseSubject\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/CourseSubject', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/CourseSubject')\r\n",
							"#Enrollment\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Enrollment', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Enrollment')\r\n",
							"#Organization\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Organization', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Organization')\r\n",
							"#Person\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Person', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Person')\r\n",
							"#PersonDemographic\r\n",
							"schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonDemographic', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographic')\r\n",
							"#PersonDemographicEthnicity\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonDemographicEthnicity', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"#PersonDemographicPersonFlag\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonDemographicPersonFlag', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"#PersonDemographicRace\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonDemographicRace', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonDemographicRace')\r\n",
							"#PersonEmailAddress\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonEmailAddress', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonEmailAddress')\r\n",
							"#PersonIdentifier\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonIdentifier', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonIdentifier')\r\n",
							"#PersonOrganizationRole\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonOrganizationRole', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonOrganizationRole')\r\n",
							"#PersonPhoneNumber\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonPhoneNumber', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonPhoneNumber')\r\n",
							"#PersonRelationship\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/PersonRelationship', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/PersonRelationship')\r\n",
							"#RefDefinition\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/RefDefinition', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/RefDefinition')\r\n",
							"#Section\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Section', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Section')\r\n",
							"#SectionGradeLevel\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/SectionGradeLevel', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionGradeLevel')\r\n",
							"#SectionSession\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/SectionSession', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionSession')\r\n",
							"#SectionSubject\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/SectionSubject', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SectionSubject')\r\n",
							"#Session\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/Session', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/Session')\r\n",
							"#SourceSystem\r\n",
							"schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"df = spark.read.csv(stage1 + '/M365/roster/2021-06-02T04-12-09/SourceSystem', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/M365/SourceSystem')\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 2) Now ingest the new activity data"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"\r\n",
							"#TechActivity\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"df = spark.read.csv(stage1 + '/M365/activity/2021-06-10/*.csv', header='false', schema=schema)\r\n",
							"df.write.format('delta').mode('append').save(stage2 + '/M365/TechActivity')\r\n",
							"#df.write.format('delta').mode('overwrite').save(stage2 + '/M365/TechActivity')\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Note that you can't rely on this having the latest for some reason (you can verify that by running a comparison query in SQL)\r\n",
							"df = spark.read.load('abfss://stage2@stoeacisd3gg1.dfs.core.windows.net/M365/TechActivity', format='delta')\r\n",
							"display(df)\r\n",
							"print(df.count())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Main')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "1) Implementation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/OEA_Core"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /2) OEA Framework/Modules/M365/M365_lib"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"process_activity()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"df = spark.sql(\"SELECT * FROM tutorial_01.person\")\n",
							"# df.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark3v3",
						"name": "spark3v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"import pytest"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"orders_schema = [\"OrderId\",\"OrderDate\", \"Region\", \"City\", \"Category\",\"Product\",\"Quantity\",\"UnitPrice\",\"TotalPrice\"]"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def load_data():\r\n",
							"    # N.B. this would actually be loading the real data from somewhere in the data lake, or elsewhere...\r\n",
							"    df = spark.createDataFrame(\r\n",
							"        [\r\n",
							"            (1,\"01/01/2020\",\"East\",\"Boston\",\"Bars\",\"Carrot\",33,1.77,58.41),\r\n",
							"            (2,\"04/01/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",87,3.49,303.63),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (3,\"07/01/2020\",\"West\",\"Los Angeles\",\"Cookies\",\"Chocolate Chip\",58,1.87,108.46),\r\n",
							"            (11,\"31/01/2020\",\"East\",\"Boston\",\"Cookies\",\"Arrowroot\",36,2.18,78.48),\r\n",
							"            (12,\"03/02/2020\",\"East\",\"Boston\",\"Cookies\",\"Chocolate Chip\",31,1.87,57.97),\r\n",
							"            (13,\"06/02/2020\",\"East\",\"Boston\",\"Crackers\",\"Whole Wheat\",28,3.49,97.72)      \r\n",
							"        ],\r\n",
							"        orders_schema \r\n",
							"    )\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def remove_duplicate_orders(df):\r\n",
							"    # Swap these lines over to fail test\r\n",
							"    #return df \r\n",
							"    #return df.distinct()\r\n",
							"    return df.dropDuplicates([\"OrderId\"])"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Tests\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def mytest():\r\n",
							"    df = load_data()\r\n",
							"    df = remove_duplicate_orders(df)\r\n",
							"    #Assert\r\n",
							"    expected_orders = 6\r\n",
							"    number_of_orders = df.count()\r\n",
							"    assert number_of_orders == 6, f'Expected {expected_orders} order after remove_duplicate_orders() but {number_of_orders} returned.'"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark3v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark3v3",
						"name": "spark3v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /testing/Notebook2"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df = load_data()\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#mssparkutils.notebook.run(\"notebook path\", <timeoutSeconds>, <parameterMap>)\r\n",
							"#mssparkutils.notebook.run(\"folder/Sample1\", 90, {\"input\": 20 })\r\n",
							"mssparkutils.notebook.run(\"Notebook2\")\r\n",
							"df = load_data()\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 12
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_Core')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"instrumentation_key = 'InstrumentationKey=5a4a6026-f008-4e9e-a48a-96a412d250d6'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\r\n",
							"oea_path = 'abfss://oea-framework@' + storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"spark = SparkSession.builder.appName(\"OEA_Framework\").getOrCreate()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Imports\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from notebookutils import mssparkutils\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"from pyspark.sql.utils import AnalysisException, FileNotFoundException"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Setup logging"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.DEBUG) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string=instrumentation_key))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Function lib"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Returns true if the path exists\r\n",
							"def path_exists(path):\r\n",
							"    tableExists = False\r\n",
							"    try:\r\n",
							"        items = mssparkutils.fs.ls(path)\r\n",
							"        tableExists = True\r\n",
							"    except Exception as e:\r\n",
							"        # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\r\n",
							"        pass\r\n",
							"    return tableExists\r\n",
							"\r\n",
							"def file_exists(path):\r\n",
							"    return folder_exists(path)\r\n",
							"\r\n",
							"def folder_exists(path):\r\n",
							"    return folder_exists(path)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_Utils')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Microsoft Spark Utilities\r\n",
							"[Intro to Microsoft Spark Utilities](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"# to list out all of the available commands\r\n",
							"mssparkutils.fs.help()\r\n",
							"\r\n",
							"# an example of deleting a directory (and everything within it)\r\n",
							"mssparkutils.fs.rm('abfss://stage1@stoeacisd3gg1.dfs.core.windows.net/tutorial_01', True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# delete from M365 stage2, move files from \"processed\" back into \"inbound\" in stage1\r\n",
							"from notebookutils import mssparkutils\r\n",
							"mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"mssparkutils.fs.mv('abfss://stage1@stoeacisd3gg1.dfs.core.windows.net/M365/processed/roster/2021-06-15T04-04-12', stage1 + '/M365/inbound/roster', True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Logging example. More info at: https://github.com/balakreshnan/Samples2021/blob/main/Synapseworkspace/opencensuslog.md#azure-synapse-spark-logs-runtime-errors-to-application-insights\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.INFO) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=<insert instrumentation key here>'))\r\n",
							"\r\n",
							"logger.info(\"OEA info msg\")\r\n",
							"logger.debug(\"OEA: debug msg\")\r\n",
							"logger.debug(\"OEA: warning msg\")\r\n",
							"logger.error(\"OEA: error msg\")\r\n",
							"logger.exception(\"OEA: exception msg\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Check on existence of a folder in the data lake.\r\n",
							"\r\n",
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"def folderExists(path, foldername):\r\n",
							"    items = mssparkutils.fs.ls(path)\r\n",
							"    tableExists = False\r\n",
							"    for item in items:\r\n",
							"        if item.name == foldername:\r\n",
							"            tableExists = True\r\n",
							"    return tableExists\r\n",
							"\r\n",
							"#print(folderExists(stage1 + '/tutorial_01/activity/2021-06-10', 'TechActivity'))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Create an empty delta table if one doesn't yet exist\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import *\r\n",
							"\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"if not DeltaTable.isDeltaTable(spark, stage2 + '/tutorial_01/TechActivity'):\r\n",
							"    emptyDF = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\r\n",
							"    emptyDF.write.format('delta').mode('overwrite').save(stage2 + '/tutorial_01/TechActivity')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							"\r\n",
							"    "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Structured Streaming\r\n",
							"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"#https://docs.delta.io/latest/delta-streaming.html#delta-table-as-a-sink\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').start(dest_path)\r\n",
							"\r\n",
							"query.stop()\r\n",
							"print(query.id)\r\n",
							"print(query.explain())\r\n",
							"print(spark.streams.active)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4bb631ba-56cb-401b-b7d3-fbbcec22718b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from notebookutils import mssparkutils\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType, ShortType\r\n",
							"from pyspark.sql import functions as F\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"import logging\r\n",
							"import pandas as pd\r\n",
							"import sys\r\n",
							"import re\r\n",
							"import json\r\n",
							"import datetime\r\n",
							"import random\r\n",
							"import io\r\n",
							"\r\n",
							"logger = logging.getLogger('OEA')\r\n",
							"\r\n",
							"class OEA:\r\n",
							"    def __init__(self, storage_account='', instrumentation_key='', salt='', logging_level=logging.DEBUG):\r\n",
							"        if storage_account:\r\n",
							"            self.storage_account = storage_account\r\n",
							"        else:\r\n",
							"            oea_id = mssparkutils.env.getWorkspaceName()[8:] # extracts the OEA id for this OEA instance from the synapse workspace name (based on OEA naming convention)\r\n",
							"            self.storage_account = 'stoea' + oea_id # sets the name of the storage account based on OEA naming convention\r\n",
							"        self.serverless_sql_endpoint = mssparkutils.env.getWorkspaceName() + '-ondemand.sql.azuresynapse.net'\r\n",
							"        self._initialize_logger(instrumentation_key, logging_level)\r\n",
							"        self.salt = salt\r\n",
							"        self.stage1np = 'abfss://stage1np@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.stage2np = 'abfss://stage2np@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.stage2p = 'abfss://stage2p@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.stage3np = 'abfss://stage3np@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.stage3p = 'abfss://stage3p@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"        self.framework_path = 'abfss://oea-framework@' + self.storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"        logger.debug(\"OEA initialized.\")\r\n",
							"\r\n",
							"    def _initialize_logger(self, instrumentation_key, logging_level):\r\n",
							"        logging.lastResort = None\r\n",
							"        # the logger will print an error like \"ValueError: I/O operation on closed file\" because we're trying to have log messages also print to stdout\r\n",
							"        # and apparently this causes issues on some of the spark executor nodes. The bottom line is that we don't want these logging errors to get printed in the notebook output.\r\n",
							"        logging.raiseExceptions = False\r\n",
							"        logger.setLevel(logging_level)\r\n",
							"\r\n",
							"        handler = logging.StreamHandler(sys.stdout)\r\n",
							"        handler.setLevel(logging_level)\r\n",
							"        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\r\n",
							"        handler.setFormatter(formatter)\r\n",
							"        logger.addHandler(handler) \r\n",
							"\r\n",
							"    def load(self, folder, table, stage=None, data_format='delta'):\r\n",
							"        \"\"\" Loads a dataframe based on the path specified in the given args \"\"\"\r\n",
							"        if stage is None: stage = self.stage2p\r\n",
							"        path = f\"{stage}/{folder}/{table}\"\r\n",
							"        try:\r\n",
							"            df = spark.read.load(f\"{stage}/{folder}/{table}\", format=data_format)\r\n",
							"            return df        \r\n",
							"        except AnalysisException as e:\r\n",
							"            raise ValueError(\"Failed to load. Are you sure you have the right path?\\nMore info below:\\n\" + str(e))\r\n",
							"\r\n",
							"    def load_from_stage1(self, path_and_filename, data_format='csv'):\r\n",
							"        \"\"\" Loads a dataframe with data from stage1, based on the path specified in the given args \"\"\"\r\n",
							"        path = f\"{self.stage1np}/{path_and_filename}\"\r\n",
							"        df = spark.read.load(path, format=data_format)\r\n",
							"        return df        \r\n",
							"\r\n",
							"    def load_sample_from_csv_file(self, path_and_filename, header=True, stage=None):\r\n",
							"        \"\"\" Loads a sample from the specified csv file and returns a pandas dataframe.\r\n",
							"            Ex: print(load_sample_from_csv_file('/student_data/students.csv'))\r\n",
							"        \"\"\"\r\n",
							"        if stage is None: stage = self.stage1np\r\n",
							"        csv_str = mssparkutils.fs.head(f\"{stage}/{path_and_filename}\") # https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#preview-file-content\r\n",
							"        complete_lines = re.match(r\".*\\n\", csv_str, re.DOTALL).group(0)\r\n",
							"        if header: header = 0 # for info on why this is needed: https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.read_csv.html\r\n",
							"        else: header = None\r\n",
							"        pdf = pd.read_csv(io.StringIO(complete_lines), sep=',', header=header)\r\n",
							"        return pdf\r\n",
							"\r\n",
							"    def print_stage(self, path):\r\n",
							"        \"\"\" Prints out the highlevel contents of the specified stage.\"\"\"\r\n",
							"        msg = path + \"\\n\"\r\n",
							"        folders = self.get_folders(path)\r\n",
							"        for folder_name in folders:\r\n",
							"            entities = self.get_folders(path + '/' + folder_name)\r\n",
							"            msg += f\"{folder_name}: {entities}\\n\"\r\n",
							"        print(msg)            \r\n",
							"\r\n",
							"    def fix_column_names(self, df):\r\n",
							"        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\r\n",
							"        df_with_valid_column_names = df.select([F.col(col).alias(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", col)) for col in df.columns])\r\n",
							"        return df_with_valid_column_names\r\n",
							"\r\n",
							"    def to_spark_schema(self, schema):#: list[list[str]]):\r\n",
							"        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \r\n",
							"            Example:\r\n",
							"            schemas['Person'] = [['Id','string','hash'],\r\n",
							"                                    ['CreateDate','timestamp','no-op'],\r\n",
							"                                    ['LastModifiedDate','timestamp','no-op']]\r\n",
							"            to_spark_schema(schemas['Person'])\r\n",
							"        \"\"\"\r\n",
							"        fields = []\r\n",
							"        for col_name, dtype, op in schema:\r\n",
							"            fields.append(StructField(col_name, globals()[dtype.lower().capitalize() + \"Type\"](), True))\r\n",
							"        spark_schema = StructType(fields)\r\n",
							"        return spark_schema\r\n",
							"\r\n",
							"    def pseudonymize(self, df, schema): #: list[list[str]]):\r\n",
							"        \"\"\" Performs pseudonymization of the given dataframe based on the provided schema.\r\n",
							"            For example, if the given df is for an entity called person, \r\n",
							"            2 dataframes will be returned, one called person that has hashed ids and masked fields, \r\n",
							"            and one called person_lookup that contains the original person_id, person_id_pseudo,\r\n",
							"            and the non-masked values for columns marked to be masked.\"\"\"\r\n",
							"        \r\n",
							"        df_pseudo = df_lookup = df\r\n",
							"\r\n",
							"        for col_name, dtype, op in schema:\r\n",
							"            if op == \"hash-no-lookup\" or op == \"hnl\":\r\n",
							"                # This means that the lookup can be performed against a different table so no lookup is needed.\r\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
							"                df_lookup = df_lookup.drop(col_name)           \r\n",
							"            elif op == \"hash\" or op == 'h':\r\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
							"                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(self.salt)), 256))\r\n",
							"            elif op == \"mask\" or op == 'm':\r\n",
							"                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\r\n",
							"            elif op == \"no-op\" or op == 'x':\r\n",
							"                df_lookup = df_lookup.drop(col_name)\r\n",
							"\r\n",
							"        df_pseudo = self.fix_column_names(df_pseudo)\r\n",
							"        df_lookup = self.fix_column_names(df_lookup)\r\n",
							"\r\n",
							"        return (df_pseudo, df_lookup)\r\n",
							"\r\n",
							"    # Returns true if the path exists\r\n",
							"    def path_exists(self, path):\r\n",
							"        tableExists = False\r\n",
							"        try:\r\n",
							"            items = mssparkutils.fs.ls(path)\r\n",
							"            tableExists = True\r\n",
							"        except Exception as e:\r\n",
							"            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\r\n",
							"            pass\r\n",
							"        return tableExists\r\n",
							"\r\n",
							"    def ls(self, path):\r\n",
							"        folders = []\r\n",
							"        files = []\r\n",
							"        try:\r\n",
							"            items = mssparkutils.fs.ls(path)\r\n",
							"            for item in items:\r\n",
							"                if item.isFile:\r\n",
							"                    files.append(item.name)\r\n",
							"                elif item.isDir:\r\n",
							"                    folders.append(item.name)\r\n",
							"        except Exception as e:\r\n",
							"            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\r\n",
							"        return (folders, files)\r\n",
							"\r\n",
							"    def print_stage(self, path):\r\n",
							"        print(path)\r\n",
							"        folders = self.get_folders(path)\r\n",
							"        for folder_name in folders:\r\n",
							"            entities = self.get_folders(path + '/' + folder_name)\r\n",
							"            print(f\"{folder_name}: {entities}\")\r\n",
							"\r\n",
							"    # Return the list of folders found in the given path.\r\n",
							"    def get_folders(self, path):\r\n",
							"        dirs = []\r\n",
							"        try:\r\n",
							"            items = mssparkutils.fs.ls(path)\r\n",
							"            for item in items:\r\n",
							"                #print(item.name, item.isDir, item.isFile, item.path, item.size)\r\n",
							"                if item.isDir:\r\n",
							"                    dirs.append(item.name)\r\n",
							"        except Exception as e:\r\n",
							"            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\r\n",
							"        return dirs\r\n",
							"\r\n",
							"    # Remove a folder if it exists (defaults to use of recursive removal).\r\n",
							"    def rm_if_exists(self, path, recursive_remove=True):\r\n",
							"        try:\r\n",
							"            mssparkutils.fs.rm(path, recursive_remove)\r\n",
							"        except Exception as e:\r\n",
							"            pass\r\n",
							"\r\n",
							"    def pop_from_path(self, path):\r\n",
							"        \"\"\" Pops the last arg in a path and returns the path and the last arg as a tuple.\r\n",
							"            pop_from_path('abfss://stage2@xyz.dfs.core.windows.net/ms_insights/test.csv') # returns ('abfss://stage2@xyz.dfs.core.windows.net/ms_insights', 'test.csv')\r\n",
							"        \"\"\"\r\n",
							"        m = re.match(r\"(.*)\\/([^/]+)\", path)\r\n",
							"        return (m.group(1), m.group(2))\r\n",
							"\r\n",
							"    def parse_source_path(self, path):\r\n",
							"        \"\"\" Parses a path that looks like this: abfss://stage2@stoeacisd3ggimpl3.dfs.core.windows.net/ms_insights\r\n",
							"            and returns a dictionary like this: {'stage_num': '2', 'ss': 'ms_insights'}\r\n",
							"            Note that it will also return a 'stage_num' of 2 if the path is stage2p - this is by design because the spark db with the s2 prefix will be used for data in stage2 and stage2p.\r\n",
							"        \"\"\"\r\n",
							"        m = re.match(r\".*:\\/\\/stage(?P<stage_num>\\d+)[n]?[p]?@[^/]+\\/(?P<ss>[^/]+)\", path)\r\n",
							"        return m.groupdict()\r\n",
							"    \r\n",
							"    def create_db(self, source_path, source_format='DELTA'):\r\n",
							"        \"\"\" Creates a spark db based on the given path (assumes that every folder in the given path is a table).\r\n",
							"            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
							"        \"\"\"\r\n",
							"        source_info = self.parse_source_path(source_path)\r\n",
							"        db_name = f\"s{source_info['stage_num']}_{source_info['ss']}\"\r\n",
							"        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\r\n",
							"        dirs = self.get_folders(source_path)\r\n",
							"        for table_name in dirs:\r\n",
							"            spark.sql(f\"create table if not exists {db_name}.{table_name} using {source_format} location '{source_path}/{table_name}'\")\r\n",
							"        result = \"Database created: \" + db_name\r\n",
							"        logger.info(result)\r\n",
							"        return result\r\n",
							"\r\n",
							"    def drop_db(self, db_name):\r\n",
							"        \"\"\" Drop all tables in a db, then drop the db. \"\"\"\r\n",
							"        df = spark.sql('SHOW TABLES FROM ' + db_name)\r\n",
							"        for row in df.rdd.collect():\r\n",
							"            spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{row['tableName']}\")\r\n",
							"        spark.sql(f\"DROP DATABASE {db_name}\")\r\n",
							"        result = \"Database dropped: \" + db_name\r\n",
							"        logger.info(result)\r\n",
							"        return result         \r\n",
							"\r\n",
							"    # List installed packages\r\n",
							"    def list_packages(self):\r\n",
							"        import pkg_resources\r\n",
							"        for d in pkg_resources.working_set:\r\n",
							"            print(d)\r\n",
							"\r\n",
							"    def print_schema_starter(self, entity_name, df):\r\n",
							"        \"\"\" Prints a starter schema that can be modified as needed when developing the oea schema for a new module. \"\"\"\r\n",
							"        st = f\"self.schemas['{entity_name}'] = [\"\r\n",
							"        for col in df.schema:\r\n",
							"            st += f\"['{col.name}', '{str(col.dataType)[:-4].lower()}', 'no-op'],\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\"\r\n",
							"        return st[:-11] + ']'\r\n",
							"\r\n",
							"    def write_dict_as_csv(data, folder, filename, container=None):\r\n",
							"        \"\"\" Writes a dictionary as a csv to the specified location. This is helpful when creating test data sets and landing them in stage1np. \"\"\"\r\n",
							"        if container == None: container = self.stage1np\r\n",
							"        pdf = pd.DataFrame(data)\r\n",
							"        mssparkutils.fs.put(f\"{container}/{folder}/{filename}\", pdf.to_csv(index=False), True) # True indicates overwrite mode  \r\n",
							"\r\n",
							"class BaseOEAModule:\r\n",
							"    \"\"\" Provides data processing methods for Contoso SIS data (the student information system for the fictional Contoso school district).  \"\"\"\r\n",
							"    def __init__(self, oea, source_folder, pseudonymize = True):\r\n",
							"        self.pseudonymize = pseudonymize\r\n",
							"        self.oea = oea\r\n",
							"        self.stage1np = f\"{oea.stage1np}/{source_folder}\"\r\n",
							"        self.stage2np = f\"{oea.stage2np}/{source_folder}\"\r\n",
							"        self.stage2p = f\"{oea.stage2p}/{source_folder}\"\r\n",
							"        self.stage3np = f\"{oea.stage3np}/{source_folder}\"\r\n",
							"        self.stage3p = f\"{oea.stage3p}/{source_folder}\"\r\n",
							"        self.module_path = f\"{oea.framework_path}/modules/{source_folder}\"\r\n",
							"        self.schemas = {}\r\n",
							"   \r\n",
							"    def _process_entity_from_stage1(self, entity_name, format='csv', write_mode='overwrite', header='true'):\r\n",
							"        spark_schema = self.oea.to_spark_schema(self.schemas[entity_name])\r\n",
							"        df = spark.read.format(format).load(f\"{self.stage1np}/{entity_name}\", header=header, schema=spark_schema)\r\n",
							"\r\n",
							"        if self.pseudonymize:\r\n",
							"            df_pseudo, df_lookup = self.oea.pseudonymize(df, self.schemas[entity_name])\r\n",
							"            df_pseudo.write.format('delta').mode(write_mode).save(f\"{self.stage2p}/{entity_name}\")\r\n",
							"            if len(df_lookup.columns) > 0:\r\n",
							"                df_lookup.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}_lookup\")\r\n",
							"        else:\r\n",
							"            df = self.oea.fix_column_names(df)   \r\n",
							"            df.write.format('delta').mode(write_mode).save(f\"{self.stage2np}/{entity_name}\")\r\n",
							"\r\n",
							"    def delete_stage1(self):\r\n",
							"        self.oea.rm_if_exists(self.stage1np)\r\n",
							"\r\n",
							"    def delete_stage2(self):\r\n",
							"        self.oea.rm_if_exists(self.stage2np)\r\n",
							"        self.oea.rm_if_exists(self.stage2p)\r\n",
							"\r\n",
							"    def delete_stage3(self):\r\n",
							"        self.oea.rm_if_exists(self.stage3np)\r\n",
							"        self.oea.rm_if_exists(self.stage3p)                \r\n",
							"\r\n",
							"    def delete_all_stages(self):\r\n",
							"        self.delete_stage1()\r\n",
							"        self.delete_stage2()\r\n",
							"        self.delete_stage3()\r\n",
							"\r\n",
							"    def create_stage2_db(self, format='DELTA'):\r\n",
							"        self.oea.create_db(self.stage2p, format)\r\n",
							"        self.oea.create_db(self.stage2np, format)\r\n",
							"\r\n",
							"    def create_stage3_db(self, format='DELTA'):\r\n",
							"        self.oea.create_db(self.stage3p, format)\r\n",
							"        self.oea.create_db(self.stage3np, format)\r\n",
							"\r\n",
							"    def copy_test_data_to_stage1(self):\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data', self.stage1np, True)   \r\n",
							"\r\n",
							"class DataLakeWriter:\r\n",
							"    def __init__(self, root_destination):\r\n",
							"        self.root_destination = root_destination\r\n",
							"\r\n",
							"    def write(self, path_and_filename, data_str, format='csv'):\r\n",
							"        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_utils_test1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Microsoft Spark Utilities\r\n",
							"[Intro to Microsoft Spark Utilities](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python)"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"# to list out all of the available commands\r\n",
							"mssparkutils.fs.help()\r\n",
							"\r\n",
							"# an example of deleting a directory (and everything within it)\r\n",
							"mssparkutils.fs.rm('abfss://stage1@stoeacisd3gg1.dfs.core.windows.net/tutorial_01', True)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OurSIS')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "1) Implementation/Custom_Modules"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Reference')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "2) OEA Framework/Docs"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Run a notebook from a notebook\r\n",
							"this allows variables to be set and functions to be defined\r\n",
							"\r\n",
							"https://docs.microsoft.com/en-us/azure/synapse-analytics/synapse-notebook-activity?tabs=classical#run-another-synapse-notebook\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"%run /testing/Notebook2"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/corgis_explore')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark4v3p1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d00541ac-382b-49ce-bb97-7b756ccb84a5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark4v3p1",
						"name": "spark4v3p1",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark4v3p1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%run /OEA_py"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"   def load_from_stage1(self, path_and_filename, data_format='csv'):\r\n",
							"        \"\"\" Loads a dataframe with data from stage1, based on the path specified in the given args \"\"\"\r\n",
							"        path = f\"{self.stage1np}/{path_and_filename}\"\r\n",
							"        df = spark.read.load(path, format=data_format)\r\n",
							"        return df   "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"oea = OEA()\r\n",
							"df = oea.load_from_stage1(\"corgis2/graduates/graduates.csv\")\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/corgis_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5ae0788d-a875-4ecd-94ea-61d7d9dbbaf6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class Corgis(BaseOEAModule):\r\n",
							"    def __init__(self, oea, source_folder='corgis', pseudonymize = True):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder, pseudonymize)\r\n",
							"        self.schemas['studentattendance'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['school_year', 'integer', 'no-op'],\r\n",
							"                                            ['school_id', 'string', 'no-op'],\r\n",
							"                                            ['attendance_date', 'timestamp', 'no-op'],\r\n",
							"                                            ['all_day', 'string', 'no-op'],\r\n",
							"                                            ['Period', 'short', 'no-op'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['AttendanceCode', 'string', 'no-op'],\r\n",
							"                                            ['PresenceFlag', 'boolean', 'no-op'],\r\n",
							"                                            ['attendance_status', 'string', 'no-op'],\r\n",
							"                                            ['attendance_type', 'string', 'no-op'],\r\n",
							"                                            ['attendance_sequence', 'short', 'no-op']]\r\n",
							"\r\n",
							"\r\n",
							"                                            \r\n",
							"    def process_data_from_stage1(self):\r\n",
							"        self._process_entity_from_stage1('studentattendance', 'csv', 'overwrite', 'true')\r\n",
							"        self._process_entity_from_stage1('studentsectionmark', 'csv', 'overwrite', 'true')\r\n",
							"\r\n",
							"    def copy_test_data_to_stage1(self):\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentattendance.csv', self.stage1np + '/studentattendance/studentattendance.csv', True)\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentsectionmark.csv', self.stage1np + '/studentsectionmark/studentsectionmark.csv', True)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/example_modules_py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5e960d15-6053-4568-84f3-0d255caf9660"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"class ContosoSIS(BaseOEAModule):\r\n",
							"    def __init__(self, oea, source_folder='contoso_sis', pseudonymize = True):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder, pseudonymize)\r\n",
							"        self.schemas['studentattendance'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['school_year', 'integer', 'no-op'],\r\n",
							"                                            ['school_id', 'string', 'no-op'],\r\n",
							"                                            ['attendance_date', 'timestamp', 'no-op'],\r\n",
							"                                            ['all_day', 'string', 'no-op'],\r\n",
							"                                            ['Period', 'short', 'no-op'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['AttendanceCode', 'string', 'no-op'],\r\n",
							"                                            ['PresenceFlag', 'boolean', 'no-op'],\r\n",
							"                                            ['attendance_status', 'string', 'no-op'],\r\n",
							"                                            ['attendance_type', 'string', 'no-op'],\r\n",
							"                                            ['attendance_sequence', 'short', 'no-op']]\r\n",
							"\r\n",
							"        self.schemas['studentsectionmark'] = [['id', 'string', 'no-op'],\r\n",
							"                                            ['student_id', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['section_id', 'string', 'no-op'],\r\n",
							"                                            ['school_year', 'string', 'no-op'],\r\n",
							"                                            ['term_id', 'string', 'no-op'],\r\n",
							"                                            ['numeric_grade_earned', 'short', 'no-op'],\r\n",
							"                                            ['alpha_grade_earned', 'string', 'no-op'],\r\n",
							"                                            ['is_final_grade', 'string', 'no-op'],\r\n",
							"                                            ['credits_attempted', 'short', 'no-op'],\r\n",
							"                                            ['credits_earned', 'short', 'no-op'],\r\n",
							"                                            ['grad_credit_type', 'string', 'no-op']]\r\n",
							"                                            \r\n",
							"    def process_data_from_stage1(self):\r\n",
							"        self._process_entity_from_stage1('studentattendance', 'csv', 'overwrite', 'true')\r\n",
							"        self._process_entity_from_stage1('studentsectionmark', 'csv', 'overwrite', 'true')\r\n",
							"\r\n",
							"    def copy_test_data_to_stage1(self):\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentattendance.csv', self.stage1np + '/studentattendance/studentattendance.csv', True)\r\n",
							"        mssparkutils.fs.cp(self.module_path + '/test_data/studentsectionmark.csv', self.stage1np + '/studentsectionmark/studentsectionmark.csv', True)\r\n",
							"\r\n",
							"class M365(BaseOEAModule):\r\n",
							"    \"\"\"\r\n",
							"    Provides data processing methods for MS Insights data v0.2 format.\r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    def __init__(self, oea, source_folder='m365'):\r\n",
							"        BaseOEAModule.__init__(self, oea, source_folder)\r\n",
							"\r\n",
							"        self.stage1np_activity = self.stage1np + '/DIPData/Activity/ApplicationUsage'\r\n",
							"        self.stage1np_roster = self.stage1np + '/DIPData/Roster'\r\n",
							"\r\n",
							"        self.schemas['Activity0p2'] = [['SignalType', 'string', 'no-op'],\r\n",
							"                                            ['StartTime', 'timestamp', 'no-op'],\r\n",
							"                                            ['UserAgent', 'string', 'no-op'],\r\n",
							"                                            ['SignalId', 'string', 'no-op'],\r\n",
							"                                            ['SISClassId', 'string', 'no-op'],\r\n",
							"                                            ['OfficeClassId', 'string', 'no-op'],\r\n",
							"                                            ['ChannelId', 'string', 'no-op'],\r\n",
							"                                            ['AppName', 'string', 'no-op'],\r\n",
							"                                            ['ActorId', 'string', 'hash-no-lookup'],\r\n",
							"                                            ['ActorRole', 'string', 'no-op'],\r\n",
							"                                            ['SchemaVersion', 'string', 'no-op'],\r\n",
							"                                            ['AssignmentId', 'string', 'no-op'],\r\n",
							"                                            ['SubmissionId', 'string', 'no-op'],\r\n",
							"                                            ['Action', 'string', 'no-op'],\r\n",
							"                                            ['AssginmentDueDate', 'string', 'no-op'],\r\n",
							"                                            ['ClassCreationDate', 'string', 'no-op'],\r\n",
							"                                            ['Grade', 'string', 'no-op'],\r\n",
							"                                            ['SourceFileExtension', 'string', 'no-op'],\r\n",
							"                                            ['MeetingDuration', 'string', 'no-op']]\r\n",
							"        self.schemas['Calendar'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['SchoolYear', 'integer', 'no-op'],\r\n",
							"                                            ['IsCurrent', 'boolean', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op']]\r\n",
							"        self.schemas['Course'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Code', 'string', 'no-op'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['CalendarId', 'string', 'no-op']]\r\n",
							"        self.schemas['Org'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Identifier', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['ParentOrgId', 'string', 'no-op'],\r\n",
							"                                            ['RefOrgTypeId', 'string', 'no-op'],\r\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\r\n",
							"        self.schemas['Person'] = [['Id', 'string', 'hash'],\r\n",
							"                                            ['FirstName', 'string', 'mask'],\r\n",
							"                                            ['MiddleName', 'string', 'mask'],\r\n",
							"                                            ['LastName', 'string', 'mask'],\r\n",
							"                                            ['GenerationCode', 'string', 'no-op'],\r\n",
							"                                            ['Prefix', 'string', 'no-op'],\r\n",
							"                                            ['EnabledUser', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'hash'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\r\n",
							"        self.schemas['PersonIdentifier'] = [['Id', 'string', 'hash'],\r\n",
							"                                            ['Identifier', 'string', 'hash'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['RefIdentifierTypeId', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'hash'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['SourceSystemId', 'string', 'no-op']]\r\n",
							"        self.schemas['RefDefinition'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['RefType', 'string', 'no-op'],\r\n",
							"                                            ['Namespace', 'string', 'no-op'],\r\n",
							"                                            ['Code', 'string', 'no-op'],\r\n",
							"                                            ['SortOrder', 'integer', 'no-op'],\r\n",
							"                                            ['Description', 'string', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op']]\r\n",
							"        self.schemas['Section'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['Code', 'string', 'no-op'],\r\n",
							"                                            ['Location', 'string', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['CourseId', 'string', 'no-op'],\r\n",
							"                                            ['RefSectionTypeId', 'string', 'no-op'],\r\n",
							"                                            ['SessionId', 'string', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op']]\r\n",
							"        self.schemas['Session'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['Name', 'string', 'no-op'],\r\n",
							"                                            ['BeginDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['EndDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['CalendarId', 'string', 'no-op'],\r\n",
							"                                            ['ParentSessionId', 'string', 'no-op'],\r\n",
							"                                            ['RefSessionTypeId', 'string', 'no-op']]\r\n",
							"        self.schemas['StaffOrgAffiliation'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['IsPrimary', 'boolean', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefStaffOrgRoleId', 'string', 'no-op']]\r\n",
							"        self.schemas['StaffSectionMembership'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['IsPrimaryStaffForSection', 'boolean', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefStaffSectionRoleId', 'string', 'no-op'],\r\n",
							"                                            ['SectionId', 'string', 'no-op']]\r\n",
							"        self.schemas['StudentOrgAffiliation'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['IsPrimary', 'boolean', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['OrgId', 'string', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefGradeLevelId', 'string', 'no-op'],\r\n",
							"                                            ['RefStudentOrgRoleId', 'string', 'no-op'],\r\n",
							"                                            ['RefEnrollmentStatusId', 'string', 'no-op']]\r\n",
							"        self.schemas['StudentSectionMembership'] = [['Id', 'string', 'no-op'],\r\n",
							"                                            ['EntryDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExitDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['ExternalId', 'string', 'no-op'],\r\n",
							"                                            ['CreateDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['LastModifiedDate', 'timestamp', 'no-op'],\r\n",
							"                                            ['IsActive', 'boolean', 'no-op'],\r\n",
							"                                            ['PersonId', 'string', 'hash'],\r\n",
							"                                            ['RefGradeLevelWhenCourseTakenId', 'string', 'no-op'],\r\n",
							"                                            ['RefStudentSectionRoleId', 'string', 'no-op'],\r\n",
							"                                            ['SectionId', 'string', 'no-op']]\r\n",
							"    \r\n",
							"    def process_activity_data_from_stage1(self):\r\n",
							"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \r\n",
							"            https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\r\n",
							"        \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights activity data from: \" + self.stage1np_activity)\r\n",
							"\r\n",
							"        spark_schema = self.oea.to_spark_schema(self.schemas['Activity0p2'])\r\n",
							"        df = spark.read.csv(self.stage1np_activity + '/*.csv', header='false', schema=spark_schema) \r\n",
							"        sqlContext.registerDataFrameAsTable(df, 'Activity')\r\n",
							"        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(self.oea.stage2np + '/m365/PersonIdentifier'), 'PersonIdentifier')\r\n",
							"        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(self.oea.stage2np + '/m365/RefDefinition'), 'RefDefinition')\r\n",
							"\r\n",
							"        df = spark.sql( \r\n",
							"            \"select act.SignalType, act.StartTime, act.UserAgent, act.SignalId, act.SISClassId, act.OfficeClassId, act.ChannelId, \\\r\n",
							"            act.AppName, act.ActorId, act.ActorRole, act.SchemaVersion, act.AssignmentId, act.SubmissionId, act.Action, act.AssginmentDueDate, \\\r\n",
							"            act.ClassCreationDate, act.Grade, act.SourceFileExtension, act.MeetingDuration, pi.PersonId \\\r\n",
							"            from PersonIdentifier pi, RefDefinition rd, Activity act \\\r\n",
							"            where \\\r\n",
							"                pi.RefIdentifierTypeId = rd.Id \\\r\n",
							"                and rd.RefType = 'RefIdentifierType' \\\r\n",
							"                and rd.Code = 'ActiveDirectoryId' \\\r\n",
							"                and pi.Identifier = act.ActorId\")\r\n",
							"\r\n",
							"        df = df.dropDuplicates(['SignalId'])\r\n",
							"        df = df.withColumn('year', F.year(F.col('StartTime'))).withColumn('month', F.month(F.col('StartTime')))\r\n",
							"        df = self.oea.fix_column_names(df)\r\n",
							"        df.write.format('parquet').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/TechActivity')\r\n",
							"\r\n",
							"    def reset_activity_processing(self):\r\n",
							"        \"\"\" Resets all TechActivity processing. This is intended for use during initial testing - use with caution. \"\"\"\r\n",
							"        self.oea.rm_if_exists(self.stage2p + '/TechActivity')\r\n",
							"        self.oea.rm_if_exists(self.stage2np + '/TechActivity')\r\n",
							"        logger.info(f\"Deleted TechActivity from stage2\")  \r\n",
							"\r\n",
							"    def _process_roster_entity(self, path):\r\n",
							"        try:\r\n",
							"            base_path, filename = self.oea.pop_from_path(path)\r\n",
							"            entity = filename[:-4]\r\n",
							"            logger.debug(f\"Processing roster entity: path={path}, entity={entity}\")\r\n",
							"            spark_schema = self.oea.to_spark_schema(self.schemas[entity])\r\n",
							"            df = spark.read.csv(path, header='false', schema=spark_schema)\r\n",
							"            df = self.oea.fix_column_names(df)\r\n",
							"            df.write.format('parquet').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/' + entity)\r\n",
							"\r\n",
							"        except (AnalysisException) as error:\r\n",
							"            logger.exception(str(error))\r\n",
							"\r\n",
							"    def process_roster_data_from_stage1(self):\r\n",
							"        \"\"\" Processes all roster data in stage1 and writes out to stage2 and stage2p \"\"\"\r\n",
							"        logger.info(\"Processing ms_insights roster data from: \" + self.stage1np)\r\n",
							"\r\n",
							"        items = mssparkutils.fs.ls(self.stage1np_roster)\r\n",
							"        #print(items)\r\n",
							"        for item in items:\r\n",
							"            if item.isFile:\r\n",
							"                self._process_roster_entity(item.path)\r\n",
							"\r\n",
							"    def reset_roster_processing(self):\r\n",
							"        \"\"\" Resets all stage1 to stage2 processing of roster data. \"\"\"\r\n",
							"        # cleanup stage2np\r\n",
							"        if self.oea.path_exists(self.stage2np):\r\n",
							"            # Delete roster tables (everything other than TechActivity)\r\n",
							"            items = mssparkutils.fs.ls(self.stage2np)\r\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"            for item in items:\r\n",
							"                if item.name != 'TechActivity':\r\n",
							"                    mssparkutils.fs.rm(item.path, True)\r\n",
							"        # cleanup stage2p\r\n",
							"        if self.oea.path_exists(self.stage2p):\r\n",
							"            # Delete roster tables (everything other than TechActivity)\r\n",
							"            items = mssparkutils.fs.ls(self.stage2p)\r\n",
							"            #print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"            for item in items:\r\n",
							"                if item.name != 'TechActivity':\r\n",
							"                    mssparkutils.fs.rm(item.path, True)    \r\n",
							"  \r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook4')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%pyspark\n",
							"df = spark.sql(\"SELECT * FROM tutorial_01.person\")\n",
							"# df.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/setup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# setup the OEA framework"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"instrumentation_key = 'InstrumentationKey=5a4a6026-f008-4e9e-a48a-96a412d250d6'"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"spark = SparkSession.builder.appName(\"OEA_M365_processing\").getOrCreate()\r\n",
							"\r\n",
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"logger = logging.getLogger(__name__)\r\n",
							"logger.setLevel(logging.DEBUG) # https://docs.python.org/3/library/logging.html#logging-levels\r\n",
							"logger.addHandler(AzureLogHandler(connection_string=instrumentation_key))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming activity data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Process M365 activity data\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from delta.tables import DeltaTable\r\n",
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"inbound_path = stage1 + '/tutorial_01/activity'\r\n",
							"dest_path = stage2 + '/tutorial_01/TechActivity'\r\n",
							"schema = StructType([StructField('SignalType', StringType()),StructField('StartTime', TimestampType()),StructField('UserAgent', StringType()),StructField('SignalId', StringType()),StructField('SisClassId', StringType()),StructField('ClassId', StringType()),StructField('ChannelId', StringType()),StructField('AppName', StringType()),StructField('ActorId', StringType()),StructField('ActorRole', StringType()),StructField('SchemaVersion', StringType()),StructField('AssignmentId', StringType()),StructField('SubmissionId', StringType()),StructField('Action', StringType()),StructField('DueDate', TimestampType()),StructField('ClassCreationDate', TimestampType()),StructField('Grade', StringType()),StructField('SourceFileExtension', StringType()),StructField('MeetingDuration', IntegerType())])\r\n",
							"\r\n",
							"csvDF = spark.readStream.csv(inbound_path + '/*/*.csv', header='false', schema=schema)\r\n",
							"csvDF = csvDF.dropDuplicates(['SignalId'])\r\n",
							"csvDF = csvDF.withColumn('year', year(col('StartTime'))).withColumn('month', month(col('StartTime')))\r\n",
							"\r\n",
							"query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", inbound_path + '/_checkpoints').partitionBy('year', 'month')\r\n",
							"query.start(dest_path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Streaming roster data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"s1_path = stage1 + '/M365/inbound/roster'\r\n",
							"s2_path = stage2 + '/M365'\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(entity, schema):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source={s1_path}, destination={s2_path}\")\r\n",
							"        csvDF = spark.readStream.csv(s1_path + '/*/' + entity + '/*.csv', header='false', schema=schema)\r\n",
							"\r\n",
							"        query = csvDF.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", s1_path + '/' + entity + '/_checkpoints')\r\n",
							"        query.start(s2_path + '/' + entity)\r\n",
							"\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        print(error)\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"#AadUser\r\n",
							"schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"s1_to_s2('AadUser', schema)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Batch process M365 data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": true,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"\r\n",
							"# Process data in s1 into s2\r\n",
							"def s1_to_s2(source_path, schema, destination_path):\r\n",
							"    try:\r\n",
							"        logger.debug(f\"[OEA] s1_to_s2: source_path={source_path}, destination_path={destination_path}\")\r\n",
							"        df = spark.read.csv(source_path, header='false', schema=schema)\r\n",
							"        df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(destination_path)\r\n",
							"    except (AnalysisException) as error:\r\n",
							"        logger.exception(\"[OEA] \" + str(error))\r\n",
							"        logger.warning(\"[OEA] Warning: \" + str(error))\r\n",
							"        return \"\"\r\n",
							"\r\n",
							"def process_M365_roster(file_path):\r\n",
							"    #AadUser\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('AnchorId', StringType()),StructField('DisplayName', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('LastSeenDateTime', TimestampType()),StructField('Mail', StringType()),StructField('MailNickname', StringType()),StructField('Role', StringType()),StructField('Surname', StringType()),StructField('UserPrincipalName', StringType()),StructField('StudentId', StringType()),StructField('TeacherId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUser', schema, stage2 + '/M365/AadUser')\r\n",
							"    #AadUserPersonMapping\r\n",
							"    schema = StructType([StructField('ObjectId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/AadUserPersonMapping', schema, stage2 + '/M365/AadUserPersonMapping')\r\n",
							"    #Course\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('AcademicYearSessionId', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Course', schema, stage2 + '/M365/Course')\r\n",
							"    #CourseGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseGradeLevel', schema, stage2 + '/M365/CourseGradeLevel')\r\n",
							"    #CourseSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('CourseId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/CourseSubject', schema, stage2 + '/M365/CourseSubject')\r\n",
							"    #Enrollment\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefSectionRoleId', StringType()),StructField('SectionId', StringType()),StructField('SourceSystemId', StringType()),StructField('EntryDate', StringType()),StructField('ExitDate', StringType()),StructField('IsPrimaryStaffForSection', BooleanType())])\r\n",
							"    s1_to_s2(file_path + '/Enrollment', schema, stage2 + '/M365/Enrollment')\r\n",
							"    #Organization\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefOrganizationTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('Identifier', StringType()),StructField('ParentOrganizationId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Organization', schema, stage2 + '/M365/Organization')\r\n",
							"    #Person\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('GivenName', StringType()),StructField('MiddleName', StringType()),StructField('PreferredGivenName', StringType()),StructField('PreferredMiddleName', StringType()),StructField('PreferredSurname', StringType()),StructField('Surname', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Person', schema, stage2 + '/M365/Person')\r\n",
							"    #PersonDemographic\r\n",
							"    schema = StructType([StructField('PersonId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('BirthCity', StringType()),StructField('BirthCountryCode', StringType()),StructField('BirthDate', StringType()),StructField('BirthState', StringType()),StructField('RefSexId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographic', schema, stage2 + '/M365/PersonDemographic')\r\n",
							"    #PersonDemographicEthnicity\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefEthnicityId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicEthnicity', schema, stage2 + '/M365/PersonDemographicEthnicity')\r\n",
							"    #PersonDemographicPersonFlag\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonFlagId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicPersonFlag', schema, stage2 + '/M365/PersonDemographicPersonFlag')\r\n",
							"    #PersonDemographicRace\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefRaceId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonDemographicRace', schema, stage2 + '/M365/PersonDemographicRace')\r\n",
							"    #PersonEmailAddress\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EmailAddress', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefEmailAddressTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonEmailAddress', schema, stage2 + '/M365/PersonEmailAddress')\r\n",
							"    #PersonIdentifier\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('Identifier', StringType()),StructField('IsPresentInSource', BooleanType()),StructField('PersonId', StringType()),StructField('RefIdentifierTypeId', StringType()),StructField('SourceSystemId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonIdentifier', schema, stage2 + '/M365/PersonIdentifier')\r\n",
							"    #PersonOrganizationRole\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('OrganizationId', StringType()),StructField('PersonId', StringType()),StructField('RefRoleId', StringType()),StructField('SessionId', StringType()),StructField('SourceSystemId', StringType()),StructField('IsPrimary', BooleanType()),StructField('RefGradeLevelId', StringType()),StructField('RoleEndDate', StringType()),StructField('RoleStartDate', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonOrganizationRole', schema, stage2 + '/M365/PersonOrganizationRole')\r\n",
							"    #PersonPhoneNumber\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('PhoneNumber', StringType()),StructField('PriorityOrder', IntegerType()),StructField('RefPhoneNumberTypeId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonPhoneNumber', schema, stage2 + '/M365/PersonPhoneNumber')\r\n",
							"    #PersonRelationship\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('PersonId', StringType()),StructField('RefPersonRelationshipId', StringType()),StructField('RelatedPersonId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/PersonRelationship', schema, stage2 + '/M365/PersonRelationship')\r\n",
							"    #RefDefinition\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('Code', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Namespace', StringType()),StructField('RefType', StringType()),StructField('SortOrder', IntegerType())])\r\n",
							"    s1_to_s2(file_path + '/RefDefinition', schema, stage2 + '/M365/RefDefinition')\r\n",
							"    #Section\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('OrganizationId', StringType()),StructField('SourceSystemId', StringType()),StructField('Code', StringType()),StructField('CourseId', StringType()),StructField('Location', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Section', schema, stage2 + '/M365/Section')\r\n",
							"    #SectionGradeLevel\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefGradeLevelId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionGradeLevel', schema, stage2 + '/M365/SectionGradeLevel')\r\n",
							"    #SectionSession\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('IsActiveInSession', BooleanType()),StructField('LastSeenDateTime', TimestampType()),StructField('SectionId', StringType()),StructField('SessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSession', schema, stage2 + '/M365/SectionSession')\r\n",
							"    #SectionSubject\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('RefAcademicSubjectId', StringType()),StructField('SectionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SectionSubject', schema, stage2 + '/M365/SectionSubject')\r\n",
							"    #Session\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('EndDate', StringType()),StructField('ExternalId', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType()),StructField('RefAcademicYearId', StringType()),StructField('RefSessionTypeId', StringType()),StructField('SourceSystemId', StringType()),StructField('StartDate', StringType()),StructField('ParentSessionId', StringType())])\r\n",
							"    s1_to_s2(file_path + '/Session', schema, stage2 + '/M365/Session')\r\n",
							"    #SourceSystem\r\n",
							"    schema = StructType([StructField('Id', StringType()),StructField('FirstSeenDateTime', TimestampType()),StructField('LastSeenDateTime', TimestampType()),StructField('Name', StringType())])\r\n",
							"    s1_to_s2(file_path + '/SourceSystem', schema, stage2 + '/M365/SourceSystem')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from opencensus.ext.azure.log_exporter import AzureLogHandler, logging\r\n",
							"\r\n",
							"inbound_path = stage1 + '/M365/inbound/roster'\r\n",
							"processed_path = stage1 + '/M365/processed/roster'\r\n",
							"\r\n",
							"logger.info(\"[OEA] Processing M365 roster data from: \" + inbound_path)\r\n",
							"\r\n",
							"items = mssparkutils.fs.ls(inbound_path)\r\n",
							"for item in items:\r\n",
							"    if item.isDir:\r\n",
							"        process_M365_roster(item.path)\r\n",
							"        mssparkutils.fs.mv(item.path, processed_path, True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset everything to run it again"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete stage2/M365 if it exists\r\n",
							"files = mssparkutils.fs.ls(stage2)\r\n",
							"#print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
							"for file in files:\r\n",
							"    print(file.name)\r\n",
							"    if file.name == 'M365':\r\n",
							"        mssparkutils.fs.rm(stage2 + '/M365', True)\r\n",
							"\r\n",
							"# Move roster data back in to \"inbound\" folder\r\n",
							"files = mssparkutils.fs.ls(stage1 + '/M365/processed/roster')\r\n",
							"for file in files:\r\n",
							"    mssparkutils.fs.mv(file.path, stage1 + '/M365/inbound/roster', True)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/stream_tutorial')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive/s1_to_s2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Structured streaming tutorial"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"file_path = '/M365/inbound/roster/2021-06-15T04-04-12'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 25
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tutorial_01')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "3) sandbox/archive"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark2v3",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark2v3",
						"name": "spark2v3",
						"type": "Spark",
						"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark2v3",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": true
							}
						},
						"source": [
							"storage_account = 'stoeacisd3gg1'\r\n",
							"stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\r\n",
							"stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#step 1 - initial ingestion (specify schema)\r\n",
							"\r\n",
							"df = spark.read.csv(stage1 + '/tutorial_01/roster/2021-06-02/Person', header='false')\r\n",
							"sqlContext.registerDataFrameAsTable(df, 'Person')\r\n",
							"sql_str = \"select _c0 Id, to_timestamp(_c1) FirstSeenDateTime, to_timestamp(_c2) LastSeenDateTime, _c3 GivenName, _c4 MiddleName, _c5 PreferredGivenName, _c6 PreferredMiddleName, _c7 PreferredSurname, _c8 Surname from Person\"\r\n",
							"df = spark.sql(sql_str)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/tutorial_01/Person')\r\n",
							"\r\n",
							"# now show the current state of the Person table\r\n",
							"df = spark.read.format(\"delta\").load(stage2 + '/tutorial_01/Person')\r\n",
							"df.show(100)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# step 2 - update table with new snapshot data\r\n",
							"\r\n",
							"# Ingest next snapshot of Person records, with the following changes:\r\n",
							"# Added 2 new students: Andrew Hudson, Amber Buchanan\r\n",
							"# Deleted 1 student: Erika Riley\r\n",
							"# Modified 1 student: Joseph Kelley was modified to be Joey Kelley\r\n",
							"df = spark.read.csv(stage1 + '/tutorial_01/roster/2021-06-10/Person', header='false')\r\n",
							"sqlContext.registerDataFrameAsTable(df, 'Person')\r\n",
							"sql_str = \"select _c0 Id, to_timestamp(_c1) FirstSeenDateTime, to_timestamp(_c2) LastSeenDateTime, _c3 GivenName, _c4 MiddleName, _c5 PreferredGivenName, _c6 PreferredMiddleName, _c7 PreferredSurname, _c8 Surname from Person\"\r\n",
							"df = spark.sql(sql_str)\r\n",
							"df.write.format('delta').mode('overwrite').save(stage2 + '/tutorial_01/Person')\r\n",
							"# now show the current state of the Person table\r\n",
							"df = spark.read.format(\"delta\").load(stage2 + '/tutorial_01/Person')\r\n",
							"df.show(30)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# step 3 - update table with delta data (upsert)\r\n",
							"# Ingest delta data consisting of a new student \"Sheldon Bergeron\", and an update to \"Shelby Berger\" (lastnamt changed to \"Bergeron\")\r\n",
							"# https://docs.delta.io/latest/api/python/index.html?highlight=whenmatchedupdateall#delta.tables.DeltaTable.merge\r\n",
							"from delta.tables import *\r\n",
							"\r\n",
							"updatesDF = spark.read.csv(stage1 + '/tutorial_01/roster/2021-06-11/Person', header='false')\r\n",
							"sqlContext.registerDataFrameAsTable(updatesDF, 'Person')\r\n",
							"sql_str = \"select _c0 Id, to_timestamp(_c1) FirstSeenDateTime, to_timestamp(_c2) LastSeenDateTime, _c3 GivenName, _c4 MiddleName, _c5 PreferredGivenName, _c6 PreferredMiddleName, _c7 PreferredSurname, _c8 Surname from Person\"\r\n",
							"updatesDF = spark.sql(sql_str)\r\n",
							"#display(updatesDF)\r\n",
							"deltaTable = DeltaTable.forPath(spark, stage2 + '/tutorial_01/Person')\r\n",
							"\r\n",
							"deltaTable.alias(\"Person\").merge(\r\n",
							"    updatesDF.alias(\"updates\"),\r\n",
							"    \"Person.Id = updates.Id\") \\\r\n",
							"  .whenMatchedUpdate(set = \r\n",
							"      {\r\n",
							"      \"FirstSeenDateTime\": \"updates.FirstSeenDateTime\",\r\n",
							"      \"LastSeenDateTime\": \"updates.LastSeenDateTime\",\r\n",
							"      \"GivenName\": \"updates.GivenName\",\r\n",
							"      \"MiddleName\": \"updates.MiddleName\",\r\n",
							"      \"PreferredGivenName\": \"updates.PreferredGivenName\",\r\n",
							"      \"PreferredMiddleName\": \"updates.PreferredMiddleName\",\r\n",
							"      \"PreferredSurname\": \"updates.PreferredSurname\",\r\n",
							"      \"Surname\": \"updates.Surname\"\r\n",
							"    } \r\n",
							"  ) \\\r\n",
							"  .whenNotMatchedInsert(values =\r\n",
							"    {\r\n",
							"      \"Id\": \"updates.Id\",\r\n",
							"      \"FirstSeenDateTime\": \"updates.FirstSeenDateTime\",\r\n",
							"      \"LastSeenDateTime\": \"updates.LastSeenDateTime\",\r\n",
							"      \"GivenName\": \"updates.GivenName\",\r\n",
							"      \"MiddleName\": \"updates.MiddleName\",\r\n",
							"      \"PreferredGivenName\": \"updates.PreferredGivenName\",\r\n",
							"      \"PreferredMiddleName\": \"updates.PreferredMiddleName\",\r\n",
							"      \"PreferredSurname\": \"updates.PreferredSurname\",\r\n",
							"      \"Surname\": \"updates.Surname\"\r\n",
							"    } \r\n",
							"  ) \\\r\n",
							"  .execute()\r\n",
							"# now show the current state of the Person table\r\n",
							"df = spark.read.format(\"delta\").load(stage2 + '/tutorial_01/Person')\r\n",
							"df.show(30)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# now show the current state of the Person table\r\n",
							"df = spark.read.format(\"delta\").load(stage2 + '/tutorial_01/Person')\r\n",
							"df.show(50)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# step 4: describe history and time travel\r\n",
							"\r\n",
							"from delta.tables import DeltaTable\r\n",
							"deltaTable = DeltaTable.forPath(spark, stage2 + '/tutorial_01/Person')\r\n",
							"print(\"######## Describe history for the table ######\")\r\n",
							"deltaTable.history().show()\r\n",
							"\r\n",
							"print(\"######## Show an earlier version of the table ######\")\r\n",
							"df1 = spark.read.format('delta').option('versionAsOf', 1).load(stage2 + '/tutorial_01/Person')\r\n",
							"df1.show(30)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Create spark db to use with Power BI\r\n",
							"#spark.sql(\"CREATE TABLE events USING DELTA LOCATION '\" + stage2 + \"/tutorial_01/Person'\")\r\n",
							"\r\n",
							"spark.sql('CREATE DATABASE IF NOT EXISTS tutorial_02')\r\n",
							"spark.sql(\"create table if not exists tutorial_02.Person using DELTA location '\" + stage2 + \"/tutorial_01/Person'\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": true
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.sql(\"SELECT * FROM tutorial_01.person\")\r\n",
							"df.show(10)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Activity data\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# insert new activity data\r\n",
							"# todo: port the example below, as described here: https://docs.delta.io/0.8.0/delta-update.html#-merge-in-dedup\r\n",
							"deltaTable.alias(\"logs\").merge(\r\n",
							"    newDedupedLogs.alias(\"newDedupedLogs\"),\r\n",
							"    \"logs.uniqueId = newDedupedLogs.uniqueId\") \\\r\n",
							"  .whenNotMatchedInsertAll() \\\r\n",
							"  .execute()\r\n",
							"\r\n",
							"from delta.tables import DeltaTable\r\n",
							"deltaTable = DeltaTable.forPath(spark, dest_path)\r\n",
							"deltaTable.alias(\"TechActivity\").merge(updatesDF.alias(\"updates\"), \"TechActivity.SignalId = updates.SignalId\") \\\r\n",
							"    .whenNotMatchedInsertAll() \\\r\n",
							"    .execute()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reset the tutorial"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Delete all data from stage2 - to reset the exercise and start over\r\n",
							"from notebookutils import mssparkutils\r\n",
							"mssparkutils.fs.rm(stage2 + '/tutorial_01', True)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": true
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"# to get a list of all the commands available\r\n",
							"mssparkutils.fs.help()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OEA_ex3_data_ingestion')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This is an example of a parent pipeline that uses a child pipeline to retrieves data from multiple HTTP endpoints, using parameters to list the URL's to retrieve data from.",
				"activities": [
					{
						"name": "get contoso_sis test data",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "OEA_ex2_data_ingestion",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"endpoints": "[\n{\"source\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentattendance.csv\", \"destinationDirectory\":\"contoso_sis\", \"destinationFilename\":\"studentattendance/studentattendance.csv\"},\n{\"source\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentdemographics.csv\", \"destinationDirectory\":\"contoso_sis\", \"destinationFilename\":\"studentdemographics/studentdemographics.csv\"},\n{\"source\":\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/Contoso_SIS/test_data/studentsectionmark.csv\", \"destinationDirectory\":\"contoso_sis\", \"destinationFilename\":\"studentsectionmark/studentsectionmark.csv\"}\n]",
								"sinkFilesystem": "stage1np"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/OEA_ex2_data_ingestion')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 14')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://stoeacisd3gg1.dfs.core.windows.net/stage1np/contoso_sis/studentattendance/studentattendance.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION='2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		}
	]
}