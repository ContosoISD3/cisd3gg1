{
	"name": "ContosoSIS_example",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark4v3p1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 4,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "4",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "713f9ee8-c60d-4876-9e4a-0d5318f6197f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark4v3p1",
				"name": "spark4v3p1",
				"type": "Spark",
				"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark4v3p1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ContosoSIS Example\n",
					"This example demonstrates how to use the OEA framework and modules to process incoming data, perform data prep, and view the data in an example Power BI dashboard.\n",
					"\n",
					"# Running the example\n",
					"1) Run the Copy_Contoso_SIS_data pipeline in order to land test data in your data lake. Do this by clicking on \"Integrate\" in the left navigation, then click on Copy_Contoso_test_data, then click on \"Add Trigger\" and then \"Trigger Now\".\n",
					"\n",
					"2) Click on \"Run all\" at the top of this tab (and wait for the processing to complete - which can take around 5 to 10 minutes).\n",
					"\n",
					"3) Open the dashboard in Power BI desktop and point it to your newly setup data lake (you can download the pbix from here: [techInequityDashboardContoso v2.pbix](https://github.com/microsoft/OpenEduAnalytics/blob/main/packages/ContosoISD/power_bi/techInequityDashboardContoso%20v2.pbix) )"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"%run /OEA_py"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"%run /ContosoSIS_py"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# 1) Initialize the OEA framework and modules needed.\n",
					"oea = OEA()\n",
					"contoso_sis = ContosoSIS(oea)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"        \"\"\" Processes activity data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
					"        logger.info(\"Processing ms_insights activity data from: \" + self.stage1np_activity)\r\n",
					"\r\n",
					"        spark_schema = self.oea.to_spark_schema(self.schemas['TechActivity'])\r\n",
					"        df = spark.readStream.csv(self.stage1np_activity + '/*/*.csv', header='false', schema=spark_schema)\r\n",
					"        df = df.dropDuplicates(['SignalId'])\r\n",
					"        df = df.withColumn('year', F.year(F.col('StartTime'))).withColumn('month', F.month(F.col('StartTime')))\r\n",
					"\r\n",
					"        df_pseudo, df_lookup = self.oea.pseudonymize(df, self.schemas['TechActivity'])\r\n",
					"\r\n",
					"        query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_activity + '/_checkpoints').partitionBy('year', 'month')\r\n",
					"        query = query.start(self.stage2p + '/TechActivity')\r\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        logger.info(query.lastProgress)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = spark.read.load(f'{oea.stage2p}/contoso_sis/studentattendance', format='delta')\r\n",
					"#display(df)\r\n",
					"df.schema\r\n",
					""
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# 2) Process the raw data (csv format) from stage1 into stage2 (adds schema details and writes out in parquet format).\n",
					"#    [Note: we're not performing pseudonymization in this example, so everything is written to container stage2np.]\n",
					"m365.process_latest_roster_from_stage1()\n",
					"contoso_sis.process_latest_from_stage1()\n",
					"m365.process_activity_data_from_stage1()"
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# 3) Run additional prep on the data to create a unified dataset that can be used in a Power BI report\n",
					"\n",
					"# Process sectionmark data. Convert id values to use the Person.Id and Section.Id values set in the m365 data.\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2p + '/contoso_sis/studentsectionmark'), 'SectionMark')\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/m365/Person'), 'Person')\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/m365/Section'), 'Section')\n",
					"df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\n",
					"sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\n",
					"sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\n",
					"from SectionMark sm, Person p, Section s \\\n",
					"where sm.student_id = p.ExternalId \\\n",
					"and sm.section_id = s.ExternalId\")\n",
					"df.write.format('delta').mode('overwrite').save(oea.stage2np + '/ContosoISD/SectionMark')\n",
					"\n",
					"# Repeat the above process, this time for student attendance\n",
					"# Convert id values to use the Person.Id, Org.Id and Section.Id values\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/contoso_sis/studentattendance'), 'Attendance')\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/m365/Org'), 'Org')\n",
					"df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\n",
					"att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\n",
					"att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\n",
					"from Attendance att, Org o, Person p, Section s \\\n",
					"where att.student_id = p.ExternalId \\\n",
					"and att.school_id = o.ExternalId \\\n",
					"and att.section_id = s.ExternalId\")\n",
					"df.write.format('delta').mode('overwrite').save(oea.stage2np +'/ContosoISD/Attendance')\n",
					"\n",
					"# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\n",
					"sqlContext.registerDataFrameAsTable(spark.read.format('delta').load(oea.stage2np + '/m365/Course'), 'Course')\n",
					"df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\n",
					"df.write.format('delta').mode('overwrite').save(oea.stage2np + '/ContosoISD/Course')"
				],
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# 4) Create spark db's that point to the data in the data lake to allow for connecting via Power BI through use of the Serverless SQL endpoint.\n",
					"contoso_sis.create_stage2_db('PARQUET')\n",
					"m365.create_stage2_db('PARQUET')\n",
					"\n",
					"spark.sql('CREATE DATABASE IF NOT EXISTS s2_ContosoISD')\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Activity using PARQUET location '\" + oea.stage2np + \"/m365/TechActivity'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Calendar using PARQUET location '\" + oea.stage2np + \"/m365/Calendar'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Org using PARQUET location '\" + oea.stage2np + \"/m365/Org'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Person using PARQUET location '\" + oea.stage2np + \"/m365/Person'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.PersonIdentifier using PARQUET location '\" + oea.stage2np + \"/m365/PersonIdentifier'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.RefDefinition using PARQUET location '\" + oea.stage2np + \"/m365/RefDefinition'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Section using PARQUET location '\" + oea.stage2np + \"/m365/Section'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Session using PARQUET location '\" + oea.stage2np + \"/m365/Session'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.StaffOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StaffOrgAffiliation'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.StaffSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StaffSectionMembership'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.StudentOrgAffiliation using PARQUET location '\" + oea.stage2np + \"/m365/StudentOrgAffiliation'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.StudentSectionMembership using PARQUET location '\" + oea.stage2np + \"/m365/StudentSectionMembership'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Course using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Course'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.Attendance using PARQUET location '\" + oea.stage2np + \"/ContosoISD/Attendance'\")\n",
					"spark.sql(\"create table if not exists s2_ContosoISD.SectionMark using PARQUET location '\" + oea.stage2np + \"/ContosoISD/SectionMark'\")\n",
					"\n",
					"print(f\"Created spark db's.\\nYou can now open the 'techInequityDashboardContoso v2.pbix' dashboard and change the datasource to point to: {oea.serverless_sql_endpoint}\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Reset everything\n",
					"You can uncomment line 11 in the cell below and run the cell to reset everything and walk through the process again from the top (starting from running the pipeline to bring test data into stage1np).\n",
					"\n",
					"Note: remember to comment out line 11 again to prevent accidental resetting of the example"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"def reset_all_processing():\n",
					"    contoso_sis.delete_all_stages()\n",
					"    m365.delete_all_stages()\n",
					"    oea.rm_if_exists(oea.stage2np + '/ContosoISD')\n",
					"\n",
					"    oea.drop_db('s2_contoso_sis')\n",
					"    oea.drop_db('s2_contosoisd')\n",
					"    oea.drop_db('s2_m365')\n",
					"\n",
					"# Uncomment the following line and run this cell to reset everything if you want to walk through the process again.\n",
					"#reset_all_processing()"
				],
				"execution_count": null
			}
		]
	}
}