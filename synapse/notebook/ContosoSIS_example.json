{
	"name": "ContosoSIS_example",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark4v3p1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 4,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "4",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "77e33bf7-b723-4212-ac57-4de95521c398"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-CISD3GG1/providers/Microsoft.Synapse/workspaces/syn-oea-cisd3gg1/bigDataPools/spark4v3p1",
				"name": "spark4v3p1",
				"type": "Spark",
				"endpoint": "https://syn-oea-cisd3gg1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark4v3p1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# ContosoSIS Example\n",
					"This example demonstrates how to use the OEA framework and modules to process incoming data, perform data prep, and view the data in an example Power BI dashboard.\n",
					"\n",
					"# Running the example\n",
					"1) Run the Copy_Contoso_SIS_data pipeline in order to land test data in your data lake. Do this by clicking on \"Integrate\" in the left navigation, then click on Copy_Contoso_test_data, then click on \"Add Trigger\" and then \"Trigger Now\".\n",
					"\n",
					"2) Click on \"Run all\" at the top of this tab (and wait for the processing to complete - which can take around 5 to 10 minutes).\n",
					"\n",
					"3) Open the dashboard in Power BI desktop and point it to your newly setup data lake (you can download the pbix from here: [techInequityDashboardContoso v2.pbix](https://github.com/microsoft/OpenEduAnalytics/blob/main/packages/ContosoISD/power_bi/techInequityDashboardContoso%20v2.pbix) )"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"%run /OEA_py"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"%run /ContosoSIS_py"
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# 1) Initialize the OEA framework and modules needed.\n",
					"oea = OEA()\n",
					"contoso_sis = ContosoSIS(oea)"
				],
				"execution_count": 59
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"contoso_sis.ingest()"
				],
				"execution_count": 60
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def ingest_incremental_csv_data(source_system, tablename, schema, partition_by, primary_key='id', has_header=True):\r\n",
					"    \"\"\" Processes incremental batch data from stage1 into stage2 \"\"\"\r\n",
					"    source_path = f'{oea.stage1np}/{source_system}/{tablename}'\r\n",
					"    p_destination_path = f'{oea.stage2p}/{source_system}/{tablename}'\r\n",
					"    np_destination_path = f'{oea.stage2np}/{source_system}/{tablename}'\r\n",
					"    logger.info(f'Processing incremental data from: {source_path} and writing out to: {p_destination_path}')\r\n",
					"\r\n",
					"    spark_schema = oea.to_spark_schema(schema)\r\n",
					"    if has_header: header_flag = 'true'\r\n",
					"    else: header_flag = 'false'\r\n",
					"    df = spark.readStream.csv(source_path + '/**/*.csv', header=header_flag, schema=spark_schema)\r\n",
					"    #df = spark.read.csv(source_path + '/**/*.csv', header=header_flag, schema=spark_schema)\r\n",
					"    #display(df)\r\n",
					"    df = df.dropDuplicates([primary_key])\r\n",
					"    df_pseudo, df_lookup = oea.pseudonymize(df, schema)\r\n",
					"\r\n",
					"    if len(df_pseudo.columns) == 0:\r\n",
					"        logger.info('No data to be written to stage2p')\r\n",
					"    else:        \r\n",
					"        query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_p').partitionBy(partition_by)\r\n",
					"        query = query.start(p_destination_path)\r\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        logger.info(query.lastProgress)\r\n",
					"\r\n",
					"    if len(df_lookup.columns) == 0:\r\n",
					"        logger.info('No data to be written to stage2np')\r\n",
					"    else:\r\n",
					"        query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", source_path + '/_checkpoints_np').partitionBy(partition_by)\r\n",
					"        query2 = query2.start(np_destination_path)\r\n",
					"        query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        logger.info(query2.lastProgress)\r\n",
					"\r\n",
					"ingest_incremental_csv_data('contoso_sis', 'studentattendance', contoso_sis.schemas['studentattendance'], 'school_year', 'id')"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# 4) Create spark db's that point to the data in the data lake to allow for connecting via Power BI through use of the Serverless SQL endpoint.\n",
					"contoso_sis.create_stage2_db()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Reset everything\n",
					"You can uncomment line 11 in the cell below and run the cell to reset everything and walk through the process again from the top (starting from running the pipeline to bring test data into stage1np).\n",
					"\n",
					"Note: remember to comment out line 11 again to prevent accidental resetting of the example"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"def reset_all_processing():\n",
					"    contoso_sis.delete_all_stages()\n",
					"    m365.delete_all_stages()\n",
					"    oea.rm_if_exists(oea.stage2np + '/ContosoISD')\n",
					"\n",
					"    oea.drop_db('s2_contoso_sis')\n",
					"    oea.drop_db('s2_contosoisd')\n",
					"    oea.drop_db('s2_m365')\n",
					"\n",
					"# Uncomment the following line and run this cell to reset everything if you want to walk through the process again.\n",
					"#reset_all_processing()"
				],
				"execution_count": null
			}
		]
	}
}